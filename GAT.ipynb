{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT Models to Compute Heritage Values and Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from itertools import product\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from torch_geometric.data import (\n",
    "    HeteroData,\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    "    extract_zip,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from torch_geometric.transforms import RandomLinkSplit, ToUndirected\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATConv, Linear, to_hetero\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric import seed_everything\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_cluster import knn_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\surf\\\\TUD\\\\Paper\\\\Venice_Graph'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.10.2\n",
      "GPU-enabled installation? True\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version {}\".format(torch.__version__))\n",
    "print(\"GPU-enabled installation? {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    if cuda:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    path = 'dataset/Venice',\n",
    "    save_dir='model_storage/GAT/',\n",
    "    model_state_file='model.pth',\n",
    "    \n",
    "    # Model hyper parameters\n",
    "    hidden_channels = 256,\n",
    "    num_layers = 3,\n",
    "    num_heads = 2,\n",
    "    k=3,\n",
    "    use_gdc=True,\n",
    "    \n",
    "    # Training hyper parameters\n",
    "    sample_nodes = 25,\n",
    "    batch_size=32,\n",
    "    early_stopping_criteria=100,\n",
    "    learning_rate=0.0001,\n",
    "    l2=2e-4,\n",
    "    dropout_p=0.1,\n",
    "    num_epochs=1000,\n",
    "    seed=42,\n",
    "    \n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/GAT/model.pth\n"
     ]
    }
   ],
   "source": [
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VEN(InMemoryDataset):\n",
    "    r\"\"\"A subset of Flickr post collected in Venice annotated with Heritage \n",
    "    Values and Attributes, as collected in the `\"Heri-Graphs: A Workflow of \n",
    "    Creating Datasets for Multi-modal Machine Learning on Graphs of Heritage \n",
    "    Values and Attributes with Social Media\" <https://arxiv.org/abs/2205.07545>`\n",
    "    paper.\n",
    "    VEN is a heterogeneous graph containing two types of nodes - nodes with only \n",
    "    visual features 'vis_only' (1,190 nodes), nodes with both visual and textual\n",
    "    features 'vis_tex' (1,761 nodes) and four types of links - social similarity\n",
    "    'SOC' (488,103 links), spatial similarity (445,779 links), temporal similarity\n",
    "    (501,191 links), and simple composed link (1,071,977 links).\n",
    "    Vis_only nodes are represented with 982-dimensional visual features and are\n",
    "    divided into 9 heritage attribute categories \n",
    "    ('architectural elements', 'form', 'gastronomy', 'interior',\n",
    "    'landscape scenery and natural features', 'monuments', 'people', 'product', \n",
    "    'urban scenery').\n",
    "    Vis_text nodes are represented with 1753-dimensional visual and textual \n",
    "    features and are divided into 9 heritage attribute categories plus 11 \n",
    "    heritage value categories ('criterion i-x', 'other').\n",
    "    Both types of nodes are also merged into a single type of node 'all' with \n",
    "    1753-dimensional features and 20-dimensional label categories.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.HeteroData` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            every access. (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.HeteroData` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "    \n",
    "    Stats:\n",
    "            * - #nodes\n",
    "              - #edges\n",
    "              - #features\n",
    "              - #classes\n",
    "            * - 2,951\n",
    "              - 1,071,977\n",
    "              - 1753\n",
    "              - 20\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://drive.google.com/uc?export=download&id=1sxcKiZr1YGDv06wr03nsk5HVZledgzi9'\n",
    "\n",
    "    def __init__(self, root: str, transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return [\n",
    "            'A_simp.npz', 'A_SOC.npz', 'A_SPA.npz', 'A_TEM.npz', 'labels.npz',\n",
    "            'node_types.npy', 'Textual_Features.npy', 'train_val_test_idx.npz',\n",
    "            'Visual_Features.npy'\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        path = download_url(self.url, self.raw_dir)\n",
    "        extract_zip(path, self.raw_dir)\n",
    "        os.remove(path)\n",
    "\n",
    "    def process(self):\n",
    "        data = HeteroData()\n",
    "\n",
    "        node_types = ['vis_only', 'vis_tex']\n",
    "        link_types = ['SOC', 'SPA', 'TEM', 'simp']\n",
    "\n",
    "        vis = np.load(osp.join(self.raw_dir, 'Visual_Features.npy'),allow_pickle=True)[:,2:].astype(float)\n",
    "        tex = np.load(osp.join(self.raw_dir, 'Textual_Features.npy'),allow_pickle=True)[:,5:].astype(float)\n",
    "\n",
    "        x = np.hstack([vis,np.nan_to_num(tex)])\n",
    "\n",
    "\n",
    "        node_type_idx = np.load(osp.join(self.raw_dir, 'node_types.npy'))\n",
    "        node_type_idx = torch.from_numpy(node_type_idx).to(torch.long)\n",
    "\n",
    "        data['vis_only'].num_nodes = int((node_type_idx == 0).sum())\n",
    "        data['vis_tex'].num_nodes = int((node_type_idx == 1).sum())\n",
    "        data['all'].num_nodes = len(node_type_idx)\n",
    "\n",
    "        data['vis_only'].x = torch.from_numpy(vis[node_type_idx==0]).to(torch.float)\n",
    "        data['vis_tex'].x = torch.from_numpy(x[node_type_idx==1]).to(torch.float)\n",
    "        data['all'].x = torch.from_numpy(x).to(torch.float)\n",
    "\n",
    "\n",
    "        y_s = np.load(osp.join(self.raw_dir, 'labels.npz'), allow_pickle=True)\n",
    "        att_lab = y_s['ATT_LAB'][:,1:10].astype(float)\n",
    "        val_lab = np.nan_to_num(y_s['VAL_LAB'][:,2:13].astype(float))\n",
    "        ys = np.hstack([att_lab, val_lab])\n",
    "\n",
    "        data['vis_only'].y = torch.from_numpy(att_lab[node_type_idx==0]).to(torch.float)\n",
    "        data['vis_tex'].y = torch.from_numpy(ys[node_type_idx==1]).to(torch.float)\n",
    "        data['all'].y = torch.from_numpy(ys).to(torch.float)\n",
    "\n",
    "        data.node_type = node_type_idx\n",
    "\n",
    "        split = np.load(osp.join(self.raw_dir, 'train_val_test_idx.npz'))\n",
    "        for name in ['train', 'val', 'test']:\n",
    "            idx = split[f'{name}_idx']\n",
    "            idx = torch.from_numpy(idx).to(torch.long)\n",
    "            mask = torch.zeros(data['all'].num_nodes, dtype=torch.bool)\n",
    "            mask[idx] = True\n",
    "            data['all'][f'{name}_mask'] = mask\n",
    "            data['vis_only'][f'{name}_mask'] = mask[node_type_idx==0]\n",
    "            data['vis_tex'][f'{name}_mask'] = mask[node_type_idx==1]\n",
    "\n",
    "        \n",
    "        s = {}\n",
    "        s['vis_only'] = np.arange(len(x))[node_type_idx==0]\n",
    "        s['vis_tex'] = np.arange(len(x))[node_type_idx==1]\n",
    "\n",
    "        for link in link_types:\n",
    "            A_sub = sp.load_npz(osp.join(self.raw_dir, f'A_{link}.npz')).tocoo()\n",
    "            if A_sub.nnz>0:\n",
    "                row = torch.from_numpy(A_sub.row).to(torch.long)\n",
    "                col = torch.from_numpy(A_sub.col).to(torch.long)\n",
    "                data['all', f'{link}_link', 'all'].edge_index = torch.stack([row, col], dim=0)\n",
    "                data['all', f'{link}_link', 'all'].edge_attr = torch.from_numpy(A_sub.data).to(torch.long)\n",
    "\n",
    "        for src, dst in product(node_types, node_types):\n",
    "            for link in link_types:\n",
    "                A_sub = sp.load_npz(osp.join(self.raw_dir, f'A_{link}.npz'))[s[src]][:,s[dst]].tocoo()\n",
    "                if A_sub.nnz>0:\n",
    "                    row = torch.from_numpy(A_sub.row).to(torch.long)\n",
    "                    col = torch.from_numpy(A_sub.col).to(torch.long)\n",
    "                    data[src, f'{link}_link', dst].edge_index = torch.stack([row, col], dim=0)\n",
    "                    data[src, f'{link}_link', dst].edge_attr = torch.from_numpy(A_sub.data).to(torch.long)\n",
    "\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VEN_Homo(InMemoryDataset):\n",
    "    r\"\"\"A subset of Flickr post collected in Venice annotated with Heritage \n",
    "    Values and Attributes, as collected in the `\"Heri-Graphs: A Workflow of \n",
    "    Creating Datasets for Multi-modal Machine Learning on Graphs of Heritage \n",
    "    Values and Attributes with Social Media\" <https://arxiv.org/abs/2205.07545>`\n",
    "    paper.\n",
    "    VEN_Homo is a homogeneous graph containing 2951 nodes and 1,071,977 links.\n",
    "    Vis_only nodes are represented with 982-dimensional visual features and are\n",
    "    divided into 9 heritage attribute categories \n",
    "    ('architectural elements', 'form', 'gastronomy', 'interior',\n",
    "    'landscape scenery and natural features', 'monuments', 'people', 'product', \n",
    "    'urban scenery').\n",
    "    Vis_text nodes are represented with 1753-dimensional visual and textual \n",
    "    features and are divided into 9 heritage attribute categories plus 11 \n",
    "    heritage value categories ('criterion i-x', 'other').\n",
    "    Both types of nodes are also merged into a single type of node 'all' with \n",
    "    1753-dimensional features and 20-dimensional label categories.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.HeteroData` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            every access. (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.HeteroData` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "    \n",
    "    Stats:\n",
    "            * - #nodes\n",
    "              - #edges\n",
    "              - #features\n",
    "              - #classes\n",
    "            * - 2,951\n",
    "              - 1,071,977\n",
    "              - 1753\n",
    "              - 20\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://drive.google.com/uc?export=download&id=1sxcKiZr1YGDv06wr03nsk5HVZledgzi9'\n",
    "\n",
    "    def __init__(self, root: str, transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return [\n",
    "            'A_simp.npz', 'A_SOC.npz', 'A_SPA.npz', 'A_TEM.npz', 'labels.npz',\n",
    "            'node_types.npy', 'Textual_Features.npy', 'train_val_test_idx.npz',\n",
    "            'Visual_Features.npy'\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        path = download_url(self.url, self.raw_dir)\n",
    "        extract_zip(path, self.raw_dir)\n",
    "        os.remove(path)\n",
    "\n",
    "    def process(self):\n",
    "        data = Data()\n",
    "\n",
    "        link_types = ['simp']\n",
    "\n",
    "        vis = np.load(osp.join(self.raw_dir, 'Visual_Features.npy'),allow_pickle=True)[:,2:].astype(float)\n",
    "        tex = np.load(osp.join(self.raw_dir, 'Textual_Features.npy'),allow_pickle=True)[:,5:].astype(float)\n",
    "\n",
    "        x = np.hstack([vis,np.nan_to_num(tex)])\n",
    "\n",
    "        node_type_idx = np.load(osp.join(self.raw_dir, 'node_types.npy'))\n",
    "        node_type_idx = torch.from_numpy(node_type_idx).to(torch.long)\n",
    "\n",
    "        data.num_nodes = len(node_type_idx)\n",
    "\n",
    "        data.x = torch.from_numpy(x).to(torch.float)\n",
    "\n",
    "\n",
    "        y_s = np.load(osp.join(self.raw_dir, 'labels.npz'), allow_pickle=True)\n",
    "        att_lab = y_s['ATT_LAB'][:,1:10].astype(float)\n",
    "        val_lab = np.nan_to_num(y_s['VAL_LAB'][:,2:13].astype(float))\n",
    "        ys = np.hstack([att_lab, val_lab])\n",
    "\n",
    "        data.y = torch.from_numpy(ys).to(torch.float)\n",
    "\n",
    "        data.node_type = node_type_idx\n",
    "        \n",
    "        data.att_lab = torch.tensor(y_s['ATT_LAB'][:,-1].astype(bool))\n",
    "        data.val_lab = torch.tensor(y_s['VAL_LAB'][:,-1].astype(bool))\n",
    "\n",
    "        split = np.load(osp.join(self.raw_dir, 'train_val_test_idx.npz'))\n",
    "        for name in ['train', 'val', 'test']:\n",
    "            idx = split[f'{name}_idx']\n",
    "            idx = torch.from_numpy(idx).to(torch.long)\n",
    "            mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "            mask[idx] = True\n",
    "            data[f'{name}_mask'] = mask\n",
    "                    \n",
    "        s = {}\n",
    "        \n",
    "        for link in link_types:\n",
    "            A_sub = sp.load_npz(osp.join(self.raw_dir, f'A_{link}.npz')).tocoo()\n",
    "            if A_sub.nnz>0:\n",
    "                row = torch.from_numpy(A_sub.row).to(torch.long)\n",
    "                col = torch.from_numpy(A_sub.col).to(torch.long)\n",
    "                data.edge_index = torch.stack([row, col], dim=0)\n",
    "                data.edge_attr = torch.from_numpy(A_sub.data).to(torch.long)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VEN_XL(InMemoryDataset):\n",
    "    r\"\"\"A large subset of Flickr post collected in Venice annotated with Heritage \n",
    "    Values and Attributes, as collected in the `\"Heri-Graphs: A Workflow of \n",
    "    Creating Datasets for Multi-modal Machine Learning on Graphs of Heritage \n",
    "    Values and Attributes with Social Media\" <https://arxiv.org/abs/2205.07545>`\n",
    "    paper.\n",
    "    VEN_XL is a heterogeneous graph containing two types of nodes - nodes with only \n",
    "    visual features 'vis_only' (31,140 nodes), nodes with both visual and textual\n",
    "    features 'vis_tex' (49,823 nodes) and four types of links - social similarity\n",
    "    'SOC' (76,422,265 links), spatial similarity (202,173,159 links), temporal similarity\n",
    "    (71,135,671 links), and simple composed link (290,091,503 links).\n",
    "    Vis_only nodes are represented with 982-dimensional visual features and are\n",
    "    divided into 9 heritage attribute categories \n",
    "    ('architectural elements', 'form', 'gastronomy', 'interior',\n",
    "    'landscape scenery and natural features', 'monuments', 'people', 'product', \n",
    "    'urban scenery').\n",
    "    Vis_text nodes are represented with 1753-dimensional visual and textual \n",
    "    features and are divided into 9 heritage attribute categories plus 11 \n",
    "    heritage value categories ('criterion i-x', 'other').\n",
    "    Both types of nodes are also merged into a single type of node 'all' with \n",
    "    1753-dimensional features and 20-dimensional label categories.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.HeteroData` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            every access. (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.HeteroData` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "    \n",
    "    Stats:\n",
    "            * - #nodes\n",
    "              - #edges\n",
    "              - #features\n",
    "              - #classes\n",
    "            * - 80,963\n",
    "              - 290,091,503\n",
    "              - 1753\n",
    "              - 20\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://drive.google.com/uc?export=download&id=1QZ5tyUWs6jYjh7mJrsnpou76iy-vb0CA'\n",
    "\n",
    "    def __init__(self, root: str, transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return [\n",
    "            'A_simp.npz', 'A_SOC.npz', 'A_SPA.npz', 'A_TEM.npz', 'labels.npz',\n",
    "            'node_types.npy', 'Textual_Features.npy', 'train_val_test_idx.npz',\n",
    "            'Visual_Features.npy'\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        path = download_url(self.url, self.raw_dir)\n",
    "        extract_zip(path, self.raw_dir)\n",
    "        os.remove(path)\n",
    "\n",
    "    def process(self):\n",
    "        data = HeteroData()\n",
    "\n",
    "        node_types = ['vis_only', 'vis_tex']\n",
    "        link_types = ['SOC', 'SPA', 'TEM', 'simp']\n",
    "\n",
    "        vis = np.load(osp.join(self.raw_dir, 'Visual_Features.npy'),allow_pickle=True)[:,2:].astype(float)\n",
    "        tex = np.load(osp.join(self.raw_dir, 'Textual_Features.npy'),allow_pickle=True)[:,5:].astype(float)\n",
    "\n",
    "        x = np.hstack([vis,np.nan_to_num(tex)])\n",
    "\n",
    "\n",
    "        node_type_idx = np.load(osp.join(self.raw_dir, 'node_types.npy'))\n",
    "        node_type_idx = torch.from_numpy(node_type_idx).to(torch.long)\n",
    "\n",
    "        data['vis_only'].num_nodes = int((node_type_idx == 0).sum())\n",
    "        data['vis_tex'].num_nodes = int((node_type_idx == 1).sum())\n",
    "        data['all'].num_nodes = len(node_type_idx)\n",
    "\n",
    "        data['vis_only'].x = torch.from_numpy(vis[node_type_idx==0]).to(torch.float)\n",
    "        data['vis_tex'].x = torch.from_numpy(x[node_type_idx==1]).to(torch.float)\n",
    "        data['all'].x = torch.from_numpy(x).to(torch.float)\n",
    "\n",
    "\n",
    "        y_s = np.load(osp.join(self.raw_dir, 'labels.npz'), allow_pickle=True)\n",
    "        att_lab = y_s['ATT_LAB'][:,1:10].astype(float)\n",
    "        val_lab = np.nan_to_num(y_s['VAL_LAB'][:,2:13].astype(float))\n",
    "        ys = np.hstack([att_lab, val_lab])\n",
    "\n",
    "        data['vis_only'].y = torch.from_numpy(att_lab[node_type_idx==0]).to(torch.float)\n",
    "        data['vis_tex'].y = torch.from_numpy(ys[node_type_idx==1]).to(torch.float)\n",
    "        data['all'].y = torch.from_numpy(ys).to(torch.float)\n",
    "        \n",
    "        data['all'].att_lab = torch.tensor(y_s['ATT_LAB'][:,-1].astype(bool))\n",
    "        data['all'].val_lab = torch.tensor(y_s['VAL_LAB'][:,-1].astype(bool))\n",
    "        data['all'].node_type = node_type_idx\n",
    "\n",
    "        data.node_type = node_type_idx\n",
    "\n",
    "        split = np.load(osp.join(self.raw_dir, 'train_val_test_idx.npz'))\n",
    "        for name in ['train', 'val', 'test']:\n",
    "            idx = split[f'{name}_idx']\n",
    "            idx = torch.from_numpy(idx).to(torch.long)\n",
    "            mask = torch.zeros(data['all'].num_nodes, dtype=torch.bool)\n",
    "            mask[idx] = True\n",
    "            data['all'][f'{name}_mask'] = mask\n",
    "            data['vis_only'][f'{name}_mask'] = mask[node_type_idx==0]\n",
    "            data['vis_tex'][f'{name}_mask'] = mask[node_type_idx==1]\n",
    "\n",
    "        \n",
    "        s = {}\n",
    "        s['vis_only'] = np.arange(len(x))[node_type_idx==0]\n",
    "        s['vis_tex'] = np.arange(len(x))[node_type_idx==1]\n",
    "\n",
    "        for link in link_types:\n",
    "            A_sub = sp.load_npz(osp.join(self.raw_dir, f'A_{link}.npz')).tocoo()\n",
    "            if A_sub.nnz>0:\n",
    "                row = torch.from_numpy(A_sub.row).to(torch.long)\n",
    "                col = torch.from_numpy(A_sub.col).to(torch.long)\n",
    "                data['all', f'{link}_link', 'all'].edge_index = torch.stack([row, col], dim=0)\n",
    "                data['all', f'{link}_link', 'all'].edge_attr = torch.from_numpy(A_sub.data).to(torch.long)\n",
    "\n",
    "        for src, dst in product(node_types, node_types):\n",
    "            for link in link_types:\n",
    "                A_sub = sp.load_npz(osp.join(self.raw_dir, f'A_{link}.npz'))[s[src]][:,s[dst]].tocoo()\n",
    "                if A_sub.nnz>0:\n",
    "                    row = torch.from_numpy(A_sub.row).to(torch.long)\n",
    "                    col = torch.from_numpy(A_sub.col).to(torch.long)\n",
    "                    data[src, f'{link}_link', dst].edge_index = torch.stack([row, col], dim=0)\n",
    "                    data[src, f'{link}_link', dst].edge_attr = torch.from_numpy(A_sub.data).to(torch.long)\n",
    "\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VEN_XL_Homo(InMemoryDataset):\n",
    "    r\"\"\"A large subset of Flickr post collected in Venice annotated with Heritage \n",
    "    Values and Attributes, as collected in the `\"Heri-Graphs: A Workflow of \n",
    "    Creating Datasets for Multi-modal Machine Learning on Graphs of Heritage \n",
    "    Values and Attributes with Social Media\" <https://arxiv.org/abs/2205.07545>`\n",
    "    paper.\n",
    "    VEN_XL is a heterogeneous graph containing two types of nodes - nodes with only \n",
    "    visual features 'vis_only' (31,140 nodes), nodes with both visual and textual\n",
    "    features 'vis_tex' (49,823 nodes) and four types of links - social similarity\n",
    "    'SOC' (76,422,265 links), spatial similarity (202,173,159 links), temporal similarity\n",
    "    (71,135,671 links), and simple composed link (290,091,503 links).\n",
    "    Vis_only nodes are represented with 982-dimensional visual features and are\n",
    "    divided into 9 heritage attribute categories \n",
    "    ('architectural elements', 'form', 'gastronomy', 'interior',\n",
    "    'landscape scenery and natural features', 'monuments', 'people', 'product', \n",
    "    'urban scenery').\n",
    "    Vis_text nodes are represented with 1753-dimensional visual and textual \n",
    "    features and are divided into 9 heritage attribute categories plus 11 \n",
    "    heritage value categories ('criterion i-x', 'other').\n",
    "    Both types of nodes are also merged into a single type of node 'all' with \n",
    "    1753-dimensional features and 20-dimensional label categories.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.HeteroData` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            every access. (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.HeteroData` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "    \n",
    "    Stats:\n",
    "            * - #nodes\n",
    "              - #edges\n",
    "              - #features\n",
    "              - #classes\n",
    "            * - 80,963\n",
    "              - 290,091,503\n",
    "              - 1753\n",
    "              - 20\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://drive.google.com/uc?export=download&id=1sxcKiZr1YGDv06wr03nsk5HVZledgzi9'\n",
    "\n",
    "    def __init__(self, root: str, transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return [\n",
    "            'A_simp.npz', 'A_SOC.npz', 'A_SPA.npz', 'A_TEM.npz', 'labels.npz',\n",
    "            'node_types.npy', 'Textual_Features.npy', 'train_val_test_idx.npz',\n",
    "            'Visual_Features.npy'\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        path = download_url(self.url, self.raw_dir)\n",
    "        extract_zip(path, self.raw_dir)\n",
    "        os.remove(path)\n",
    "\n",
    "    def process(self):\n",
    "        data = Data()\n",
    "\n",
    "        link_types = ['simp']\n",
    "\n",
    "        vis = np.load(osp.join(self.raw_dir, 'Visual_Features.npy'),allow_pickle=True)[:,2:].astype(float)\n",
    "        tex = np.load(osp.join(self.raw_dir, 'Textual_Features.npy'),allow_pickle=True)[:,5:].astype(float)\n",
    "\n",
    "        x = np.hstack([vis,np.nan_to_num(tex)])\n",
    "\n",
    "        node_type_idx = np.load(osp.join(self.raw_dir, 'node_types.npy'))\n",
    "        node_type_idx = torch.from_numpy(node_type_idx).to(torch.long)\n",
    "\n",
    "        data.num_nodes = len(node_type_idx)\n",
    "\n",
    "        data.x = torch.from_numpy(x).to(torch.float)\n",
    "\n",
    "\n",
    "        y_s = np.load(osp.join(self.raw_dir, 'labels.npz'), allow_pickle=True)\n",
    "        att_lab = y_s['ATT_LAB'][:,1:10].astype(float)\n",
    "        val_lab = np.nan_to_num(y_s['VAL_LAB'][:,2:13].astype(float))\n",
    "        ys = np.hstack([att_lab, val_lab])\n",
    "\n",
    "        data.y = torch.from_numpy(ys).to(torch.float)\n",
    "        \n",
    "        data.att_lab = torch.tensor(y_s['ATT_LAB'][:,-1].astype(bool))\n",
    "        data.val_lab = torch.tensor(y_s['VAL_LAB'][:,-1].astype(bool))\n",
    "\n",
    "        data.node_type = node_type_idx\n",
    "\n",
    "        split = np.load(osp.join(self.raw_dir, 'train_val_test_idx.npz'))\n",
    "        for name in ['train', 'val', 'test']:\n",
    "            idx = split[f'{name}_idx']\n",
    "            idx = torch.from_numpy(idx).to(torch.long)\n",
    "            mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "            mask[idx] = True\n",
    "            data[f'{name}_mask'] = mask\n",
    "                    \n",
    "        s = {}\n",
    "        \n",
    "        for link in link_types:\n",
    "            A_sub = sp.load_npz(osp.join(self.raw_dir, f'A_{link}.npz')).tocoo()\n",
    "            if A_sub.nnz>0:\n",
    "                row = torch.from_numpy(A_sub.row).to(torch.long)\n",
    "                col = torch.from_numpy(A_sub.col).to(torch.long)\n",
    "                data.edge_index = torch.stack([row, col], dim=0)\n",
    "                data.edge_attr = torch.from_numpy(A_sub.data).to(torch.long)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = osp.join(os.getcwd(), '../../data/VEN')\n",
    "transform = T.NormalizeFeatures()\n",
    "dataset = VEN_Homo('dataset/Venice_homo')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=2951, x=[2951, 1753], y=[2951, 20], node_type=[2951], att_lab=[2951], val_lab=[2951], train_mask=[2951], val_mask=[2951], test_mask=[2951], edge_index=[2, 1071977], edge_attr=[1071977], n_id=[2951])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.n_id = torch.arange(data.num_nodes)\n",
    "data = data.to(device)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2861, 0.3745, 1.1453,  ..., 1.0000, 0.0000, 0.0000],\n",
       "        [0.3977, 0.1582, 0.3059,  ..., 1.0000, 0.0000, 0.0000],\n",
       "        [0.5185, 0.5124, 1.3662,  ..., 1.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0124, 1.7083, 0.3258,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        [0.4402, 0.8374, 0.4974,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        [0.2102, 1.5535, 0.3352,  ..., 0.0000, 1.0000, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader for Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.train_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "val_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.val_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.test_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=2942, x=[2942, 1753], y=[2942, 20], node_type=[2942], att_lab=[2942], val_lab=[2942], train_mask=[2942], val_mask=[2942], test_mask=[2942], edge_index=[2, 103612], edge_attr=[103612], n_id=[2942], batch_size=32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data = next(iter(train_loader))\n",
    "batch = sampled_data\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(360, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['train_mask'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,  28,  30,  34,  35,  42,  46,  47,  62,  64,  94,  95, 100, 101,\n",
       "        104, 106, 110, 120, 127, 151, 158, 160, 165, 178, 179, 199, 222, 245,\n",
       "        251, 252, 260, 261], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.n_id[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2942, 20])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_ATT_acc_val': 0,\n",
    "            'early_stopping_best_VAL_acc_val': 0,\n",
    "            'early_stopping_best_ATT_acc_val_2': 0,\n",
    "            'early_stopping_lowest_loss': 1000,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_ATT_loss': [],\n",
    "            'train_VAL_loss':[],\n",
    "            'train_ATT_acc': [],\n",
    "            'train_VAL_acc': [],\n",
    "            'train_VAL_jac': [],\n",
    "            'train_VAL_acc_1':[], \n",
    "            'val_loss': [],\n",
    "            'val_ATT_loss': [],\n",
    "            'val_VAL_loss':[],\n",
    "            'val_ATT_acc': [],\n",
    "            'val_VAL_acc': [],\n",
    "            'val_VAL_jac': [],\n",
    "            'val_VAL_acc_1': [],\n",
    "            'test_loss': -1,\n",
    "            'test_ATT_loss': -1,\n",
    "            'test_VAL_loss':-1,\n",
    "            'test_ATT_acc': -1,\n",
    "            'test_VAL_acc': -1,\n",
    "            'test_VAL_jac': -1,\n",
    "            'test_VAL_acc_1': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        ATT_acc_tm1, ATT_acc_t = train_state['val_ATT_acc'][-2:]\n",
    "        #ATT_acc_2_tm1, ATT_acc_2_t = train_state['val_ATT_acc_2'][-2:]\n",
    "        VAL_acc_tm1, VAL_acc_t = train_state['val_VAL_acc'][-2:]\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If accuracy worsened\n",
    "        #if loss_t >= train_state['early_stopping_lowest_loss']:\n",
    "        #    train_state['early_stopping_step'] += 1\n",
    "        \n",
    "        if ATT_acc_t <= train_state['early_stopping_best_ATT_acc_val'] and VAL_acc_t <= train_state['early_stopping_best_VAL_acc_val']:# and ATT_acc_2_t <= train_state['early_stopping_best_ATT_acc_val_2']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model from sklearn\n",
    "            if VAL_acc_t > train_state['early_stopping_best_VAL_acc_val']:\n",
    "                train_state['early_stopping_best_VAL_acc_val'] = VAL_acc_t\n",
    "                \n",
    "            if ATT_acc_t > train_state['early_stopping_best_ATT_acc_val']:\n",
    "                train_state['early_stopping_best_ATT_acc_val'] = ATT_acc_t\n",
    "            \n",
    "            #if ATT_acc_2_t > train_state['early_stopping_best_ATT_acc_val_2']:\n",
    "            #    train_state['early_stopping_best_ATT_acc_val_2'] = ATT_acc_2_t\n",
    "                \n",
    "            if loss_t < train_state['early_stopping_lowest_loss']:\n",
    "                train_state['early_stopping_lowest_loss'] = loss_t\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                \n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_entropy(y_pred, y_target):\n",
    "    y_target = y_target.cpu().float()\n",
    "    y_pred = y_pred.cpu().float()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    return criterion(y_target, y_pred)\n",
    "\n",
    "def compute_1_accuracy(y_pred, y_target):\n",
    "    y_target_indices = y_target.max(dim=1)[1]\n",
    "    y_pred_indices = y_pred.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target_indices).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "def compute_k_accuracy(y_pred, y_target, k=3):\n",
    "    y_pred_indices = y_pred.topk(k, dim=1)[1]\n",
    "    y_target_indices = y_target.max(dim=1)[1]\n",
    "    n_correct = torch.tensor([y_pred_indices[i] in y_target_indices[i] for i in range(len(y_pred))]).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "def compute_k_jaccard_index(y_pred, y_target, k=3):\n",
    "    y_target_indices = y_target.topk(k, dim=1)[1]\n",
    "    y_pred_indices = y_pred.max(dim=1)[1]\n",
    "    jaccard = torch.tensor([len(np.intersect1d(y_target_indices[i], y_pred_indices[i]))/\n",
    "                            len(np.union1d(y_target_indices[i], y_pred_indices[i]))\n",
    "                            for i in range(len(y_pred))]).sum().item()\n",
    "    return jaccard / len(y_pred_indices)\n",
    "\n",
    "def compute_jaccard_index(y_pred, y_target, k=3, multilabel=False):\n",
    "    \n",
    "    threshold = 1.0/(k+1)\n",
    "    threshold_2 = 0.5\n",
    "    \n",
    "    if multilabel:\n",
    "        y_pred_indices = y_pred.gt(threshold_2)\n",
    "    else:\n",
    "        y_pred_indices = y_pred.gt(threshold)\n",
    "    \n",
    "    y_target_indices = y_target.gt(threshold)\n",
    "        \n",
    "    jaccard = ((y_target_indices*y_pred_indices).sum(axis=1)/((y_target_indices+y_pred_indices).sum(axis=1)+1e-8)).sum().item()\n",
    "    return jaccard / len(y_pred_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred, soft_targets):\n",
    "    logsoftmax = nn.LogSoftmax(dim=1)\n",
    "    return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searched Best Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.save_dir+'42/hyperdict.p', 'rb') as fp:\n",
    "    hyperdict= pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hyperdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(0.1, 2, 32, 0.01), (0.1, 2, 32, 0.001), (0.1, 2, 32, 0.0001), (0.1, 2, 64, 0.01), (0.1, 2, 64, 0.001), (0.1, 2, 64, 0.0001), (0.1, 2, 128, 0.01), (0.1, 2, 128, 0.001), (0.1, 2, 128, 0.0001), (0.1, 2, 256, 0.01), (0.1, 2, 256, 0.001), (0.1, 2, 256, 0.0001), (0.1, 2, 512, 0.01), (0.1, 2, 512, 0.001), (0.1, 2, 512, 0.0001), (0.1, 5, 32, 0.01), (0.1, 5, 32, 0.001), (0.1, 5, 32, 0.0001), (0.1, 5, 64, 0.01), (0.1, 5, 64, 0.001), (0.1, 5, 64, 0.0001), (0.1, 5, 128, 0.01), (0.1, 5, 128, 0.001), (0.1, 5, 128, 0.0001), (0.1, 5, 256, 0.01), (0.1, 5, 256, 0.001), (0.1, 5, 256, 0.0001), (0.1, 5, 512, 0.01), (0.1, 5, 512, 0.001), (0.1, 5, 512, 0.0001), (0.1, 8, 32, 0.01), (0.1, 8, 32, 0.001), (0.1, 8, 32, 0.0001), (0.1, 8, 64, 0.01), (0.1, 8, 64, 0.001), (0.1, 8, 64, 0.0001), (0.1, 8, 128, 0.01), (0.1, 8, 128, 0.001), (0.1, 8, 128, 0.0001), (0.1, 8, 256, 0.01), (0.1, 8, 256, 0.001), (0.1, 8, 256, 0.0001), (0.3, 2, 32, 0.01), (0.3, 2, 32, 0.001), (0.3, 2, 32, 0.0001), (0.3, 2, 64, 0.01), (0.3, 2, 64, 0.001), (0.3, 2, 64, 0.0001), (0.3, 2, 128, 0.01), (0.3, 2, 128, 0.001), (0.3, 2, 128, 0.0001), (0.3, 2, 256, 0.01), (0.3, 2, 256, 0.001), (0.3, 2, 256, 0.0001), (0.3, 5, 32, 0.01), (0.3, 5, 32, 0.001), (0.3, 5, 32, 0.0001), (0.3, 5, 64, 0.01), (0.3, 5, 64, 0.001), (0.3, 5, 64, 0.0001), (0.3, 5, 128, 0.01), (0.3, 5, 128, 0.001), (0.3, 5, 128, 0.0001), (0.3, 5, 256, 0.01), (0.3, 5, 256, 0.001), (0.3, 5, 256, 0.0001), (0.3, 8, 32, 0.01), (0.3, 8, 32, 0.001), (0.3, 8, 32, 0.0001), (0.3, 8, 64, 0.01), (0.3, 8, 64, 0.001), (0.3, 8, 64, 0.0001), (0.3, 8, 128, 0.01), (0.3, 8, 128, 0.001), (0.3, 8, 128, 0.0001), (0.3, 8, 256, 0.01), (0.3, 8, 256, 0.001), (0.3, 8, 256, 0.0001), (0.6, 2, 32, 0.01), (0.6, 2, 32, 0.001), (0.6, 2, 32, 0.0001), (0.6, 2, 64, 0.01), (0.6, 2, 64, 0.001), (0.6, 2, 64, 0.0001), (0.6, 2, 128, 0.01), (0.6, 2, 128, 0.001), (0.6, 2, 128, 0.0001), (0.6, 2, 256, 0.01), (0.6, 2, 256, 0.001), (0.6, 2, 256, 0.0001), (0.6, 5, 32, 0.01), (0.6, 5, 32, 0.001), (0.6, 5, 32, 0.0001), (0.6, 5, 64, 0.01), (0.6, 5, 64, 0.001), (0.6, 5, 64, 0.0001), (0.6, 5, 128, 0.01), (0.6, 5, 128, 0.001), (0.6, 5, 128, 0.0001), (0.6, 5, 256, 0.01), (0.6, 5, 256, 0.001), (0.6, 5, 256, 0.0001), (0.6, 8, 32, 0.01), (0.6, 8, 32, 0.001), (0.6, 8, 32, 0.0001), (0.6, 8, 64, 0.01), (0.6, 8, 64, 0.001), (0.6, 8, 64, 0.0001), (0.6, 8, 128, 0.01), (0.6, 8, 128, 0.001), (0.6, 8, 128, 0.0001), (0.6, 8, 256, 0.01), (0.6, 8, 256, 0.001), (0.6, 8, 256, 0.0001)])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperdict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_df = pd.DataFrame(hyperdict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': True,\n",
       " 'early_stopping_step': 100,\n",
       " 'early_stopping_best_ATT_acc_val': 86.33093525179856,\n",
       " 'early_stopping_best_VAL_acc_val': 96.40287769784173,\n",
       " 'early_stopping_best_ATT_acc_val_2': 0,\n",
       " 'early_stopping_lowest_loss': 4.250159640792463,\n",
       " 'learning_rate': 0.001,\n",
       " 'epoch_index': 125,\n",
       " 'train_loss': [4.115122119585673,\n",
       "  3.487265487511953,\n",
       "  3.1288148363431296,\n",
       "  2.8914007544517517,\n",
       "  2.700275182723999,\n",
       "  2.626474976539612,\n",
       "  2.6187708576520285,\n",
       "  2.5704930623372397,\n",
       "  2.5644010106722512,\n",
       "  2.6033581495285034,\n",
       "  2.593102812767029,\n",
       "  2.555069386959076,\n",
       "  2.542228897412618,\n",
       "  2.585222144921621,\n",
       "  2.545814832051595,\n",
       "  2.5474761724472046,\n",
       "  2.530776262283325,\n",
       "  2.559297184149424,\n",
       "  2.5298489928245544,\n",
       "  2.5131618976593018,\n",
       "  2.520117243131002,\n",
       "  2.495916942755381,\n",
       "  2.51517790555954,\n",
       "  2.4745981891949973,\n",
       "  2.4969033002853394,\n",
       "  2.4866838653882346,\n",
       "  2.4931185841560364,\n",
       "  2.491485377152761,\n",
       "  2.4887433449427285,\n",
       "  2.4877787232398987,\n",
       "  2.4769266843795776,\n",
       "  2.5029664238293967,\n",
       "  2.5056411822636924,\n",
       "  2.4718963503837585,\n",
       "  2.5082231163978577,\n",
       "  2.465532422065735,\n",
       "  2.4790549675623574,\n",
       "  2.4590170979499817,\n",
       "  2.4948864579200745,\n",
       "  2.4657257397969565,\n",
       "  2.4626083175341287,\n",
       "  2.4756033023198447,\n",
       "  2.467275083065033,\n",
       "  2.4580281376838684,\n",
       "  2.4526090025901794,\n",
       "  2.47102427482605,\n",
       "  2.501132527987162,\n",
       "  2.44733589887619,\n",
       "  2.4443086783091226,\n",
       "  2.459414839744568,\n",
       "  2.437439481417338,\n",
       "  2.459737797578176,\n",
       "  2.4291546742121377,\n",
       "  2.441383878389994,\n",
       "  2.440361042817434,\n",
       "  2.457509378592173,\n",
       "  2.4649165074030557,\n",
       "  2.4434433579444885,\n",
       "  2.4610459407170615,\n",
       "  2.439304292201996,\n",
       "  2.4617759784062705,\n",
       "  2.4780028462409973,\n",
       "  2.457951029141744,\n",
       "  2.4669955372810364,\n",
       "  2.4378308057785034,\n",
       "  2.449029803276062,\n",
       "  2.4505229393641152,\n",
       "  2.442183335622152,\n",
       "  2.455494145552317,\n",
       "  2.4289334217707315,\n",
       "  2.4441307187080383,\n",
       "  2.451918661594391,\n",
       "  2.4397411346435547,\n",
       "  2.450321137905121,\n",
       "  2.450182934602102,\n",
       "  2.427260160446167,\n",
       "  2.4552537401517234,\n",
       "  2.439291258653005,\n",
       "  2.4441404740015664,\n",
       "  2.423162798086802,\n",
       "  2.450102468331655,\n",
       "  2.4218836426734924,\n",
       "  2.4624963998794556,\n",
       "  2.4352348248163858,\n",
       "  2.438288986682892,\n",
       "  2.4358774026234946,\n",
       "  2.438818335533142,\n",
       "  2.4355412125587463,\n",
       "  2.4207194447517395,\n",
       "  2.449324091275533,\n",
       "  2.4262516498565674,\n",
       "  2.4527310331662497,\n",
       "  2.4364274740219116,\n",
       "  2.4519628087679544,\n",
       "  2.4378074606259665,\n",
       "  2.454179326693217,\n",
       "  2.4113530119260154,\n",
       "  2.429544428984324,\n",
       "  2.4392587741216025,\n",
       "  2.453466514746348,\n",
       "  2.429205596446991,\n",
       "  2.442771772543589,\n",
       "  2.4536256988843284,\n",
       "  2.442746122678121,\n",
       "  2.407721777757009,\n",
       "  2.426638921101888,\n",
       "  2.4113823771476746,\n",
       "  2.428186515967051,\n",
       "  2.4175403118133545,\n",
       "  2.432513733704885,\n",
       "  2.4318822423617044,\n",
       "  2.42503430445989,\n",
       "  2.428997258345286,\n",
       "  2.428327997525533,\n",
       "  2.4233654538790383,\n",
       "  2.4368362625439963,\n",
       "  2.425381282965342,\n",
       "  2.4182590444882712,\n",
       "  2.42344331741333,\n",
       "  2.430448373158773,\n",
       "  2.4245638251304626,\n",
       "  2.452707270781199,\n",
       "  2.440020720163981,\n",
       "  2.4188085794448853,\n",
       "  2.4432520270347595,\n",
       "  2.4248443643252053],\n",
       " 'train_ATT_loss': [1.8476570999853499,\n",
       "  1.4783387055357406,\n",
       "  1.234674311740907,\n",
       "  1.0121129916315263,\n",
       "  0.9158724695031333,\n",
       "  0.8645829173666618,\n",
       "  0.8412208053543957,\n",
       "  0.8313146362344314,\n",
       "  0.8171178023240573,\n",
       "  0.8382430766758165,\n",
       "  0.8407226559858243,\n",
       "  0.7904984667030398,\n",
       "  0.8196662515814614,\n",
       "  0.7873272643194965,\n",
       "  0.8034263523992079,\n",
       "  0.7981078454662228,\n",
       "  0.8200442079361786,\n",
       "  0.832043457889821,\n",
       "  0.8305580033159652,\n",
       "  0.7725204884840841,\n",
       "  0.7782831187063307,\n",
       "  0.7769166761157915,\n",
       "  0.779415416090112,\n",
       "  0.7843956194425884,\n",
       "  0.7701168773577154,\n",
       "  0.7603459934448602,\n",
       "  0.7688290712906053,\n",
       "  0.7808409140050576,\n",
       "  0.7670474602244898,\n",
       "  0.7780894331654683,\n",
       "  0.77858134996858,\n",
       "  0.7847839475008259,\n",
       "  0.7715275309092451,\n",
       "  0.7673577307994346,\n",
       "  0.7655603240071241,\n",
       "  0.7701948801896579,\n",
       "  0.7696918736204216,\n",
       "  0.7691418901705015,\n",
       "  0.7519521323597662,\n",
       "  0.7598460052482309,\n",
       "  0.7621317727083645,\n",
       "  0.764670958149136,\n",
       "  0.7634094674501393,\n",
       "  0.7480430527076827,\n",
       "  0.7615025604860934,\n",
       "  0.7620167251769195,\n",
       "  0.7639280580747821,\n",
       "  0.751814366212512,\n",
       "  0.7688174029796738,\n",
       "  0.7510514743110149,\n",
       "  0.7479744517241819,\n",
       "  0.750117371287042,\n",
       "  0.7569978687901906,\n",
       "  0.7565386869900774,\n",
       "  0.7562349831628667,\n",
       "  0.7457167623445928,\n",
       "  0.7535755001937253,\n",
       "  0.754885762352032,\n",
       "  0.7499935689725374,\n",
       "  0.7500941474351857,\n",
       "  0.7465287679780553,\n",
       "  0.7466078648276606,\n",
       "  0.7500730533348886,\n",
       "  0.7422678173413898,\n",
       "  0.7465195132424627,\n",
       "  0.7524392046756692,\n",
       "  0.7563781338715487,\n",
       "  0.7451802437655484,\n",
       "  0.7644229060725162,\n",
       "  0.7481514681079051,\n",
       "  0.7509441666325704,\n",
       "  0.7417042131569247,\n",
       "  0.7467294817485968,\n",
       "  0.7397430615081682,\n",
       "  0.7567335573590033,\n",
       "  0.7706258549584576,\n",
       "  0.7595398038047833,\n",
       "  0.74678029726747,\n",
       "  0.7443689566900196,\n",
       "  0.7479223097130202,\n",
       "  0.7398240543138287,\n",
       "  0.7502271810396887,\n",
       "  0.74083970045449,\n",
       "  0.7464309096996804,\n",
       "  0.7431278161064739,\n",
       "  0.7441472845394526,\n",
       "  0.7431268741549547,\n",
       "  0.741899387327918,\n",
       "  0.7518026132002431,\n",
       "  0.7513635328601932,\n",
       "  0.7455662006816706,\n",
       "  0.7465872929697221,\n",
       "  0.7490081793713768,\n",
       "  0.7548430933516441,\n",
       "  0.742336243308482,\n",
       "  0.7363104173020973,\n",
       "  0.7513728546303725,\n",
       "  0.7375365639327306,\n",
       "  0.7494189455238406,\n",
       "  0.7435455026719049,\n",
       "  0.7508071921868998,\n",
       "  0.757657225442395,\n",
       "  0.74807511257663,\n",
       "  0.7484541650954376,\n",
       "  0.7454524517389546,\n",
       "  0.7478285141268596,\n",
       "  0.7437412344187581,\n",
       "  0.7403458053055232,\n",
       "  0.7585845838953583,\n",
       "  0.7498810704064831,\n",
       "  0.7399333205249501,\n",
       "  0.7391858672171088,\n",
       "  0.7421637132887695,\n",
       "  0.7344038422087883,\n",
       "  0.7503005964604111,\n",
       "  0.7394314310887514,\n",
       "  0.7402244372050848,\n",
       "  0.7425692188772799,\n",
       "  0.7382371881331763,\n",
       "  0.7444037388896678,\n",
       "  0.7486609801691325,\n",
       "  0.7435565939240179,\n",
       "  0.7405634515833657,\n",
       "  0.7375465472979559,\n",
       "  0.7407370990663354,\n",
       "  0.738456343844987],\n",
       " 'train_VAL_loss': [1.7696744593226679,\n",
       "  1.71304753975855,\n",
       "  1.6937502688648298,\n",
       "  1.7123740151978595,\n",
       "  1.6613689178905329,\n",
       "  1.649919588810189,\n",
       "  1.663447563668037,\n",
       "  1.6536876177853825,\n",
       "  1.6538309410338257,\n",
       "  1.6645594831979176,\n",
       "  1.6436291219785273,\n",
       "  1.650754201445223,\n",
       "  1.6725824453824114,\n",
       "  1.6521087822161222,\n",
       "  1.6436488928887323,\n",
       "  1.658690788409056,\n",
       "  1.649772637438576,\n",
       "  1.6378880240580382,\n",
       "  1.6405379554241317,\n",
       "  1.651471289901522,\n",
       "  1.6559865296382323,\n",
       "  1.6431862799414638,\n",
       "  1.631332201310472,\n",
       "  1.6303383950711618,\n",
       "  1.6299676885235013,\n",
       "  1.6263603741442398,\n",
       "  1.6305719792347535,\n",
       "  1.6365900019859674,\n",
       "  1.6274127388925104,\n",
       "  1.6339400425512045,\n",
       "  1.6213760438718294,\n",
       "  1.6289328934413245,\n",
       "  1.6288832987444553,\n",
       "  1.6207740528101406,\n",
       "  1.6232874284490655,\n",
       "  1.6221721423299689,\n",
       "  1.6184215456494995,\n",
       "  1.6227959305295654,\n",
       "  1.6191535537592923,\n",
       "  1.6201982432125017,\n",
       "  1.6181867829320173,\n",
       "  1.6176166184391012,\n",
       "  1.6172998076991032,\n",
       "  1.614745724564444,\n",
       "  1.6213364984190035,\n",
       "  1.621608045292693,\n",
       "  1.6292577238954666,\n",
       "  1.6179931688837068,\n",
       "  1.6167721824302568,\n",
       "  1.616701824182949,\n",
       "  1.6206539255099943,\n",
       "  1.6148712278403075,\n",
       "  1.6196845911876647,\n",
       "  1.6210565794868153,\n",
       "  1.6309062044376152,\n",
       "  1.6219612980813531,\n",
       "  1.6151249765359132,\n",
       "  1.6144793443072205,\n",
       "  1.6128124388301142,\n",
       "  1.6137232575720366,\n",
       "  1.6167453742093327,\n",
       "  1.6214269809115296,\n",
       "  1.622208587020388,\n",
       "  1.6140545510849464,\n",
       "  1.615774055927414,\n",
       "  1.6135758597434722,\n",
       "  1.617838373805017,\n",
       "  1.6185213961429543,\n",
       "  1.6123124724279811,\n",
       "  1.612035951786094,\n",
       "  1.6124468193159869,\n",
       "  1.6125194012623414,\n",
       "  1.6121082778122287,\n",
       "  1.6107679410654423,\n",
       "  1.613010449092474,\n",
       "  1.6138724389168695,\n",
       "  1.612285117693555,\n",
       "  1.6126994148846148,\n",
       "  1.6123722619957541,\n",
       "  1.610641929581555,\n",
       "  1.610609025836321,\n",
       "  1.6122693847090914,\n",
       "  1.6110065534834717,\n",
       "  1.6100005317592885,\n",
       "  1.6089550606761942,\n",
       "  1.6204150337261507,\n",
       "  1.6169551227891874,\n",
       "  1.6151414247761142,\n",
       "  1.6098006967032055,\n",
       "  1.6102703862573302,\n",
       "  1.6099181720094338,\n",
       "  1.6098247650917878,\n",
       "  1.6091218575876505,\n",
       "  1.6119159430347982,\n",
       "  1.6137312049680799,\n",
       "  1.6097144476264467,\n",
       "  1.6120471336834978,\n",
       "  1.616067581229593,\n",
       "  1.612782996116913,\n",
       "  1.6116125821406821,\n",
       "  1.6151035168824763,\n",
       "  1.6173811398054425,\n",
       "  1.6139138071821006,\n",
       "  1.61538933419785,\n",
       "  1.613992332091292,\n",
       "  1.6137323148362854,\n",
       "  1.6132051782925043,\n",
       "  1.6111181706933104,\n",
       "  1.612081288300723,\n",
       "  1.6117514711998175,\n",
       "  1.6126104474397909,\n",
       "  1.6093999107788806,\n",
       "  1.6087649512489086,\n",
       "  1.6108979795778227,\n",
       "  1.6134461392326038,\n",
       "  1.6146942898837484,\n",
       "  1.6104863558449574,\n",
       "  1.610522165853231,\n",
       "  1.6110228729380134,\n",
       "  1.608497124629668,\n",
       "  1.6114203252950863,\n",
       "  1.6112838481601917,\n",
       "  1.6094328597642047,\n",
       "  1.608325598312547,\n",
       "  1.6085578084322225,\n",
       "  1.6081191793373086],\n",
       " 'train_ATT_acc': [19.113573407202217,\n",
       "  61.21883656509695,\n",
       "  69.25207756232687,\n",
       "  84.7645429362881,\n",
       "  92.5207756232687,\n",
       "  96.67590027700831,\n",
       "  95.8448753462604,\n",
       "  94.73684210526316,\n",
       "  96.67590027700831,\n",
       "  96.67590027700831,\n",
       "  96.39889196675901,\n",
       "  98.61495844875347,\n",
       "  96.95290858725762,\n",
       "  99.16897506925208,\n",
       "  98.89196675900277,\n",
       "  98.33795013850416,\n",
       "  96.67590027700831,\n",
       "  96.1218836565097,\n",
       "  95.56786703601108,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.16897506925208,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  98.89196675900277,\n",
       "  98.89196675900277,\n",
       "  98.89196675900277,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.16897506925208,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  99.16897506925208,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  99.16897506925208,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  98.89196675900277,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.16897506925208,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0],\n",
       " 'train_VAL_acc': [70.91412742382272,\n",
       "  95.01385041551247,\n",
       "  95.8448753462604,\n",
       "  86.70360110803324,\n",
       "  97.50692520775624,\n",
       "  97.78393351800554,\n",
       "  96.67590027700831,\n",
       "  95.8448753462604,\n",
       "  96.67590027700831,\n",
       "  90.85872576177286,\n",
       "  98.33795013850416,\n",
       "  97.78393351800554,\n",
       "  95.01385041551247,\n",
       "  96.39889196675901,\n",
       "  98.33795013850416,\n",
       "  95.01385041551247,\n",
       "  96.95290858725762,\n",
       "  99.16897506925208,\n",
       "  99.7229916897507,\n",
       "  98.61495844875347,\n",
       "  98.06094182825485,\n",
       "  98.33795013850416,\n",
       "  99.16897506925208,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.16897506925208,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  99.16897506925208,\n",
       "  98.33795013850416,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0],\n",
       " 'train_VAL_jac': [0.23397968614530695,\n",
       "  0.2725761861352049,\n",
       "  0.2622345418480955,\n",
       "  0.26872444615139524,\n",
       "  0.2661852090312503,\n",
       "  0.26883657024837926,\n",
       "  0.2664589122391804,\n",
       "  0.26604670698953137,\n",
       "  0.2681242613911298,\n",
       "  0.2680055395057657,\n",
       "  0.26943675897128033,\n",
       "  0.27096030520600295,\n",
       "  0.2746999105258001,\n",
       "  0.27391505241394043,\n",
       "  0.27825484909839576,\n",
       "  0.27903970423827873,\n",
       "  0.2798245706056294,\n",
       "  0.27682364019990957,\n",
       "  0.27497691725099516,\n",
       "  0.2781163437544804,\n",
       "  0.27885504112349324,\n",
       "  0.2803324210016351,\n",
       "  0.2789935372212587,\n",
       "  0.2769621422416286,\n",
       "  0.2803324160483405,\n",
       "  0.2773776609151317,\n",
       "  0.28250231009771287,\n",
       "  0.28153278061557674,\n",
       "  0.27950139124968043,\n",
       "  0.27608496000231797,\n",
       "  0.2769621468647035,\n",
       "  0.2761311220660434,\n",
       "  0.2812557768623585,\n",
       "  0.28084026281193025,\n",
       "  0.2790858745574951,\n",
       "  0.2847645503992519,\n",
       "  0.28596491232473104,\n",
       "  0.2875346301995486,\n",
       "  0.2838411869434769,\n",
       "  0.28139427626232033,\n",
       "  0.28559556463088354,\n",
       "  0.28088643478224484,\n",
       "  0.28185596228306314,\n",
       "  0.28665743111903647,\n",
       "  0.28647275743722256,\n",
       "  0.28859649536682297,\n",
       "  0.28527239386064523,\n",
       "  0.28859648579045344,\n",
       "  0.28116343886568274,\n",
       "  0.28739612287431543,\n",
       "  0.2813942736205632,\n",
       "  0.293905815589461,\n",
       "  0.29127424113307965,\n",
       "  0.2926131159975258,\n",
       "  0.29529085482917006,\n",
       "  0.28882734068873184,\n",
       "  0.2882271433801202,\n",
       "  0.2797322326089537,\n",
       "  0.28208679505662576,\n",
       "  0.2801939100439859,\n",
       "  0.28065559144165375,\n",
       "  0.28245614704332855,\n",
       "  0.28379501728469975,\n",
       "  0.28771929991872686,\n",
       "  0.2842566920779749,\n",
       "  0.28670359714539756,\n",
       "  0.2853647321875406,\n",
       "  0.2890120094172512,\n",
       "  0.2868882777618239,\n",
       "  0.2873499532155383,\n",
       "  0.2895660235280806,\n",
       "  0.2939981522652581,\n",
       "  0.2889658295216653,\n",
       "  0.2904893817003414,\n",
       "  0.28808864001752266,\n",
       "  0.28855032405694764,\n",
       "  0.28795013665492514,\n",
       "  0.2881809766933198,\n",
       "  0.2909048970716482,\n",
       "  0.2897506955587963,\n",
       "  0.2944598382860009,\n",
       "  0.291828255243909,\n",
       "  0.28887349218542885,\n",
       "  0.28665743838386853,\n",
       "  0.2873499591594918,\n",
       "  0.29025854100150744,\n",
       "  0.28711911515846145,\n",
       "  0.28873499740854197,\n",
       "  0.2859187469588092,\n",
       "  0.2894275155424081,\n",
       "  0.2883194807163566,\n",
       "  0.2876731378550014,\n",
       "  0.28882733804697475,\n",
       "  0.2808402674350052,\n",
       "  0.2847183827217926,\n",
       "  0.2892890115193713,\n",
       "  0.2910895664606068,\n",
       "  0.2885041547283902,\n",
       "  0.2890581714809766,\n",
       "  0.2899815263510411,\n",
       "  0.29459833636508426,\n",
       "  0.29219759666358336,\n",
       "  0.2912742358495654,\n",
       "  0.2922899386228947,\n",
       "  0.29372113959610957,\n",
       "  0.2900738653383757,\n",
       "  0.29205909198010727,\n",
       "  0.29284394678977055,\n",
       "  0.28951985816215875,\n",
       "  0.2892890055754178,\n",
       "  0.2894736865220638,\n",
       "  0.2935826451494423,\n",
       "  0.29058171936679744,\n",
       "  0.29035087305422963,\n",
       "  0.28716527689196725,\n",
       "  0.28056324981256203,\n",
       "  0.2897506975401141,\n",
       "  0.2886426561096699,\n",
       "  0.28767313059016936,\n",
       "  0.2871191118562651,\n",
       "  0.2882271493240737,\n",
       "  0.29067405373105715,\n",
       "  0.28868882874042373,\n",
       "  0.29639888635302514,\n",
       "  0.29076638710465785,\n",
       "  0.28739612650673146],\n",
       " 'val_loss': [5.334232200735764,\n",
       "  4.983429148214327,\n",
       "  4.784476396677305,\n",
       "  4.644192029074799,\n",
       "  4.476048434809815,\n",
       "  4.394104867959194,\n",
       "  4.396388589306701,\n",
       "  4.357530392073899,\n",
       "  4.355590224694863,\n",
       "  4.420083363193402,\n",
       "  4.396052172286905,\n",
       "  4.30787973172373,\n",
       "  4.380612231844621,\n",
       "  4.309422230634758,\n",
       "  4.342740501173966,\n",
       "  4.329023195856767,\n",
       "  4.330297497022066,\n",
       "  4.338106912846188,\n",
       "  4.35406737962215,\n",
       "  4.328584785427122,\n",
       "  4.347966926475223,\n",
       "  4.30494642858025,\n",
       "  4.280759547854499,\n",
       "  4.296895604082149,\n",
       "  4.271123910636353,\n",
       "  4.250159640792463,\n",
       "  4.266036294432853,\n",
       "  4.30528777125928,\n",
       "  4.2591060026086485,\n",
       "  4.284499919071472,\n",
       "  4.267197349774752,\n",
       "  4.285643217718,\n",
       "  4.292257828678159,\n",
       "  4.258778679542404,\n",
       "  4.2560207840350035,\n",
       "  4.267062067985535,\n",
       "  4.267867694913054,\n",
       "  4.269728667530225,\n",
       "  4.2398649204549175,\n",
       "  4.248402599036265,\n",
       "  4.250407251608457,\n",
       "  4.260727329648656,\n",
       "  4.24391545726241,\n",
       "  4.231412957983909,\n",
       "  4.252137623502197,\n",
       "  4.262081643403005,\n",
       "  4.270129048652786,\n",
       "  4.240397049368714,\n",
       "  4.262966513119156,\n",
       "  4.2415514735866795,\n",
       "  4.2389749377751516,\n",
       "  4.235660294320086,\n",
       "  4.253325054714148,\n",
       "  4.255096145756811,\n",
       "  4.268154783557645,\n",
       "  4.244763585732137,\n",
       "  4.2400672981207315,\n",
       "  4.249886429138321,\n",
       "  4.233658954393949,\n",
       "  4.242330030400118,\n",
       "  4.246113064048958,\n",
       "  4.269752687258686,\n",
       "  4.273588015449991,\n",
       "  4.24430685952413,\n",
       "  4.253054769433659,\n",
       "  4.2526969822190654,\n",
       "  4.264879124627696,\n",
       "  4.253031329628375,\n",
       "  4.248623587330468,\n",
       "  4.2466918674304335,\n",
       "  4.250990962210319,\n",
       "  4.237463419128665,\n",
       "  4.240111417564556,\n",
       "  4.231945734144115,\n",
       "  4.264868944325893,\n",
       "  4.296105099153175,\n",
       "  4.265961650635699,\n",
       "  4.254146666749776,\n",
       "  4.247463249988693,\n",
       "  4.250411263901553,\n",
       "  4.233113126102969,\n",
       "  4.269135510664192,\n",
       "  4.240901732702049,\n",
       "  4.241278520848254,\n",
       "  4.241024526932256,\n",
       "  4.279316733981208,\n",
       "  4.263812564431334,\n",
       "  4.255715478667252,\n",
       "  4.251689752486112,\n",
       "  4.2540376669211355,\n",
       "  4.255684587252226,\n",
       "  4.243115808723641,\n",
       "  4.252523908512197,\n",
       "  4.278419660578528,\n",
       "  4.258318733311386,\n",
       "  4.238765002240379,\n",
       "  4.281636326347323,\n",
       "  4.257776137135869,\n",
       "  4.256454981831338,\n",
       "  4.258448163296679,\n",
       "  4.29103919207621,\n",
       "  4.284264912450913,\n",
       "  4.268140534531298,\n",
       "  4.27067632597985,\n",
       "  4.265863456657464,\n",
       "  4.274150836038933,\n",
       "  4.2745533214198606,\n",
       "  4.251075207586768,\n",
       "  4.280791801342862,\n",
       "  4.276516678007386,\n",
       "  4.262852711042912,\n",
       "  4.244799723470812,\n",
       "  4.239898828551066,\n",
       "  4.241158988064141,\n",
       "  4.286763321324218,\n",
       "  4.272219646748879,\n",
       "  4.258296751118392,\n",
       "  4.2579002113651025,\n",
       "  4.250585006295348,\n",
       "  4.263300335578781,\n",
       "  4.287176916410597,\n",
       "  4.267209409981323,\n",
       "  4.2592179324987125,\n",
       "  4.241427866589252,\n",
       "  4.25571378709601,\n",
       "  4.259943306188789],\n",
       " 'val_ATT_loss': [1.9467535530062887,\n",
       "  1.6802288681483097,\n",
       "  1.5139106712752965,\n",
       "  1.3501080075613887,\n",
       "  1.2696894748605412,\n",
       "  1.221431899413788,\n",
       "  1.1977337003611832,\n",
       "  1.1762267613582473,\n",
       "  1.1702008545827522,\n",
       "  1.2224831224345474,\n",
       "  1.2310556993210058,\n",
       "  1.131024040249612,\n",
       "  1.1646368579041186,\n",
       "  1.1240230929079673,\n",
       "  1.1704052223576058,\n",
       "  1.1351405593131085,\n",
       "  1.1501032817277976,\n",
       "  1.173754020045987,\n",
       "  1.1775581840130922,\n",
       "  1.1302910293606545,\n",
       "  1.1318068706732003,\n",
       "  1.120318894077548,\n",
       "  1.1135375201273308,\n",
       "  1.1358958587372046,\n",
       "  1.1128027519733785,\n",
       "  1.101135403989888,\n",
       "  1.1118275452003206,\n",
       "  1.1359860286438208,\n",
       "  1.1098562801484582,\n",
       "  1.1240069015420597,\n",
       "  1.1235533113959881,\n",
       "  1.1356939092814493,\n",
       "  1.1348076132561664,\n",
       "  1.1213907466517936,\n",
       "  1.1144546011369005,\n",
       "  1.125201167820169,\n",
       "  1.1301001840358158,\n",
       "  1.1274121363385976,\n",
       "  1.1026366274991481,\n",
       "  1.1111211064908144,\n",
       "  1.116416259120694,\n",
       "  1.1265687301004532,\n",
       "  1.1074955880213126,\n",
       "  1.100107603107425,\n",
       "  1.1114035405701013,\n",
       "  1.1160386817918406,\n",
       "  1.1187841972858785,\n",
       "  1.101940346621781,\n",
       "  1.1273544750625282,\n",
       "  1.1098750404316744,\n",
       "  1.0975502566467943,\n",
       "  1.1018215373265658,\n",
       "  1.1169509407427671,\n",
       "  1.1124713264780937,\n",
       "  1.1119343299659894,\n",
       "  1.1018522952100356,\n",
       "  1.1109970471841826,\n",
       "  1.1172460087769323,\n",
       "  1.104120704424467,\n",
       "  1.1123115098733696,\n",
       "  1.1033362894607104,\n",
       "  1.116625582743034,\n",
       "  1.1169069791011672,\n",
       "  1.1095238203625026,\n",
       "  1.1104152233480549,\n",
       "  1.1157033052375849,\n",
       "  1.117015766925949,\n",
       "  1.109329711619041,\n",
       "  1.1226307867242278,\n",
       "  1.1145519256591796,\n",
       "  1.119053316116333,\n",
       "  1.108534698520633,\n",
       "  1.113140492645099,\n",
       "  1.1075780763900538,\n",
       "  1.1286967277526856,\n",
       "  1.1531864872939295,\n",
       "  1.1271249721376158,\n",
       "  1.118506786463072,\n",
       "  1.1168267845249862,\n",
       "  1.1197021585574254,\n",
       "  1.10681240918825,\n",
       "  1.1377551174849916,\n",
       "  1.1123699594744676,\n",
       "  1.1163610629898182,\n",
       "  1.11213752142817,\n",
       "  1.1231790630079859,\n",
       "  1.1137539458789414,\n",
       "  1.1152164668487987,\n",
       "  1.1210448472619914,\n",
       "  1.1233390989921075,\n",
       "  1.1239136423138405,\n",
       "  1.1175253039641346,\n",
       "  1.1261163720124059,\n",
       "  1.1414485509447057,\n",
       "  1.1223438691749847,\n",
       "  1.1084430190299055,\n",
       "  1.144129032711331,\n",
       "  1.1107914708501143,\n",
       "  1.1197300428966823,\n",
       "  1.1281077866931613,\n",
       "  1.1472943510082987,\n",
       "  1.1367446127555354,\n",
       "  1.1320238475319293,\n",
       "  1.1305141414669777,\n",
       "  1.1268990854565188,\n",
       "  1.132325507239472,\n",
       "  1.1352461739409743,\n",
       "  1.1155128021034406,\n",
       "  1.1462821735752573,\n",
       "  1.1423245785047682,\n",
       "  1.1220272750305615,\n",
       "  1.1208062571587323,\n",
       "  1.1169170570030487,\n",
       "  1.1142739795094772,\n",
       "  1.1505715656623565,\n",
       "  1.1231662290559399,\n",
       "  1.1257287792164645,\n",
       "  1.127394741387676,\n",
       "  1.120621292882686,\n",
       "  1.1338713651080783,\n",
       "  1.1459488102000395,\n",
       "  1.1318105814268262,\n",
       "  1.1281847236825409,\n",
       "  1.1177046321278854,\n",
       "  1.1271334691013364,\n",
       "  1.1308314381743507],\n",
       " 'val_VAL_loss': [1.1291595492431585,\n",
       "  1.1010667600220057,\n",
       "  1.0901885751340028,\n",
       "  1.0980280071711368,\n",
       "  1.0687863199830914,\n",
       "  1.057557656181802,\n",
       "  1.0662182963151725,\n",
       "  1.0604345435718838,\n",
       "  1.0617964567040368,\n",
       "  1.0658667469196181,\n",
       "  1.0549988243219663,\n",
       "  1.0589518971580396,\n",
       "  1.0719917913135006,\n",
       "  1.061799712575597,\n",
       "  1.0574450929387869,\n",
       "  1.0646275455145526,\n",
       "  1.0600647384314228,\n",
       "  1.054784297600067,\n",
       "  1.0588363985363527,\n",
       "  1.0660979186888222,\n",
       "  1.0720533519340076,\n",
       "  1.0615425115009005,\n",
       "  1.0557406759090562,\n",
       "  1.053666581781648,\n",
       "  1.0527737195543247,\n",
       "  1.0496747456008582,\n",
       "  1.051402916410844,\n",
       "  1.0564339142051533,\n",
       "  1.0497499074867302,\n",
       "  1.053497672509804,\n",
       "  1.0478813461262546,\n",
       "  1.0499831028121838,\n",
       "  1.0524834051406642,\n",
       "  1.0457959776302035,\n",
       "  1.047188727632701,\n",
       "  1.0472869667217886,\n",
       "  1.0459225036257462,\n",
       "  1.0474388437305422,\n",
       "  1.0457427643185897,\n",
       "  1.0457604975151502,\n",
       "  1.0446636641625877,\n",
       "  1.0447195331827341,\n",
       "  1.0454732897470322,\n",
       "  1.0437684516254946,\n",
       "  1.046911360977365,\n",
       "  1.0486809872037215,\n",
       "  1.0504482837889693,\n",
       "  1.0461522342489777,\n",
       "  1.0452040126855424,\n",
       "  1.0438921443850016,\n",
       "  1.0471415603761192,\n",
       "  1.0446129189978401,\n",
       "  1.0454580379904603,\n",
       "  1.047541606426239,\n",
       "  1.052073484530552,\n",
       "  1.0476370968407007,\n",
       "  1.0430234169788497,\n",
       "  1.0442134734537962,\n",
       "  1.043179416656494,\n",
       "  1.0433395068422495,\n",
       "  1.0475922581960828,\n",
       "  1.051042368171884,\n",
       "  1.0522270121162745,\n",
       "  1.0449276797205425,\n",
       "  1.0475465153618682,\n",
       "  1.0456645589938267,\n",
       "  1.0492877859005825,\n",
       "  1.0479005393364447,\n",
       "  1.04199760020208,\n",
       "  1.0440466472570844,\n",
       "  1.043979215364662,\n",
       "  1.0429762402026774,\n",
       "  1.0423236416398192,\n",
       "  1.0414558859180203,\n",
       "  1.045390738857736,\n",
       "  1.0476395372864153,\n",
       "  1.0462788928326943,\n",
       "  1.0452132934289013,\n",
       "  1.0435454884879023,\n",
       "  1.0435697017813759,\n",
       "  1.042100238971573,\n",
       "  1.0437934643930669,\n",
       "  1.042843924409194,\n",
       "  1.0416391526194786,\n",
       "  1.0429623351680288,\n",
       "  1.0520458903244074,\n",
       "  1.0500195395174643,\n",
       "  1.0468330039394844,\n",
       "  1.0435483017413736,\n",
       "  1.043566189309676,\n",
       "  1.0439236483127952,\n",
       "  1.0418635015865023,\n",
       "  1.0421358454999305,\n",
       "  1.0456570365446076,\n",
       "  1.0453249547121337,\n",
       "  1.043440661070158,\n",
       "  1.045835764545331,\n",
       "  1.0489948887619183,\n",
       "  1.0455749796448852,\n",
       "  1.0434467922011725,\n",
       "  1.047914947022637,\n",
       "  1.0491734332317928,\n",
       "  1.0453722289997898,\n",
       "  1.0467207281709574,\n",
       "  1.0463214570669819,\n",
       "  1.04727510959982,\n",
       "  1.0464357158262954,\n",
       "  1.0451874684944427,\n",
       "  1.0448365425892014,\n",
       "  1.044730699834206,\n",
       "  1.0469418120041167,\n",
       "  1.0413311554373597,\n",
       "  1.040993923849339,\n",
       "  1.0422950028515547,\n",
       "  1.0453972518872872,\n",
       "  1.049684472564313,\n",
       "  1.0441893239673092,\n",
       "  1.0435018233258089,\n",
       "  1.0433212378042207,\n",
       "  1.043142990156901,\n",
       "  1.0470760354035191,\n",
       "  1.0451329428514988,\n",
       "  1.0436777362720573,\n",
       "  1.0412410781537886,\n",
       "  1.0428601059982245,\n",
       "  1.043037289338146],\n",
       " 'val_ATT_acc': [20.431654676258994,\n",
       "  52.94964028776978,\n",
       "  54.24460431654676,\n",
       "  68.05755395683454,\n",
       "  77.41007194244604,\n",
       "  79.42446043165468,\n",
       "  79.56834532374101,\n",
       "  79.28057553956835,\n",
       "  79.42446043165468,\n",
       "  75.39568345323741,\n",
       "  74.10071942446044,\n",
       "  84.1726618705036,\n",
       "  80.0,\n",
       "  82.58992805755396,\n",
       "  80.28776978417267,\n",
       "  83.88489208633094,\n",
       "  82.44604316546763,\n",
       "  80.43165467625899,\n",
       "  79.42446043165468,\n",
       "  83.45323741007195,\n",
       "  80.71942446043165,\n",
       "  82.73381294964028,\n",
       "  82.44604316546763,\n",
       "  82.87769784172662,\n",
       "  85.17985611510791,\n",
       "  86.33093525179856,\n",
       "  83.16546762589928,\n",
       "  80.71942446043165,\n",
       "  84.46043165467626,\n",
       "  84.1726618705036,\n",
       "  84.89208633093526,\n",
       "  81.43884892086331,\n",
       "  82.58992805755396,\n",
       "  83.16546762589928,\n",
       "  83.45323741007195,\n",
       "  83.02158273381295,\n",
       "  81.87050359712231,\n",
       "  82.01438848920863,\n",
       "  84.31654676258992,\n",
       "  85.03597122302158,\n",
       "  86.0431654676259,\n",
       "  83.7410071942446,\n",
       "  85.03597122302158,\n",
       "  85.46762589928058,\n",
       "  83.45323741007195,\n",
       "  83.16546762589928,\n",
       "  84.74820143884892,\n",
       "  85.75539568345324,\n",
       "  83.45323741007195,\n",
       "  85.03597122302158,\n",
       "  86.0431654676259,\n",
       "  83.7410071942446,\n",
       "  84.60431654676259,\n",
       "  82.87769784172662,\n",
       "  84.74820143884892,\n",
       "  85.89928057553956,\n",
       "  85.75539568345324,\n",
       "  83.45323741007195,\n",
       "  84.02877697841727,\n",
       "  84.1726618705036,\n",
       "  84.89208633093526,\n",
       "  84.46043165467626,\n",
       "  84.31654676258992,\n",
       "  85.61151079136691,\n",
       "  84.89208633093526,\n",
       "  83.7410071942446,\n",
       "  83.16546762589928,\n",
       "  84.60431654676259,\n",
       "  82.58992805755396,\n",
       "  84.1726618705036,\n",
       "  82.58992805755396,\n",
       "  84.02877697841727,\n",
       "  83.59712230215827,\n",
       "  85.03597122302158,\n",
       "  83.16546762589928,\n",
       "  80.57553956834532,\n",
       "  81.43884892086331,\n",
       "  84.60431654676259,\n",
       "  82.87769784172662,\n",
       "  83.88489208633094,\n",
       "  84.1726618705036,\n",
       "  83.30935251798562,\n",
       "  83.88489208633094,\n",
       "  83.45323741007195,\n",
       "  84.02877697841727,\n",
       "  83.30935251798562,\n",
       "  84.46043165467626,\n",
       "  83.7410071942446,\n",
       "  83.02158273381295,\n",
       "  82.3021582733813,\n",
       "  82.73381294964028,\n",
       "  82.44604316546763,\n",
       "  81.87050359712231,\n",
       "  81.58273381294964,\n",
       "  82.3021582733813,\n",
       "  83.59712230215827,\n",
       "  81.15107913669065,\n",
       "  83.02158273381295,\n",
       "  82.3021582733813,\n",
       "  81.58273381294964,\n",
       "  80.71942446043165,\n",
       "  81.72661870503597,\n",
       "  82.3021582733813,\n",
       "  82.73381294964028,\n",
       "  82.58992805755396,\n",
       "  81.87050359712231,\n",
       "  82.3021582733813,\n",
       "  83.88489208633094,\n",
       "  80.71942446043165,\n",
       "  81.87050359712231,\n",
       "  83.16546762589928,\n",
       "  81.87050359712231,\n",
       "  83.88489208633094,\n",
       "  83.16546762589928,\n",
       "  81.58273381294964,\n",
       "  83.59712230215827,\n",
       "  82.87769784172662,\n",
       "  82.87769784172662,\n",
       "  83.59712230215827,\n",
       "  81.43884892086331,\n",
       "  82.44604316546763,\n",
       "  82.87769784172662,\n",
       "  81.43884892086331,\n",
       "  82.58992805755396,\n",
       "  82.44604316546763,\n",
       "  83.02158273381295],\n",
       " 'val_ATT_acc_2': [],\n",
       " 'val_VAL_acc': [83.30935251798562,\n",
       "  76.11510791366906,\n",
       "  69.7841726618705,\n",
       "  52.66187050359712,\n",
       "  92.23021582733813,\n",
       "  63.884892086330936,\n",
       "  93.9568345323741,\n",
       "  60.86330935251799,\n",
       "  96.40287769784173,\n",
       "  55.10791366906475,\n",
       "  72.23021582733813,\n",
       "  77.55395683453237,\n",
       "  70.64748201438849,\n",
       "  74.96402877697842,\n",
       "  80.43165467625899,\n",
       "  91.94244604316546,\n",
       "  89.49640287769785,\n",
       "  78.12949640287769,\n",
       "  64.31654676258992,\n",
       "  60.28776978417266,\n",
       "  60.86330935251799,\n",
       "  65.75539568345324,\n",
       "  78.12949640287769,\n",
       "  64.74820143884892,\n",
       "  82.73381294964028,\n",
       "  71.22302158273381,\n",
       "  70.64748201438849,\n",
       "  61.43884892086331,\n",
       "  76.11510791366906,\n",
       "  59.280575539568346,\n",
       "  66.76258992805755,\n",
       "  59.13669064748201,\n",
       "  59.856115107913666,\n",
       "  70.07194244604317,\n",
       "  60.143884892086334,\n",
       "  72.66187050359713,\n",
       "  81.72661870503597,\n",
       "  62.014388489208635,\n",
       "  66.33093525179856,\n",
       "  62.87769784172662,\n",
       "  74.10071942446044,\n",
       "  73.5251798561151,\n",
       "  64.02877697841727,\n",
       "  66.90647482014388,\n",
       "  63.884892086330936,\n",
       "  72.37410071942446,\n",
       "  59.42446043165467,\n",
       "  62.589928057553955,\n",
       "  62.15827338129496,\n",
       "  65.32374100719424,\n",
       "  59.856115107913666,\n",
       "  67.62589928057554,\n",
       "  60.86330935251799,\n",
       "  60.431654676258994,\n",
       "  60.0,\n",
       "  60.431654676258994,\n",
       "  67.05035971223022,\n",
       "  73.23741007194245,\n",
       "  66.33093525179856,\n",
       "  67.9136690647482,\n",
       "  75.10791366906474,\n",
       "  81.00719424460432,\n",
       "  74.53237410071942,\n",
       "  72.37410071942446,\n",
       "  65.61151079136691,\n",
       "  64.31654676258992,\n",
       "  62.014388489208635,\n",
       "  61.58273381294964,\n",
       "  63.02158273381295,\n",
       "  65.32374100719424,\n",
       "  61.43884892086331,\n",
       "  62.302158273381295,\n",
       "  63.16546762589928,\n",
       "  68.05755395683454,\n",
       "  72.23021582733813,\n",
       "  79.71223021582733,\n",
       "  76.97841726618705,\n",
       "  67.62589928057554,\n",
       "  65.46762589928058,\n",
       "  66.33093525179856,\n",
       "  64.31654676258992,\n",
       "  61.007194244604314,\n",
       "  66.61870503597122,\n",
       "  72.80575539568345,\n",
       "  72.23021582733813,\n",
       "  70.93525179856115,\n",
       "  72.94964028776978,\n",
       "  72.94964028776978,\n",
       "  81.15107913669065,\n",
       "  73.66906474820144,\n",
       "  67.9136690647482,\n",
       "  60.0,\n",
       "  65.03597122302158,\n",
       "  62.44604316546763,\n",
       "  60.57553956834533,\n",
       "  73.5251798561151,\n",
       "  79.56834532374101,\n",
       "  76.69064748201438,\n",
       "  70.50359712230215,\n",
       "  70.35971223021583,\n",
       "  73.66906474820144,\n",
       "  73.9568345323741,\n",
       "  67.4820143884892,\n",
       "  62.44604316546763,\n",
       "  66.4748201438849,\n",
       "  69.7841726618705,\n",
       "  74.53237410071942,\n",
       "  72.80575539568345,\n",
       "  74.24460431654676,\n",
       "  71.36690647482014,\n",
       "  73.66906474820144,\n",
       "  70.2158273381295,\n",
       "  67.19424460431655,\n",
       "  75.68345323741008,\n",
       "  72.80575539568345,\n",
       "  73.81294964028777,\n",
       "  73.5251798561151,\n",
       "  75.53956834532374,\n",
       "  66.90647482014388,\n",
       "  70.35971223021583,\n",
       "  65.46762589928058,\n",
       "  65.03597122302158,\n",
       "  66.4748201438849,\n",
       "  67.76978417266187,\n",
       "  73.81294964028777,\n",
       "  77.84172661870504],\n",
       " 'val_VAL_jac': [0.11454608131655686,\n",
       "  0.13347722231912956,\n",
       "  0.13016786815451203,\n",
       "  0.13141487159317347,\n",
       "  0.13107913981238714,\n",
       "  0.1319904085543516,\n",
       "  0.1310071943475188,\n",
       "  0.13098321458418594,\n",
       "  0.13195615209263864,\n",
       "  0.13225419727160775,\n",
       "  0.13229188130056257,\n",
       "  0.13281946147946144,\n",
       "  0.134388491061094,\n",
       "  0.13443645195995305,\n",
       "  0.1368345346382196,\n",
       "  0.13721822800396158,\n",
       "  0.13690647564345984,\n",
       "  0.1360191870078766,\n",
       "  0.13525180027639266,\n",
       "  0.13635491673037303,\n",
       "  0.13673861146830826,\n",
       "  0.13719424481014553,\n",
       "  0.13654676375629232,\n",
       "  0.13604316711425782,\n",
       "  0.13709832335547578,\n",
       "  0.1362110333476993,\n",
       "  0.13827338218688964,\n",
       "  0.13793765040610334,\n",
       "  0.13719424755453205,\n",
       "  0.13563549261298968,\n",
       "  0.1364988032004816,\n",
       "  0.13544364730231195,\n",
       "  0.13714628528347975,\n",
       "  0.13834532627956472,\n",
       "  0.13676259294688273,\n",
       "  0.13853717159024245,\n",
       "  0.13988009596900117,\n",
       "  0.1399760194819608,\n",
       "  0.13952038614012355,\n",
       "  0.1373141508308246,\n",
       "  0.13916067185161782,\n",
       "  0.13755395841255463,\n",
       "  0.13764988192551428,\n",
       "  0.1395923274884121,\n",
       "  0.14019184695730963,\n",
       "  0.1412230230921464,\n",
       "  0.13901678606760587,\n",
       "  0.14038369364018063,\n",
       "  0.13705036279966507,\n",
       "  0.1390887284450394,\n",
       "  0.13760192171275187,\n",
       "  0.14275779381072778,\n",
       "  0.14206234911362903,\n",
       "  0.1418465216382802,\n",
       "  0.14285371800978408,\n",
       "  0.14095923265964866,\n",
       "  0.13990407779062394,\n",
       "  0.13796163325687105,\n",
       "  0.1383693077581392,\n",
       "  0.13717026367461937,\n",
       "  0.14011990629511772,\n",
       "  0.1403117526349404,\n",
       "  0.14148681146635425,\n",
       "  0.14316547002723748,\n",
       "  0.14088729234050504,\n",
       "  0.1414388498813986,\n",
       "  0.1411270994076626,\n",
       "  0.14306954479903625,\n",
       "  0.14158273463626558,\n",
       "  0.14095923437489022,\n",
       "  0.14208633110677596,\n",
       "  0.14148681095178178,\n",
       "  0.14203836849267534,\n",
       "  0.14369304420279083,\n",
       "  0.14232613988917509,\n",
       "  0.14184652266742515,\n",
       "  0.14165467564150583,\n",
       "  0.1423261405752717,\n",
       "  0.14318945116276363,\n",
       "  0.14223021637621544,\n",
       "  0.1455395691686397,\n",
       "  0.14417266348283067,\n",
       "  0.14237410147413077,\n",
       "  0.14129496444043496,\n",
       "  0.14127098399100543,\n",
       "  0.14335731413724612,\n",
       "  0.141534772022165,\n",
       "  0.14215827262658867,\n",
       "  0.14122302137690482,\n",
       "  0.1421342932063041,\n",
       "  0.14158273463626558,\n",
       "  0.14129496478348327,\n",
       "  0.1421342932063041,\n",
       "  0.13796163257077443,\n",
       "  0.13949640431850077,\n",
       "  0.14266187064081645,\n",
       "  0.14390887287880877,\n",
       "  0.14182254290409224,\n",
       "  0.14359712120440365,\n",
       "  0.1446522791608632,\n",
       "  0.1461870510801137,\n",
       "  0.1448441261867825,\n",
       "  0.14201438855781828,\n",
       "  0.1422062359267859,\n",
       "  0.1426858514332943,\n",
       "  0.14143884885225363,\n",
       "  0.14383693050137528,\n",
       "  0.1448920860564966,\n",
       "  0.14333333420238906,\n",
       "  0.1433573153379152,\n",
       "  0.1410071935585077,\n",
       "  0.14390887184966383,\n",
       "  0.1424460448807092,\n",
       "  0.14189448459542914,\n",
       "  0.1404796195544785,\n",
       "  0.1379616318846778,\n",
       "  0.14230215943974556,\n",
       "  0.1423741001019375,\n",
       "  0.1400719436810171,\n",
       "  0.14011990560902107,\n",
       "  0.14251798725814271,\n",
       "  0.14187050397447545,\n",
       "  0.1415587533292153,\n",
       "  0.14455635427571029,\n",
       "  0.14232613886003015,\n",
       "  0.1402398102575069],\n",
       " 'test_loss': -1,\n",
       " 'test_ATT_loss': -1,\n",
       " 'test_VAL_loss': -1,\n",
       " 'test_ATT_acc': -1,\n",
       " 'test_ATT_acc_2': -1,\n",
       " 'test_VAL_acc': -1,\n",
       " 'test_VAL_jac': -1,\n",
       " 'model_filename': 'model_storage/GAT/model.pth'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperdict[(0.1,8,128,0.001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>stop_early</th>\n",
       "      <th>early_stopping_step</th>\n",
       "      <th>early_stopping_best_ATT_acc_val</th>\n",
       "      <th>early_stopping_best_VAL_acc_val</th>\n",
       "      <th>early_stopping_best_ATT_acc_val_2</th>\n",
       "      <th>early_stopping_lowest_loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch_index</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_ATT_loss</th>\n",
       "      <th>...</th>\n",
       "      <th>val_VAL_acc</th>\n",
       "      <th>val_VAL_jac</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_ATT_loss</th>\n",
       "      <th>test_VAL_loss</th>\n",
       "      <th>test_ATT_acc</th>\n",
       "      <th>test_ATT_acc_2</th>\n",
       "      <th>test_VAL_acc</th>\n",
       "      <th>test_VAL_jac</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">0.1</th>\n",
       "      <th>2</th>\n",
       "      <th>128</th>\n",
       "      <th>0.0010</th>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>90.071942</td>\n",
       "      <td>98.129496</td>\n",
       "      <td>0</td>\n",
       "      <td>4.161721</td>\n",
       "      <td>0.001</td>\n",
       "      <td>194</td>\n",
       "      <td>[3.749527335166931, 3.1979790131251016, 2.8717...</td>\n",
       "      <td>[1.7099867015664267, 1.3311545033203929, 1.054...</td>\n",
       "      <td>...</td>\n",
       "      <td>[94.38848920863309, 90.79136690647482, 78.2733...</td>\n",
       "      <td>[0.13235012267133314, 0.13556355195079775, 0.1...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>model_storage/GAT/model.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>32</th>\n",
       "      <th>0.0001</th>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>90.071942</td>\n",
       "      <td>98.417266</td>\n",
       "      <td>0</td>\n",
       "      <td>4.151279</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>450</td>\n",
       "      <td>[4.2933124502499895, 3.8550999959309897, 3.717...</td>\n",
       "      <td>[1.983609967945025, 1.8928287551013387, 1.8518...</td>\n",
       "      <td>...</td>\n",
       "      <td>[35.10791366906475, 68.05755395683454, 82.8776...</td>\n",
       "      <td>[0.13362110707399658, 0.13347722231912956, 0.1...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>model_storage/GAT/model.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <th>0.0001</th>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>90.935252</td>\n",
       "      <td>98.848921</td>\n",
       "      <td>0</td>\n",
       "      <td>4.142859</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>241</td>\n",
       "      <td>[3.853951096534729, 3.6204877694447837, 3.3742...</td>\n",
       "      <td>[1.8365001850181009, 1.742488514023144, 1.5668...</td>\n",
       "      <td>...</td>\n",
       "      <td>[85.32374100719424, 94.96402877697842, 86.1870...</td>\n",
       "      <td>[0.13347722231912956, 0.13347722231912956, 0.1...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>model_storage/GAT/model.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>32</th>\n",
       "      <th>0.0100</th>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>88.057554</td>\n",
       "      <td>98.848921</td>\n",
       "      <td>0</td>\n",
       "      <td>4.230725</td>\n",
       "      <td>0.01</td>\n",
       "      <td>250</td>\n",
       "      <td>[8.748551845550537, 3.861710508664449, 3.47437...</td>\n",
       "      <td>[5.154336715338963, 4.724637833328458, 5.21760...</td>\n",
       "      <td>...</td>\n",
       "      <td>[57.410071942446045, 76.83453237410072, 76.978...</td>\n",
       "      <td>[0.10496346092910218, 0.11135834601285646, 0.0...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>model_storage/GAT/model.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.3</th>\n",
       "      <th>2</th>\n",
       "      <th>128</th>\n",
       "      <th>0.0010</th>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>88.345324</td>\n",
       "      <td>98.129496</td>\n",
       "      <td>0</td>\n",
       "      <td>4.173171</td>\n",
       "      <td>0.001</td>\n",
       "      <td>282</td>\n",
       "      <td>[3.745348811149597, 3.177211801211039, 2.85880...</td>\n",
       "      <td>[1.729639697273022, 1.2917713636836847, 1.0431...</td>\n",
       "      <td>...</td>\n",
       "      <td>[60.28776978417266, 90.2158273381295, 66.04316...</td>\n",
       "      <td>[0.1276738619632858, 0.1344844152601503, 0.134...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>model_storage/GAT/model.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>32</th>\n",
       "      <th>0.0100</th>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>85.611511</td>\n",
       "      <td>98.273381</td>\n",
       "      <td>0</td>\n",
       "      <td>4.316985</td>\n",
       "      <td>0.01</td>\n",
       "      <td>342</td>\n",
       "      <td>[7.232279658317566, 3.786673982938131, 3.27010...</td>\n",
       "      <td>[3.9959895993864105, 5.370007290404259, 8.0130...</td>\n",
       "      <td>...</td>\n",
       "      <td>[34.24460431654676, 34.24460431654676, 80.8633...</td>\n",
       "      <td>[0.10254368276047192, 0.11244261968050072, 0.0...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>model_storage/GAT/model.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>64</th>\n",
       "      <th>0.0100</th>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>87.769784</td>\n",
       "      <td>98.129496</td>\n",
       "      <td>0</td>\n",
       "      <td>4.318487</td>\n",
       "      <td>0.01</td>\n",
       "      <td>412</td>\n",
       "      <td>[12.568278908729553, 6.387039542198181, 4.4356...</td>\n",
       "      <td>[8.291277605410759, 3.3177685050753016, 3.9909...</td>\n",
       "      <td>...</td>\n",
       "      <td>[81.00719424460432, 34.24460431654676, 77.6978...</td>\n",
       "      <td>[0.09522096819157223, 0.08165467540137201, 0.0...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>model_storage/GAT/model.pth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 stop_early early_stopping_step  \\\n",
       "0.1 2 128 0.0010       True                 100   \n",
       "    5 32  0.0001       True                 100   \n",
       "      512 0.0001       True                 100   \n",
       "    8 32  0.0100       True                 100   \n",
       "0.3 2 128 0.0010       True                 100   \n",
       "    5 32  0.0100       True                 100   \n",
       "    8 64  0.0100       True                 100   \n",
       "\n",
       "                 early_stopping_best_ATT_acc_val  \\\n",
       "0.1 2 128 0.0010                       90.071942   \n",
       "    5 32  0.0001                       90.071942   \n",
       "      512 0.0001                       90.935252   \n",
       "    8 32  0.0100                       88.057554   \n",
       "0.3 2 128 0.0010                       88.345324   \n",
       "    5 32  0.0100                       85.611511   \n",
       "    8 64  0.0100                       87.769784   \n",
       "\n",
       "                 early_stopping_best_VAL_acc_val  \\\n",
       "0.1 2 128 0.0010                       98.129496   \n",
       "    5 32  0.0001                       98.417266   \n",
       "      512 0.0001                       98.848921   \n",
       "    8 32  0.0100                       98.848921   \n",
       "0.3 2 128 0.0010                       98.129496   \n",
       "    5 32  0.0100                       98.273381   \n",
       "    8 64  0.0100                       98.129496   \n",
       "\n",
       "                 early_stopping_best_ATT_acc_val_2 early_stopping_lowest_loss  \\\n",
       "0.1 2 128 0.0010                                 0                   4.161721   \n",
       "    5 32  0.0001                                 0                   4.151279   \n",
       "      512 0.0001                                 0                   4.142859   \n",
       "    8 32  0.0100                                 0                   4.230725   \n",
       "0.3 2 128 0.0010                                 0                   4.173171   \n",
       "    5 32  0.0100                                 0                   4.316985   \n",
       "    8 64  0.0100                                 0                   4.318487   \n",
       "\n",
       "                 learning_rate epoch_index  \\\n",
       "0.1 2 128 0.0010         0.001         194   \n",
       "    5 32  0.0001        0.0001         450   \n",
       "      512 0.0001        0.0001         241   \n",
       "    8 32  0.0100          0.01         250   \n",
       "0.3 2 128 0.0010         0.001         282   \n",
       "    5 32  0.0100          0.01         342   \n",
       "    8 64  0.0100          0.01         412   \n",
       "\n",
       "                                                         train_loss  \\\n",
       "0.1 2 128 0.0010  [3.749527335166931, 3.1979790131251016, 2.8717...   \n",
       "    5 32  0.0001  [4.2933124502499895, 3.8550999959309897, 3.717...   \n",
       "      512 0.0001  [3.853951096534729, 3.6204877694447837, 3.3742...   \n",
       "    8 32  0.0100  [8.748551845550537, 3.861710508664449, 3.47437...   \n",
       "0.3 2 128 0.0010  [3.745348811149597, 3.177211801211039, 2.85880...   \n",
       "    5 32  0.0100  [7.232279658317566, 3.786673982938131, 3.27010...   \n",
       "    8 64  0.0100  [12.568278908729553, 6.387039542198181, 4.4356...   \n",
       "\n",
       "                                                     train_ATT_loss  ...  \\\n",
       "0.1 2 128 0.0010  [1.7099867015664267, 1.3311545033203929, 1.054...  ...   \n",
       "    5 32  0.0001  [1.983609967945025, 1.8928287551013387, 1.8518...  ...   \n",
       "      512 0.0001  [1.8365001850181009, 1.742488514023144, 1.5668...  ...   \n",
       "    8 32  0.0100  [5.154336715338963, 4.724637833328458, 5.21760...  ...   \n",
       "0.3 2 128 0.0010  [1.729639697273022, 1.2917713636836847, 1.0431...  ...   \n",
       "    5 32  0.0100  [3.9959895993864105, 5.370007290404259, 8.0130...  ...   \n",
       "    8 64  0.0100  [8.291277605410759, 3.3177685050753016, 3.9909...  ...   \n",
       "\n",
       "                                                        val_VAL_acc  \\\n",
       "0.1 2 128 0.0010  [94.38848920863309, 90.79136690647482, 78.2733...   \n",
       "    5 32  0.0001  [35.10791366906475, 68.05755395683454, 82.8776...   \n",
       "      512 0.0001  [85.32374100719424, 94.96402877697842, 86.1870...   \n",
       "    8 32  0.0100  [57.410071942446045, 76.83453237410072, 76.978...   \n",
       "0.3 2 128 0.0010  [60.28776978417266, 90.2158273381295, 66.04316...   \n",
       "    5 32  0.0100  [34.24460431654676, 34.24460431654676, 80.8633...   \n",
       "    8 64  0.0100  [81.00719424460432, 34.24460431654676, 77.6978...   \n",
       "\n",
       "                                                        val_VAL_jac test_loss  \\\n",
       "0.1 2 128 0.0010  [0.13235012267133314, 0.13556355195079775, 0.1...        -1   \n",
       "    5 32  0.0001  [0.13362110707399658, 0.13347722231912956, 0.1...        -1   \n",
       "      512 0.0001  [0.13347722231912956, 0.13347722231912956, 0.1...        -1   \n",
       "    8 32  0.0100  [0.10496346092910218, 0.11135834601285646, 0.0...        -1   \n",
       "0.3 2 128 0.0010  [0.1276738619632858, 0.1344844152601503, 0.134...        -1   \n",
       "    5 32  0.0100  [0.10254368276047192, 0.11244261968050072, 0.0...        -1   \n",
       "    8 64  0.0100  [0.09522096819157223, 0.08165467540137201, 0.0...        -1   \n",
       "\n",
       "                 test_ATT_loss test_VAL_loss test_ATT_acc test_ATT_acc_2  \\\n",
       "0.1 2 128 0.0010            -1            -1           -1             -1   \n",
       "    5 32  0.0001            -1            -1           -1             -1   \n",
       "      512 0.0001            -1            -1           -1             -1   \n",
       "    8 32  0.0100            -1            -1           -1             -1   \n",
       "0.3 2 128 0.0010            -1            -1           -1             -1   \n",
       "    5 32  0.0100            -1            -1           -1             -1   \n",
       "    8 64  0.0100            -1            -1           -1             -1   \n",
       "\n",
       "                 test_VAL_acc test_VAL_jac               model_filename  \n",
       "0.1 2 128 0.0010           -1           -1  model_storage/GAT/model.pth  \n",
       "    5 32  0.0001           -1           -1  model_storage/GAT/model.pth  \n",
       "      512 0.0001           -1           -1  model_storage/GAT/model.pth  \n",
       "    8 32  0.0100           -1           -1  model_storage/GAT/model.pth  \n",
       "0.3 2 128 0.0010           -1           -1  model_storage/GAT/model.pth  \n",
       "    5 32  0.0100           -1           -1  model_storage/GAT/model.pth  \n",
       "    8 64  0.0100           -1           -1  model_storage/GAT/model.pth  \n",
       "\n",
       "[7 rows x 29 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_df[(hyper_df.early_stopping_best_VAL_acc_val + hyper_df.early_stopping_best_VAL_acc_val)>196]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>stop_early</th>\n",
       "      <th>early_stopping_step</th>\n",
       "      <th>early_stopping_best_ATT_acc_val</th>\n",
       "      <th>early_stopping_best_VAL_acc_val</th>\n",
       "      <th>early_stopping_best_ATT_acc_val_2</th>\n",
       "      <th>early_stopping_lowest_loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch_index</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_ATT_loss</th>\n",
       "      <th>...</th>\n",
       "      <th>val_VAL_acc</th>\n",
       "      <th>val_VAL_jac</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_ATT_loss</th>\n",
       "      <th>test_VAL_loss</th>\n",
       "      <th>test_ATT_acc</th>\n",
       "      <th>test_ATT_acc_2</th>\n",
       "      <th>test_VAL_acc</th>\n",
       "      <th>test_VAL_jac</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">0.1</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>32</th>\n",
       "      <th>0.0001</th>\n",
       "      <td>True</td>\n",
       "      <td>246</td>\n",
       "      <td>91.366906</td>\n",
       "      <td>92.661871</td>\n",
       "      <td>0</td>\n",
       "      <td>4.138368</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>999</td>\n",
       "      <td>[4.324005583922069, 4.024964610735576, 3.84491...</td>\n",
       "      <td>[2.0784328353702195, 1.978809388720758, 1.9042...</td>\n",
       "      <td>...</td>\n",
       "      <td>[75.53956834532374, 46.330935251798564, 45.035...</td>\n",
       "      <td>[0.1282733869209564, 0.1447721822656316, 0.133...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>model_storage/GAT/model.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <th>0.0001</th>\n",
       "      <td>True</td>\n",
       "      <td>372</td>\n",
       "      <td>91.223022</td>\n",
       "      <td>97.697842</td>\n",
       "      <td>0</td>\n",
       "      <td>4.139587</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>999</td>\n",
       "      <td>[4.118880391120911, 3.7441560427347818, 3.6038...</td>\n",
       "      <td>[1.9698226494802333, 1.8714637346875305, 1.814...</td>\n",
       "      <td>...</td>\n",
       "      <td>[80.14388489208633, 90.50359712230215, 86.3309...</td>\n",
       "      <td>[0.13347722231912956, 0.13347722231912956, 0.1...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>model_storage/GAT/model.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <th>0.0001</th>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>90.791367</td>\n",
       "      <td>97.122302</td>\n",
       "      <td>0</td>\n",
       "      <td>4.132101</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>311</td>\n",
       "      <td>[3.99049578110377, 3.564414660135905, 3.452611...</td>\n",
       "      <td>[1.8507629979680449, 1.7754817012274364, 1.683...</td>\n",
       "      <td>...</td>\n",
       "      <td>[86.4748201438849, 88.92086330935251, 61.15107...</td>\n",
       "      <td>[0.13347722231912956, 0.13347722231912956, 0.1...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>model_storage/GAT/model.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
       "      <th>32</th>\n",
       "      <th>0.0010</th>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>90.791367</td>\n",
       "      <td>95.827338</td>\n",
       "      <td>0</td>\n",
       "      <td>4.138736</td>\n",
       "      <td>0.001</td>\n",
       "      <td>432</td>\n",
       "      <td>[3.88113001982371, 3.507416069507599, 3.287329...</td>\n",
       "      <td>[1.8174761467693255, 1.6012966916831908, 1.449...</td>\n",
       "      <td>...</td>\n",
       "      <td>[65.61151079136691, 62.87769784172662, 73.0935...</td>\n",
       "      <td>[0.13347722231912956, 0.13304556736843193, 0.1...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>model_storage/GAT/model.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <th>0.0010</th>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>90.215827</td>\n",
       "      <td>91.942446</td>\n",
       "      <td>0</td>\n",
       "      <td>4.139871</td>\n",
       "      <td>0.001</td>\n",
       "      <td>209</td>\n",
       "      <td>[3.867542048295339, 3.36818265914917, 3.088464...</td>\n",
       "      <td>[1.8341679361718513, 1.471865924771803, 1.2299...</td>\n",
       "      <td>...</td>\n",
       "      <td>[55.25179856115108, 91.94244604316546, 81.5827...</td>\n",
       "      <td>[0.13354916435351474, 0.1382494013944118, 0.13...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>model_storage/GAT/model.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <th>0.0001</th>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>90.647482</td>\n",
       "      <td>97.841727</td>\n",
       "      <td>0</td>\n",
       "      <td>4.138529</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>535</td>\n",
       "      <td>[4.0143164197603864, 3.616700828075409, 3.5310...</td>\n",
       "      <td>[1.8767330029004168, 1.8049459582880925, 1.726...</td>\n",
       "      <td>...</td>\n",
       "      <td>[52.23021582733813, 81.87050359712231, 80.4316...</td>\n",
       "      <td>[0.13347722231912956, 0.13347722231912956, 0.1...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>model_storage/GAT/model.pth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 stop_early early_stopping_step  \\\n",
       "0.1 2 32  0.0001       True                 246   \n",
       "      64  0.0001       True                 372   \n",
       "      256 0.0001       True                 100   \n",
       "    5 32  0.0010       True                 100   \n",
       "      64  0.0010       True                 100   \n",
       "      128 0.0001       True                 100   \n",
       "\n",
       "                 early_stopping_best_ATT_acc_val  \\\n",
       "0.1 2 32  0.0001                       91.366906   \n",
       "      64  0.0001                       91.223022   \n",
       "      256 0.0001                       90.791367   \n",
       "    5 32  0.0010                       90.791367   \n",
       "      64  0.0010                       90.215827   \n",
       "      128 0.0001                       90.647482   \n",
       "\n",
       "                 early_stopping_best_VAL_acc_val  \\\n",
       "0.1 2 32  0.0001                       92.661871   \n",
       "      64  0.0001                       97.697842   \n",
       "      256 0.0001                       97.122302   \n",
       "    5 32  0.0010                       95.827338   \n",
       "      64  0.0010                       91.942446   \n",
       "      128 0.0001                       97.841727   \n",
       "\n",
       "                 early_stopping_best_ATT_acc_val_2 early_stopping_lowest_loss  \\\n",
       "0.1 2 32  0.0001                                 0                   4.138368   \n",
       "      64  0.0001                                 0                   4.139587   \n",
       "      256 0.0001                                 0                   4.132101   \n",
       "    5 32  0.0010                                 0                   4.138736   \n",
       "      64  0.0010                                 0                   4.139871   \n",
       "      128 0.0001                                 0                   4.138529   \n",
       "\n",
       "                 learning_rate epoch_index  \\\n",
       "0.1 2 32  0.0001        0.0001         999   \n",
       "      64  0.0001        0.0001         999   \n",
       "      256 0.0001        0.0001         311   \n",
       "    5 32  0.0010         0.001         432   \n",
       "      64  0.0010         0.001         209   \n",
       "      128 0.0001        0.0001         535   \n",
       "\n",
       "                                                         train_loss  \\\n",
       "0.1 2 32  0.0001  [4.324005583922069, 4.024964610735576, 3.84491...   \n",
       "      64  0.0001  [4.118880391120911, 3.7441560427347818, 3.6038...   \n",
       "      256 0.0001  [3.99049578110377, 3.564414660135905, 3.452611...   \n",
       "    5 32  0.0010  [3.88113001982371, 3.507416069507599, 3.287329...   \n",
       "      64  0.0010  [3.867542048295339, 3.36818265914917, 3.088464...   \n",
       "      128 0.0001  [4.0143164197603864, 3.616700828075409, 3.5310...   \n",
       "\n",
       "                                                     train_ATT_loss  ...  \\\n",
       "0.1 2 32  0.0001  [2.0784328353702195, 1.978809388720758, 1.9042...  ...   \n",
       "      64  0.0001  [1.9698226494802333, 1.8714637346875305, 1.814...  ...   \n",
       "      256 0.0001  [1.8507629979680449, 1.7754817012274364, 1.683...  ...   \n",
       "    5 32  0.0010  [1.8174761467693255, 1.6012966916831908, 1.449...  ...   \n",
       "      64  0.0010  [1.8341679361718513, 1.471865924771803, 1.2299...  ...   \n",
       "      128 0.0001  [1.8767330029004168, 1.8049459582880925, 1.726...  ...   \n",
       "\n",
       "                                                        val_VAL_acc  \\\n",
       "0.1 2 32  0.0001  [75.53956834532374, 46.330935251798564, 45.035...   \n",
       "      64  0.0001  [80.14388489208633, 90.50359712230215, 86.3309...   \n",
       "      256 0.0001  [86.4748201438849, 88.92086330935251, 61.15107...   \n",
       "    5 32  0.0010  [65.61151079136691, 62.87769784172662, 73.0935...   \n",
       "      64  0.0010  [55.25179856115108, 91.94244604316546, 81.5827...   \n",
       "      128 0.0001  [52.23021582733813, 81.87050359712231, 80.4316...   \n",
       "\n",
       "                                                        val_VAL_jac test_loss  \\\n",
       "0.1 2 32  0.0001  [0.1282733869209564, 0.1447721822656316, 0.133...        -1   \n",
       "      64  0.0001  [0.13347722231912956, 0.13347722231912956, 0.1...        -1   \n",
       "      256 0.0001  [0.13347722231912956, 0.13347722231912956, 0.1...        -1   \n",
       "    5 32  0.0010  [0.13347722231912956, 0.13304556736843193, 0.1...        -1   \n",
       "      64  0.0010  [0.13354916435351474, 0.1382494013944118, 0.13...        -1   \n",
       "      128 0.0001  [0.13347722231912956, 0.13347722231912956, 0.1...        -1   \n",
       "\n",
       "                 test_ATT_loss test_VAL_loss test_ATT_acc test_ATT_acc_2  \\\n",
       "0.1 2 32  0.0001            -1            -1           -1             -1   \n",
       "      64  0.0001            -1            -1           -1             -1   \n",
       "      256 0.0001            -1            -1           -1             -1   \n",
       "    5 32  0.0010            -1            -1           -1             -1   \n",
       "      64  0.0010            -1            -1           -1             -1   \n",
       "      128 0.0001            -1            -1           -1             -1   \n",
       "\n",
       "                 test_VAL_acc test_VAL_jac               model_filename  \n",
       "0.1 2 32  0.0001           -1           -1  model_storage/GAT/model.pth  \n",
       "      64  0.0001           -1           -1  model_storage/GAT/model.pth  \n",
       "      256 0.0001           -1           -1  model_storage/GAT/model.pth  \n",
       "    5 32  0.0010           -1           -1  model_storage/GAT/model.pth  \n",
       "      64  0.0010           -1           -1  model_storage/GAT/model.pth  \n",
       "      128 0.0001           -1           -1  model_storage/GAT/model.pth  \n",
       "\n",
       "[6 rows x 29 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_df[(hyper_df.early_stopping_lowest_loss<4.14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1, 2, 256, 0.0001)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hyper_df['val_VAL_loss'].apply(lambda x: min(x)) + hyper_df['val_ATT_loss'].apply(lambda x: min(x))).index[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run model and get Inference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_L(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.p = dropout\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads, dropout=dropout)\n",
    "        # On the Pubmed dataset, use `heads` output heads in `conv2`.\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1,\n",
    "                             concat=False, dropout=dropout)\n",
    "        self.lin1 = Linear(in_channels, hidden_channels)\n",
    "        self.lin2 = Linear(2*hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=self.p, training=self.training)\n",
    "        x_0 = self.lin1(x).relu()\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.p, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.lin2(torch.hstack([x_0,x]))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Homo(model, optimizer, train_loader):\n",
    "    model.train()\n",
    "\n",
    "    total_examples = total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        batch_size = args.batch_size\n",
    "        edge_index = to_undirected(batch.edge_index)\n",
    "        out = model(batch.x, edge_index)[:batch_size]\n",
    "        out_att = out[:,:9]\n",
    "        out_val = out[:,9:]\n",
    "        y = batch.y\n",
    "        y_att = y[:,:9]\n",
    "        y_val = y[:,9:]\n",
    "        \n",
    "        loss = F.cross_entropy(out_att, y_att[:batch_size]) + F.cross_entropy(out_val, y_val[:batch_size])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_examples += batch_size\n",
    "        total_loss += float(loss) * batch_size\n",
    "\n",
    "    return total_loss / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_Homo(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_examples_att = total_examples_val = 0\n",
    "    running_loss_1 = running_loss_2 = 0.\n",
    "    running_1_acc = 0.\n",
    "    running_1_val = 0.\n",
    "    running_k_acc = 0.\n",
    "    running_k_jac = 0.\n",
    "    \n",
    "    for batch in tqdm(loader):\n",
    "        loss_1 = 0\n",
    "        acc_1_t = 0\n",
    "        loss_2 = 0\n",
    "        acc_1_val = 0\n",
    "        acc_k_t = 0\n",
    "        jac_k_t = 0\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch.batch_size\n",
    "        edge_index = to_undirected(batch.edge_index)\n",
    "        out = model(batch.x, edge_index)[:batch_size]\n",
    "        out_att = out[:,:9]\n",
    "        out_val = out[:,9:]\n",
    "        att_node = (batch.att_lab[:batch_size]).nonzero().squeeze()\n",
    "        val_node = (batch.val_lab[:batch_size]).nonzero().squeeze()\n",
    "\n",
    "        #print(type_node)\n",
    "\n",
    "        #pred_att = out_att.argmax(dim=-1)\n",
    "        #pred_val = out_val.argmax(dim=-1)\n",
    "\n",
    "        y = batch.y\n",
    "        y_att = y[:,:9]\n",
    "        y_val = y[:,9:]\n",
    "\n",
    "        if not att_node.shape[0]==0:\n",
    "            loss_1 = F.cross_entropy(out_att[att_node], y_att[:batch_size][att_node])\n",
    "            acc_1_t = compute_1_accuracy(y_att[:batch_size][att_node], out_att[att_node])\n",
    "\n",
    "        if not val_node.shape[0]==0:\n",
    "            loss_2 = F.cross_entropy(out_val[val_node], y_val[val_node])\n",
    "            acc_1_val = compute_1_accuracy(y_val[val_node], out_val[val_node])\n",
    "            acc_k_t = compute_k_accuracy(y_val[val_node], out_val[val_node], args.k)\n",
    "            jac_k_t = compute_jaccard_index(y_val[val_node], F.softmax(out_val[val_node],dim=-1), args.k)\n",
    "            #loss_3 = loss_1 + loss_2\n",
    "\n",
    "        total_examples_att += att_node.shape[0]\n",
    "        total_examples_val += val_node.shape[0]\n",
    "        #total_correct_att += int((pred_att == y_att[:batch_size]).sum())\n",
    "        #total_correct_val += int((pred_val == y_val[:batch_size]).sum())\n",
    "\n",
    "        running_loss_1 += float(loss_1) * att_node.shape[0]\n",
    "        running_loss_2 += float(loss_2) * val_node.shape[0]\n",
    "        running_1_acc += float(acc_1_t) * att_node.shape[0]\n",
    "        running_1_val += float(acc_1_val) * val_node.shape[0]\n",
    "        running_k_acc += float(acc_k_t) * val_node.shape[0]\n",
    "        running_k_jac += float(jac_k_t) * val_node.shape[0]\n",
    "    \n",
    "    return running_loss_1/total_examples_att, running_loss_2/total_examples_val, running_1_acc/ total_examples_att, running_k_acc/ total_examples_val, running_k_jac/ total_examples_val, running_1_val/total_examples_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization():\n",
    "    set_seed_everywhere(args.seed, args.cuda)\n",
    "    #transform = T.Compose([T.ToSparseTensor()])\n",
    "    dataset = VEN_Homo('dataset/Venice_homo')\n",
    "    data = dataset[0]\n",
    "    data.n_id = torch.arange(data.num_nodes)\n",
    "    data = data.to(device)\n",
    "    \n",
    "    train_loader = NeighborLoader(\n",
    "        data,\n",
    "        # Sample 25 neighbors for each node and edge type for 2 iterations\n",
    "        num_neighbors=[3*args.sample_nodes] * 2,\n",
    "        # Use a batch size of 32 for sampling training nodes\n",
    "        batch_size=args.batch_size,\n",
    "        input_nodes=data.train_mask,\n",
    "    )\n",
    "    val_loader = NeighborLoader(\n",
    "        data,\n",
    "        # Sample 25 neighbors for each node and edge type for 2 iterations\n",
    "        num_neighbors=[3*args.sample_nodes] * 2,\n",
    "        # Use a batch size of 32 for sampling validating nodes\n",
    "        batch_size=args.batch_size,\n",
    "        input_nodes=data.val_mask,\n",
    "    )\n",
    "    test_loader = NeighborLoader(\n",
    "        data,\n",
    "        # Sample 25 neighbors for each node and edge type for 2 iterations\n",
    "        num_neighbors=[3*args.sample_nodes] * 2,\n",
    "        # Use a batch size of 32 for sampling testing nodes\n",
    "        batch_size=args.batch_size,\n",
    "        input_nodes=data.test_mask,\n",
    "    )\n",
    " \n",
    "    model = GAT_L(in_channels=data.x.shape[-1], hidden_channels = 256, \n",
    "            out_channels = data.y.shape[-1], dropout = 0.1, heads=2).to(device)\n",
    "    return data, model, train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(verbose=False):\n",
    "    \n",
    "    _, model, train_loader, val_loader, test_loader = initialization()\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Use {} GPUs !\".format(torch.cuda.device_count()))\n",
    "        model = DataParallel(model)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=args.l2)\n",
    "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "    #                                           mode='min', factor=0.5,\n",
    "    #                                           patience=1)\n",
    "\n",
    "    train_state = make_train_state(args)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(args.num_epochs):\n",
    "            train_state['epoch_index'] = epoch\n",
    "            \n",
    "            loss = train_Homo(model, optimizer, train_loader)\n",
    "            train_loss_att, train_loss_val, train_att_acc, train_val_acc, train_val_jac, train_val_1 = test_Homo(model, train_loader)\n",
    "            val_loss_att, val_loss_val, val_att_acc, val_val_acc, val_val_jac, val_val_1 = test_Homo(model, val_loader)\n",
    "            if verbose:\n",
    "                print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train_ATT: {train_att_acc:.4f}, Train_VAL: {train_val_acc:.4f}, Val_vis_tex_ATT: {val_att_acc:.4f}, Val_vis_tex_VAL: {val_val_acc:.4f}')\n",
    "            \n",
    "            train_state['train_loss'].append(loss)\n",
    "            train_state['train_ATT_loss'].append(train_loss_att)\n",
    "            train_state['train_VAL_loss'].append(train_loss_val)\n",
    "            train_state['train_ATT_acc'].append(train_att_acc)\n",
    "            train_state['train_VAL_acc'].append(train_val_acc)\n",
    "            train_state['train_VAL_jac'].append(train_val_jac)\n",
    "            train_state['train_VAL_acc_1'].append(train_val_1)\n",
    "            \n",
    "            train_state['val_ATT_loss'].append(val_loss_att)\n",
    "            train_state['val_VAL_loss'].append(val_loss_val)\n",
    "            train_state['val_loss'].append(val_loss_att + 3*val_loss_val)\n",
    "            train_state['val_ATT_acc'].append(val_att_acc)\n",
    "            train_state['val_VAL_acc'].append(val_val_acc)\n",
    "            train_state['val_VAL_jac'].append(val_val_jac)\n",
    "            train_state['val_VAL_acc_1'].append(val_val_1)\n",
    "            \n",
    "            train_state = update_train_state(args=args, model=model,\n",
    "                                                train_state=train_state)\n",
    "            if train_state['stop_early']:\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting loop\")\n",
    "        pass\n",
    "    \n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_state = training_loop(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT_L(dataset.num_features, 256, dataset.num_classes,\n",
    "            heads=2, dropout=.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(args.save_dir+'GAT_best_model/model.pth',map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT_L(\n",
       "  (conv1): GATConv(1753, 256, heads=2)\n",
       "  (conv2): GATConv(512, 256, heads=1)\n",
       "  (lin1): Linear(1753, 256, bias=True)\n",
       "  (lin2): Linear(512, 20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 25.78it/s]\n"
     ]
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_loss_att, test_loss_val, test_att_acc, test_val_acc, test_val_jac, test_val_1 = test_Homo(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 31.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.723601653470227,\n",
       " 1.5968123238502778,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 0.8827331627504977,\n",
       " 92.797783933518)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_Homo(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 27.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8012134174748164,\n",
       " 1.618552706511737,\n",
       " 98.17073170731707,\n",
       " 99.50738916256158,\n",
       " 0.7619047658196811,\n",
       " 80.29556650246306)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_Homo(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 24.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8164416074041818,\n",
       " 1.6323064925769966,\n",
       " 97.21669980119285,\n",
       " 99.47916666666667,\n",
       " 0.7647569527228674,\n",
       " 82.8125)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_Homo(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 22.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.89it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.00it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 32.31it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.77it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.28it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.94it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.58it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.35it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 32.77it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 32.51it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 32.61it/s]\n"
     ]
    }
   ],
   "source": [
    "val_numbers = []\n",
    "test_numbers = []\n",
    "for seed in [0,1,2,42,100,233,1024,1337,2333,4399]:\n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    val_numbers.append(test_Homo(model, val_loader))\n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    test_numbers.append(test_Homo(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame(val_numbers, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])\n",
    "test_df = pd.DataFrame(test_numbers, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.801250</td>\n",
       "      <td>1.618615</td>\n",
       "      <td>98.069106</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.763218</td>\n",
       "      <td>80.394089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.107123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.452678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.801060</td>\n",
       "      <td>1.618526</td>\n",
       "      <td>97.967480</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.759442</td>\n",
       "      <td>79.802956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.801157</td>\n",
       "      <td>1.618573</td>\n",
       "      <td>97.967480</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>80.295567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.801229</td>\n",
       "      <td>1.618601</td>\n",
       "      <td>98.069106</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.764368</td>\n",
       "      <td>80.295567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.801316</td>\n",
       "      <td>1.618619</td>\n",
       "      <td>98.170732</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.764368</td>\n",
       "      <td>80.665025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.801511</td>\n",
       "      <td>1.618778</td>\n",
       "      <td>98.170732</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.766831</td>\n",
       "      <td>81.280788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ATT_loss   VAL_loss    ATT_acc  VAL_k_acc  VAL_k_jac  VAL_1_acc\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000\n",
       "mean    0.801250   1.618615  98.069106  99.507389   0.763218  80.394089\n",
       "std     0.000132   0.000075   0.107123   0.000000   0.002263   0.452678\n",
       "min     0.801060   1.618526  97.967480  99.507389   0.759442  79.802956\n",
       "25%     0.801157   1.618573  97.967480  99.507389   0.761905  80.295567\n",
       "50%     0.801229   1.618601  98.069106  99.507389   0.764368  80.295567\n",
       "75%     0.801316   1.618619  98.170732  99.507389   0.764368  80.665025\n",
       "max     0.801511   1.618778  98.170732  99.507389   0.766831  81.280788"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.816202</td>\n",
       "      <td>1.632337</td>\n",
       "      <td>97.375746</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>0.761111</td>\n",
       "      <td>82.552083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.083824</td>\n",
       "      <td>1.497956e-14</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.442623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.815966</td>\n",
       "      <td>1.632191</td>\n",
       "      <td>97.216700</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>0.756944</td>\n",
       "      <td>81.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.816115</td>\n",
       "      <td>1.632289</td>\n",
       "      <td>97.415507</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>0.759549</td>\n",
       "      <td>82.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.816205</td>\n",
       "      <td>1.632367</td>\n",
       "      <td>97.415507</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>0.760851</td>\n",
       "      <td>82.552083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.816243</td>\n",
       "      <td>1.632390</td>\n",
       "      <td>97.415507</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>0.764106</td>\n",
       "      <td>82.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.816471</td>\n",
       "      <td>1.632416</td>\n",
       "      <td>97.415507</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>0.764757</td>\n",
       "      <td>83.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ATT_loss   VAL_loss    ATT_acc     VAL_k_acc  VAL_k_jac  VAL_1_acc\n",
       "count  10.000000  10.000000  10.000000  1.000000e+01  10.000000  10.000000\n",
       "mean    0.816202   1.632337  97.375746  9.947917e+01   0.761111  82.552083\n",
       "std     0.000146   0.000073   0.083824  1.497956e-14   0.003057   0.442623\n",
       "min     0.815966   1.632191  97.216700  9.947917e+01   0.756944  81.770833\n",
       "25%     0.816115   1.632289  97.415507  9.947917e+01   0.759549  82.291667\n",
       "50%     0.816205   1.632367  97.415507  9.947917e+01   0.760851  82.552083\n",
       "75%     0.816243   1.632390  97.415507  9.947917e+01   0.764106  82.812500\n",
       "max     0.816471   1.632416  97.415507  9.947917e+01   0.764757  83.333333"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.to_csv(args.save_dir + 'val_metrics.csv', sep='\\t')\n",
    "test_df.to_csv(args.save_dir + 'test_metrics.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state['test_ATT_loss']=test_loss_att\n",
    "train_state['test_VAL_loss']=test_loss_val\n",
    "train_state['test_loss']=test_loss_att + 3*test_loss_val\n",
    "train_state['test_ATT_acc']=test_att_acc\n",
    "train_state['test_VAL_acc_1']=test_val_1\n",
    "train_state['test_VAL_acc']=test_val_acc\n",
    "train_state['test_VAL_jac']=test_val_jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': True,\n",
       " 'early_stopping_step': 100,\n",
       " 'early_stopping_best_ATT_acc_val': 98.17073170731707,\n",
       " 'early_stopping_best_VAL_acc_val': 100.0,\n",
       " 'early_stopping_best_ATT_acc_val_2': 0,\n",
       " 'early_stopping_lowest_loss': 5.656711502732424,\n",
       " 'learning_rate': 0.0001,\n",
       " 'epoch_index': 441,\n",
       " 'train_loss': [4.016014138857524,\n",
       "  3.580979287624359,\n",
       "  3.476141393184662,\n",
       "  3.359775106112162,\n",
       "  3.262439409891764,\n",
       "  3.1556219458580017,\n",
       "  3.0627198815345764,\n",
       "  2.9832192262013755,\n",
       "  2.894130051136017,\n",
       "  2.8221245805422464,\n",
       "  2.7742998798688254,\n",
       "  2.710253198941549,\n",
       "  2.663776616255442,\n",
       "  2.6352937618891397,\n",
       "  2.583137810230255,\n",
       "  2.5726837515830994,\n",
       "  2.550245145956675,\n",
       "  2.5174156427383423,\n",
       "  2.498042583465576,\n",
       "  2.5006632804870605,\n",
       "  2.483529508113861,\n",
       "  2.469638546307882,\n",
       "  2.4693893790245056,\n",
       "  2.454873959223429,\n",
       "  2.452928980191549,\n",
       "  2.445686995983124,\n",
       "  2.449835201104482,\n",
       "  2.4269318183263144,\n",
       "  2.441512187321981,\n",
       "  2.4307491381963096,\n",
       "  2.425937592983246,\n",
       "  2.4177355766296387,\n",
       "  2.412558356920878,\n",
       "  2.414771775404612,\n",
       "  2.420555830001831,\n",
       "  2.4095747470855713,\n",
       "  2.417506297429403,\n",
       "  2.4048957228660583,\n",
       "  2.4012410243352256,\n",
       "  2.4133909344673157,\n",
       "  2.4146655797958374,\n",
       "  2.4124674002329507,\n",
       "  2.395471533139547,\n",
       "  2.4041011730829873,\n",
       "  2.373327632745107,\n",
       "  2.4147808949152627,\n",
       "  2.40478777885437,\n",
       "  2.3814737200737,\n",
       "  2.4040048917134604,\n",
       "  2.393053491910299,\n",
       "  2.3750780622164407,\n",
       "  2.3751267790794373,\n",
       "  2.38983682791392,\n",
       "  2.3842284083366394,\n",
       "  2.3853240609169006,\n",
       "  2.3510618011156716,\n",
       "  2.4029164711634317,\n",
       "  2.396230181058248,\n",
       "  2.393138885498047,\n",
       "  2.3957826495170593,\n",
       "  2.380471487840017,\n",
       "  2.386141916116079,\n",
       "  2.3704294562339783,\n",
       "  2.365254561106364,\n",
       "  2.382726530234019,\n",
       "  2.370084822177887,\n",
       "  2.393240511417389,\n",
       "  2.3738020062446594,\n",
       "  2.378271222114563,\n",
       "  2.3794250090916953,\n",
       "  2.361500859260559,\n",
       "  2.376123607158661,\n",
       "  2.3814006447792053,\n",
       "  2.357136607170105,\n",
       "  2.353886822859446,\n",
       "  2.3945969343185425,\n",
       "  2.3695674737294516,\n",
       "  2.3480570713678994,\n",
       "  2.3620495994885764,\n",
       "  2.3731311162312827,\n",
       "  2.363567809263865,\n",
       "  2.3861716985702515,\n",
       "  2.3616434733072915,\n",
       "  2.382958730061849,\n",
       "  2.3751869002978006,\n",
       "  2.374918540318807,\n",
       "  2.3699220220247903,\n",
       "  2.370315968990326,\n",
       "  2.3803426027297974,\n",
       "  2.3490475018819175,\n",
       "  2.3744342724482217,\n",
       "  2.3611080646514893,\n",
       "  2.3741276264190674,\n",
       "  2.3455232779184976,\n",
       "  2.370332916577657,\n",
       "  2.352614084879557,\n",
       "  2.3653539617856345,\n",
       "  2.3722551266352334,\n",
       "  2.373527407646179,\n",
       "  2.3717002471288047,\n",
       "  2.3835099140803018,\n",
       "  2.344513158003489,\n",
       "  2.3756223320961,\n",
       "  2.378075281778971,\n",
       "  2.35638827085495,\n",
       "  2.3701693415641785,\n",
       "  2.3813953002293906,\n",
       "  2.3654622435569763,\n",
       "  2.367119789123535,\n",
       "  2.3470069567362466,\n",
       "  2.3602728247642517,\n",
       "  2.35102109114329,\n",
       "  2.3567301829655967,\n",
       "  2.3593870401382446,\n",
       "  2.351957162221273,\n",
       "  2.3552029530207315,\n",
       "  2.3550225297609964,\n",
       "  2.3517017563184104,\n",
       "  2.34880667924881,\n",
       "  2.326340893904368,\n",
       "  2.3484432101249695,\n",
       "  2.33596408367157,\n",
       "  2.3655043045679727,\n",
       "  2.3533435066541037,\n",
       "  2.362076759338379,\n",
       "  2.350716491540273,\n",
       "  2.3421265482902527,\n",
       "  2.36586731672287,\n",
       "  2.3265228470166526,\n",
       "  2.344319979349772,\n",
       "  2.3607678016026816,\n",
       "  2.3719481428464255,\n",
       "  2.351541817188263,\n",
       "  2.377408742904663,\n",
       "  2.3533135255177817,\n",
       "  2.361546834309896,\n",
       "  2.3601023157437644,\n",
       "  2.3594932357470193,\n",
       "  2.3442017634709678,\n",
       "  2.3446496725082397,\n",
       "  2.342009743054708,\n",
       "  2.332191546758016,\n",
       "  2.3377412954966226,\n",
       "  2.3603710730870566,\n",
       "  2.3483949502309165,\n",
       "  2.3711174925168357,\n",
       "  2.3486301501592,\n",
       "  2.33932759364446,\n",
       "  2.3672545552253723,\n",
       "  2.361992677052816,\n",
       "  2.360950549443563,\n",
       "  2.3473278482755027,\n",
       "  2.339685559272766,\n",
       "  2.3594000339508057,\n",
       "  2.3548028667767844,\n",
       "  2.346066474914551,\n",
       "  2.348200738430023,\n",
       "  2.365846574306488,\n",
       "  2.378258923689524,\n",
       "  2.34381635983785,\n",
       "  2.34015945593516,\n",
       "  2.3532375494639077,\n",
       "  2.363723119099935,\n",
       "  2.356558680534363,\n",
       "  2.3491830229759216,\n",
       "  2.3644002874692283,\n",
       "  2.3535417119661965,\n",
       "  2.3440672159194946,\n",
       "  2.3644270499547324,\n",
       "  2.3466598192850747,\n",
       "  2.353440006573995,\n",
       "  2.354663133621216,\n",
       "  2.358858565489451,\n",
       "  2.3360355695088706,\n",
       "  2.3566197554270425,\n",
       "  2.343908647696177,\n",
       "  2.342079281806946,\n",
       "  2.355863948663076,\n",
       "  2.3527610500653586,\n",
       "  2.3339257637659707,\n",
       "  2.3355759580930076,\n",
       "  2.353414257367452,\n",
       "  2.3423049251238504,\n",
       "  2.3546899358431497,\n",
       "  2.3722722927729287,\n",
       "  2.3433383901913962,\n",
       "  2.3614091873168945,\n",
       "  2.349238634109497,\n",
       "  2.355355183283488,\n",
       "  2.3364749352137246,\n",
       "  2.337398966153463,\n",
       "  2.341395318508148,\n",
       "  2.348423659801483,\n",
       "  2.3420873085657754,\n",
       "  2.355209986368815,\n",
       "  2.3347582817077637,\n",
       "  2.3461950620015464,\n",
       "  2.3514471650123596,\n",
       "  2.3524585564931235,\n",
       "  2.3533819715181985,\n",
       "  2.3327417969703674,\n",
       "  2.3412396709124246,\n",
       "  2.3482646346092224,\n",
       "  2.3258248368899026,\n",
       "  2.3464712699254355,\n",
       "  2.3454625010490417,\n",
       "  2.3443915247917175,\n",
       "  2.338489611943563,\n",
       "  2.3427399595578513,\n",
       "  2.33308607339859,\n",
       "  2.3355269034703574,\n",
       "  2.3439786235491433,\n",
       "  2.348806897799174,\n",
       "  2.3625837564468384,\n",
       "  2.3384679357210794,\n",
       "  2.352301855882009,\n",
       "  2.3518435756365457,\n",
       "  2.356831729412079,\n",
       "  2.3450879057248435,\n",
       "  2.354252338409424,\n",
       "  2.3316397865613303,\n",
       "  2.353452821572622,\n",
       "  2.341839094956716,\n",
       "  2.333790957927704,\n",
       "  2.326503574848175,\n",
       "  2.33182285229365,\n",
       "  2.3546353578567505,\n",
       "  2.351185659567515,\n",
       "  2.358602782090505,\n",
       "  2.327974776426951,\n",
       "  2.368851621945699,\n",
       "  2.3591676553090415,\n",
       "  2.3567435145378113,\n",
       "  2.3524892926216125,\n",
       "  2.345068355401357,\n",
       "  2.3501105904579163,\n",
       "  2.326898137728373,\n",
       "  2.3310365875562034,\n",
       "  2.325144648551941,\n",
       "  2.343954583009084,\n",
       "  2.3431854248046875,\n",
       "  2.3376614848772683,\n",
       "  2.3492897351582847,\n",
       "  2.347874959309896,\n",
       "  2.3289391001065574,\n",
       "  2.343879222869873,\n",
       "  2.3160665035247803,\n",
       "  2.350836932659149,\n",
       "  2.348975638548533,\n",
       "  2.3467960556348166,\n",
       "  2.342164178689321,\n",
       "  2.3382447163263955,\n",
       "  2.3284965554873147,\n",
       "  2.3246918519337973,\n",
       "  2.3569940527280173,\n",
       "  2.3582049012184143,\n",
       "  2.331792930761973,\n",
       "  2.354574223359426,\n",
       "  2.347904841105143,\n",
       "  2.336475888888041,\n",
       "  2.3510162631670632,\n",
       "  2.3373314142227173,\n",
       "  2.332490344842275,\n",
       "  2.3340445359547934,\n",
       "  2.3527621229489646,\n",
       "  2.3332653244336448,\n",
       "  2.339162290096283,\n",
       "  2.3371700843175254,\n",
       "  2.3342640002568564,\n",
       "  2.345694343249003,\n",
       "  2.3330258329709372,\n",
       "  2.334805130958557,\n",
       "  2.352565904458364,\n",
       "  2.3447222312291465,\n",
       "  2.3365224798520408,\n",
       "  2.3403624296188354,\n",
       "  2.347195327281952,\n",
       "  2.3541723092397056,\n",
       "  2.3352925380071006,\n",
       "  2.3585981329282126,\n",
       "  2.3527050813039145,\n",
       "  2.331176201502482,\n",
       "  2.3434005975723267,\n",
       "  2.3381160696347556,\n",
       "  2.34757928053538,\n",
       "  2.35615066687266,\n",
       "  2.3308611710866294,\n",
       "  2.3315317829449973,\n",
       "  2.336939255396525,\n",
       "  2.341226855913798,\n",
       "  2.3377283811569214,\n",
       "  2.3366944591204324,\n",
       "  2.34161506096522,\n",
       "  2.338366448879242,\n",
       "  2.3276671767234802,\n",
       "  2.3387354214986167,\n",
       "  2.3410407503445945,\n",
       "  2.33526748418808,\n",
       "  2.346993883450826,\n",
       "  2.349900722503662,\n",
       "  2.346663494904836,\n",
       "  2.33445543050766,\n",
       "  2.3330488999684653,\n",
       "  2.3246732155481973,\n",
       "  2.347734888394674,\n",
       "  2.3388441602389016,\n",
       "  2.343725879987081,\n",
       "  2.3512978752454123,\n",
       "  2.3514621456464133,\n",
       "  2.337273279825846,\n",
       "  2.347037196159363,\n",
       "  2.334003428618113,\n",
       "  2.336957593758901,\n",
       "  2.3508103092511496,\n",
       "  2.3240662217140198,\n",
       "  2.325262506802877,\n",
       "  2.335076371828715,\n",
       "  2.3389208714167276,\n",
       "  2.3398008743921914,\n",
       "  2.3493410547574363,\n",
       "  2.3438364466031394,\n",
       "  2.339883248011271,\n",
       "  2.327455143133799,\n",
       "  2.3383060495058694,\n",
       "  2.3283671935399375,\n",
       "  2.3406534592310586,\n",
       "  2.335781713326772,\n",
       "  2.3415627678235373,\n",
       "  2.350347022215525,\n",
       "  2.3511073191960654,\n",
       "  2.3411142230033875,\n",
       "  2.3267287214597068,\n",
       "  2.3461877504984536,\n",
       "  2.338520348072052,\n",
       "  2.3456786473592124,\n",
       "  2.346846640110016,\n",
       "  2.3434359033902488,\n",
       "  2.3368441661198935,\n",
       "  2.351143538951874,\n",
       "  2.3497967521349588,\n",
       "  2.3415074348449707,\n",
       "  2.3448121547698975,\n",
       "  2.330967823664347,\n",
       "  2.344583789507548,\n",
       "  2.345853865146637,\n",
       "  2.3340320587158203,\n",
       "  2.354803959528605,\n",
       "  2.3505939245224,\n",
       "  2.324528614679972,\n",
       "  2.341008643309275,\n",
       "  2.342706819375356,\n",
       "  2.340651591618856,\n",
       "  2.345959742863973,\n",
       "  2.334669212500254,\n",
       "  2.3426411151885986,\n",
       "  2.3402794003486633,\n",
       "  2.314810554186503,\n",
       "  2.3397565881411233,\n",
       "  2.3291311065355935,\n",
       "  2.3365612427393594,\n",
       "  2.338789085547129,\n",
       "  2.33844264348348,\n",
       "  2.3296879132588706,\n",
       "  2.3373787800470986,\n",
       "  2.335778295993805,\n",
       "  2.3319231271743774,\n",
       "  2.3401660323143005,\n",
       "  2.3352246085802713,\n",
       "  2.343611796696981,\n",
       "  2.341631293296814,\n",
       "  2.3541815280914307,\n",
       "  2.3327117959658303,\n",
       "  2.3396732409795127,\n",
       "  2.3253584702809653,\n",
       "  2.3409852584203086,\n",
       "  2.316529154777527,\n",
       "  2.3245728611946106,\n",
       "  2.348609904448191,\n",
       "  2.3354889154434204,\n",
       "  2.346611519654592,\n",
       "  2.3248634934425354,\n",
       "  2.3364249070485434,\n",
       "  2.343295991420746,\n",
       "  2.3471620678901672,\n",
       "  2.344647705554962,\n",
       "  2.3279197216033936,\n",
       "  2.3319934606552124,\n",
       "  2.342416266600291,\n",
       "  2.350442965825399,\n",
       "  2.3323514660199485,\n",
       "  2.327678660551707,\n",
       "  2.3262283404668174,\n",
       "  2.35122412443161,\n",
       "  2.3228691617647805,\n",
       "  2.348047693570455,\n",
       "  2.3212486505508423,\n",
       "  2.339704155921936,\n",
       "  2.343515157699585,\n",
       "  2.340518673261007,\n",
       "  2.3130542039871216,\n",
       "  2.341365655263265,\n",
       "  2.3362598220507302,\n",
       "  2.328787684440613,\n",
       "  2.3327929973602295,\n",
       "  2.3491464058558145,\n",
       "  2.3489977518717446,\n",
       "  2.3255539735158286,\n",
       "  2.3327155311902366,\n",
       "  2.33163845539093,\n",
       "  2.3310036857922873,\n",
       "  2.3505712151527405,\n",
       "  2.335424800713857,\n",
       "  2.323285222053528,\n",
       "  2.3169257044792175,\n",
       "  2.350167195002238,\n",
       "  2.3357329765955606,\n",
       "  2.343007961908976,\n",
       "  2.3223320841789246,\n",
       "  2.3037697076797485,\n",
       "  2.3163230617841086,\n",
       "  2.341017425060272,\n",
       "  2.334459145863851,\n",
       "  2.330294648806254,\n",
       "  2.3369978864987693,\n",
       "  2.341898262500763,\n",
       "  2.32839165131251,\n",
       "  2.332941552003225,\n",
       "  2.315711955229441,\n",
       "  2.3391212224960327,\n",
       "  2.343146582444509,\n",
       "  2.337365706761678,\n",
       "  2.3202757438023887,\n",
       "  2.323006272315979,\n",
       "  2.3393031557401023,\n",
       "  2.340575853983561,\n",
       "  2.3420414527257285,\n",
       "  2.342699646949768,\n",
       "  2.338997999827067,\n",
       "  2.345905820528666,\n",
       "  2.3282124598821006,\n",
       "  2.320876717567444,\n",
       "  2.3267672260602317],\n",
       " 'train_ATT_loss': [1.8532001830204041,\n",
       "  1.7776473103467778,\n",
       "  1.6890199808532842,\n",
       "  1.596837328081316,\n",
       "  1.5042494182111155,\n",
       "  1.4120924202028735,\n",
       "  1.3285729832266175,\n",
       "  1.2471784141915656,\n",
       "  1.1763947257705012,\n",
       "  1.1157375930749147,\n",
       "  1.06068320122452,\n",
       "  1.0168971080859281,\n",
       "  0.9788560883820552,\n",
       "  0.9477597227387151,\n",
       "  0.9203120628552424,\n",
       "  0.8994157225141235,\n",
       "  0.8788090704880923,\n",
       "  0.8630103480452646,\n",
       "  0.8490958230317134,\n",
       "  0.838406703313632,\n",
       "  0.8288723106529574,\n",
       "  0.8206878297877114,\n",
       "  0.8140230418242246,\n",
       "  0.8077772457844002,\n",
       "  0.8037556568341242,\n",
       "  0.7980426724927907,\n",
       "  0.7959237567605735,\n",
       "  0.7901630530396987,\n",
       "  0.7873424494695795,\n",
       "  0.7848718891183425,\n",
       "  0.7826293995836104,\n",
       "  0.7801715346914909,\n",
       "  0.778477581088893,\n",
       "  0.7746466066368399,\n",
       "  0.7732256738432887,\n",
       "  0.7714343145283306,\n",
       "  0.7698662832503174,\n",
       "  0.7684384295484696,\n",
       "  0.7671561678690924,\n",
       "  0.7659320567123117,\n",
       "  0.7642437386380668,\n",
       "  0.7634141494692858,\n",
       "  0.7621951634864067,\n",
       "  0.760398675365131,\n",
       "  0.7603445764723907,\n",
       "  0.7595063690663705,\n",
       "  0.7574731666295482,\n",
       "  0.7574498501510831,\n",
       "  0.7559135330020555,\n",
       "  0.7556448704317996,\n",
       "  0.7545122609904599,\n",
       "  0.753666478511039,\n",
       "  0.7542994073553428,\n",
       "  0.7523661595962715,\n",
       "  0.7516431181054366,\n",
       "  0.7507087652703072,\n",
       "  0.7511410250888306,\n",
       "  0.7503145347011386,\n",
       "  0.7498333308835439,\n",
       "  0.7493915894685359,\n",
       "  0.7486384142799061,\n",
       "  0.7486564870356193,\n",
       "  0.748575861269087,\n",
       "  0.7479037097616539,\n",
       "  0.7462827421291383,\n",
       "  0.746590820541012,\n",
       "  0.7450899215946567,\n",
       "  0.7445979950500657,\n",
       "  0.7441276505052878,\n",
       "  0.7440025273122286,\n",
       "  0.7443718551928977,\n",
       "  0.7431464505657925,\n",
       "  0.7428064625349071,\n",
       "  0.7428604158007868,\n",
       "  0.7419323401107683,\n",
       "  0.7422525172748724,\n",
       "  0.7427585402353979,\n",
       "  0.740982572457797,\n",
       "  0.7412498086443238,\n",
       "  0.7405156432096317,\n",
       "  0.7419787452161477,\n",
       "  0.7423888141098445,\n",
       "  0.7395259503512501,\n",
       "  0.7390721754023903,\n",
       "  0.7402037996334382,\n",
       "  0.7392368031010403,\n",
       "  0.7395902409778077,\n",
       "  0.7411323848523592,\n",
       "  0.7393561821234854,\n",
       "  0.7382916246754971,\n",
       "  0.7390845294144015,\n",
       "  0.738854606726163,\n",
       "  0.7371043615724241,\n",
       "  0.7376131813281791,\n",
       "  0.7367665032930982,\n",
       "  0.736567066812119,\n",
       "  0.7365485317489117,\n",
       "  0.7367191284977498,\n",
       "  0.7362648529359178,\n",
       "  0.7362723201926065,\n",
       "  0.7353496729832276,\n",
       "  0.7366520409108529,\n",
       "  0.7351883288235546,\n",
       "  0.7358779902273268,\n",
       "  0.735584875057939,\n",
       "  0.7342957930221452,\n",
       "  0.7345288952301744,\n",
       "  0.7343491025248393,\n",
       "  0.7337812878748716,\n",
       "  0.7339543711775888,\n",
       "  0.7336702183343037,\n",
       "  0.7338763186475907,\n",
       "  0.7337280596062087,\n",
       "  0.7332413026500607,\n",
       "  0.7329091664166332,\n",
       "  0.7327013908991192,\n",
       "  0.7323159968423711,\n",
       "  0.732326660460052,\n",
       "  0.7326873516111823,\n",
       "  0.7324774149712433,\n",
       "  0.7319303993703256,\n",
       "  0.7318530624262845,\n",
       "  0.7317473341883715,\n",
       "  0.7317556722342473,\n",
       "  0.7316551107779103,\n",
       "  0.7311899957564398,\n",
       "  0.731713398341657,\n",
       "  0.7321104207527604,\n",
       "  0.7311428618563179,\n",
       "  0.7309487258958685,\n",
       "  0.7338046997868123,\n",
       "  0.7317968010902405,\n",
       "  0.7329095914093081,\n",
       "  0.7315538094644731,\n",
       "  0.731093992486885,\n",
       "  0.7306223685061172,\n",
       "  0.7317365399688235,\n",
       "  0.7301555462491149,\n",
       "  0.7299735265425368,\n",
       "  0.7296640184447376,\n",
       "  0.7299872237229281,\n",
       "  0.7300789024360953,\n",
       "  0.7299988264852614,\n",
       "  0.7300869505491283,\n",
       "  0.7310722850036093,\n",
       "  0.729293672001593,\n",
       "  0.7299124700871201,\n",
       "  0.7291371620923198,\n",
       "  0.7293155208188741,\n",
       "  0.7293438533336501,\n",
       "  0.7290324894014818,\n",
       "  0.7303290195412253,\n",
       "  0.7292042224360965,\n",
       "  0.7288499079912983,\n",
       "  0.728675001876176,\n",
       "  0.7292910546807371,\n",
       "  0.7293093090572516,\n",
       "  0.7281923498803559,\n",
       "  0.7281015541745025,\n",
       "  0.7291165131280957,\n",
       "  0.7292670749561279,\n",
       "  0.7282308097361198,\n",
       "  0.7293972495189994,\n",
       "  0.7306242144338972,\n",
       "  0.7290684526316678,\n",
       "  0.7277001193685875,\n",
       "  0.7275822801603175,\n",
       "  0.7272060726488065,\n",
       "  0.7279571182509869,\n",
       "  0.7275230034236433,\n",
       "  0.7274508349122764,\n",
       "  0.7273870100605191,\n",
       "  0.7271634753060803,\n",
       "  0.728069175478494,\n",
       "  0.7273443957114814,\n",
       "  0.7274652062360599,\n",
       "  0.7277022005778601,\n",
       "  0.7277770866317432,\n",
       "  0.7269776265376823,\n",
       "  0.7281365632350425,\n",
       "  0.7269186757277915,\n",
       "  0.7269277118579833,\n",
       "  0.7265683118656402,\n",
       "  0.7260423462806976,\n",
       "  0.7298248753653339,\n",
       "  0.7272012573860359,\n",
       "  0.7268276751206523,\n",
       "  0.7274379436329131,\n",
       "  0.7262439331519637,\n",
       "  0.7261296225056424,\n",
       "  0.7260835015542619,\n",
       "  0.7262451777497817,\n",
       "  0.7263772639541415,\n",
       "  0.7257016860877378,\n",
       "  0.72641814118277,\n",
       "  0.7264384531578529,\n",
       "  0.7257744197700162,\n",
       "  0.7263420367835302,\n",
       "  0.7258310689490257,\n",
       "  0.7264738160487357,\n",
       "  0.726399896547735,\n",
       "  0.7252345780256382,\n",
       "  0.7259319705315904,\n",
       "  0.7257806969151273,\n",
       "  0.7269314103179361,\n",
       "  0.7250471849851,\n",
       "  0.7255012556456463,\n",
       "  0.7255887927441056,\n",
       "  0.7256967745659424,\n",
       "  0.7252712277824529,\n",
       "  0.7253981810527496,\n",
       "  0.725366256573854,\n",
       "  0.725282161684908,\n",
       "  0.7260907227973198,\n",
       "  0.7255211583134871,\n",
       "  0.7248249978570066,\n",
       "  0.7246593114741952,\n",
       "  0.7248049266120403,\n",
       "  0.7259795880383731,\n",
       "  0.7246588945719014,\n",
       "  0.7249160072149663,\n",
       "  0.724920851702175,\n",
       "  0.7262575456310177,\n",
       "  0.7254560697111727,\n",
       "  0.7245825426730422,\n",
       "  0.7242854197930101,\n",
       "  0.7242474267027055,\n",
       "  0.7259072083515473,\n",
       "  0.7243984539753182,\n",
       "  0.7250174762799799,\n",
       "  0.7246781325076095,\n",
       "  0.7245069431796298,\n",
       "  0.7247310644041468,\n",
       "  0.724472232802753,\n",
       "  0.7245838306947429,\n",
       "  0.724630523916757,\n",
       "  0.7260483493435086,\n",
       "  0.7247099764129131,\n",
       "  0.7245677169670358,\n",
       "  0.7254152248440687,\n",
       "  0.7256223333179125,\n",
       "  0.7241778852536738,\n",
       "  0.72400396732082,\n",
       "  0.7250591904502827,\n",
       "  0.7244116056328666,\n",
       "  0.7236389521086315,\n",
       "  0.7236586269249216,\n",
       "  0.7242784415916063,\n",
       "  0.7238537688995001,\n",
       "  0.7236851255649345,\n",
       "  0.7244751491374917,\n",
       "  0.7238893302523859,\n",
       "  0.7238475672095767,\n",
       "  0.7263983118236891,\n",
       "  0.7243216320088035,\n",
       "  0.7250160812671165,\n",
       "  0.7245870097192041,\n",
       "  0.7242323039641341,\n",
       "  0.7253330204625539,\n",
       "  0.7236008637499611,\n",
       "  0.7258608247104444,\n",
       "  0.7233291425203022,\n",
       "  0.7241020002853837,\n",
       "  0.7254553813683359,\n",
       "  0.723633699800169,\n",
       "  0.7231112665747012,\n",
       "  0.7240473514118353,\n",
       "  0.7239407338924355,\n",
       "  0.7232631667829287,\n",
       "  0.7232764576280546,\n",
       "  0.7235529828930165,\n",
       "  0.7233055390809712,\n",
       "  0.7242772595704097,\n",
       "  0.7253400445644875,\n",
       "  0.7243580197363348,\n",
       "  0.7240862641638336,\n",
       "  0.7231907529183702,\n",
       "  0.7255693045019113,\n",
       "  0.7236527896323692,\n",
       "  0.7238445944072798,\n",
       "  0.7233345430973824,\n",
       "  0.7237776778741557,\n",
       "  0.7235984541372579,\n",
       "  0.7228541613615781,\n",
       "  0.7233217421330904,\n",
       "  0.7241677527942816,\n",
       "  0.723748263890063,\n",
       "  0.7229621679498879,\n",
       "  0.72282485288266,\n",
       "  0.7239062132267411,\n",
       "  0.7241912246410867,\n",
       "  0.7242702825247747,\n",
       "  0.7235152792402252,\n",
       "  0.7233378139229032,\n",
       "  0.7229864942400079,\n",
       "  0.7233558501893463,\n",
       "  0.7228495259034006,\n",
       "  0.7233583755440329,\n",
       "  0.7237980126343936,\n",
       "  0.7235744080385013,\n",
       "  0.7229696738422743,\n",
       "  0.7229133155867664,\n",
       "  0.7225648433878151,\n",
       "  0.7227717334213679,\n",
       "  0.7231755022196888,\n",
       "  0.7234249673061424,\n",
       "  0.7221797683232378,\n",
       "  0.7227218471405579,\n",
       "  0.7227018893260375,\n",
       "  0.7235872976667663,\n",
       "  0.724191876824873,\n",
       "  0.7227291304649078,\n",
       "  0.7229820508045502,\n",
       "  0.7222965356385609,\n",
       "  0.7224212599262967,\n",
       "  0.7230102218419231,\n",
       "  0.7231117557950958,\n",
       "  0.7239929706766335,\n",
       "  0.7225665701063055,\n",
       "  0.7226329825591513,\n",
       "  0.7239367126427859,\n",
       "  0.722240401271968,\n",
       "  0.7230378537957358,\n",
       "  0.7220849999100217,\n",
       "  0.7227224970128068,\n",
       "  0.7232504709935915,\n",
       "  0.7244518244035356,\n",
       "  0.722379205794876,\n",
       "  0.7225144437805767,\n",
       "  0.7223131167921664,\n",
       "  0.7231993419311714,\n",
       "  0.7227160978515392,\n",
       "  0.7229145151096038,\n",
       "  0.7228111423283733,\n",
       "  0.7231903758075429,\n",
       "  0.7217913572147613,\n",
       "  0.7220722623479003,\n",
       "  0.72199449512767,\n",
       "  0.7232715649287786,\n",
       "  0.7228636599643739,\n",
       "  0.7221500267612637,\n",
       "  0.7236904705000056,\n",
       "  0.7238189485594837,\n",
       "  0.722490071589927,\n",
       "  0.7223381802976296,\n",
       "  0.7245659590427895,\n",
       "  0.7259759166564307,\n",
       "  0.7231150913436657,\n",
       "  0.7242170341127137,\n",
       "  0.7218882281364166,\n",
       "  0.7231741689578979,\n",
       "  0.723114707298226,\n",
       "  0.722048444430914,\n",
       "  0.7218747254553924,\n",
       "  0.7221595027440142,\n",
       "  0.7216675808224982,\n",
       "  0.7225825101384826,\n",
       "  0.7219351563097037,\n",
       "  0.7230528540228213,\n",
       "  0.7219632101191048,\n",
       "  0.7215334294245184,\n",
       "  0.7221935541676022,\n",
       "  0.7220908213520314,\n",
       "  0.7226232954009418,\n",
       "  0.721956675072456,\n",
       "  0.7218578642755334,\n",
       "  0.7223947089464711,\n",
       "  0.7219007901868001,\n",
       "  0.722438315440413,\n",
       "  0.7217418937801985,\n",
       "  0.7213342614121054,\n",
       "  0.7216521740950376,\n",
       "  0.7219265701037695,\n",
       "  0.7221096694634562,\n",
       "  0.7229301397820259,\n",
       "  0.7226136098938305,\n",
       "  0.7219067848290103,\n",
       "  0.7216220598802012,\n",
       "  0.7217942796585632,\n",
       "  0.7217748394633264,\n",
       "  0.7217053823854124,\n",
       "  0.7230024935466101,\n",
       "  0.7216325531045485,\n",
       "  0.7223805423588634,\n",
       "  0.7222476806336823,\n",
       "  0.7218268183459866,\n",
       "  0.72199986846163,\n",
       "  0.7214610568374148,\n",
       "  0.7222606860039307,\n",
       "  0.7220310283169522,\n",
       "  0.7213798687068379,\n",
       "  0.7210980640554032,\n",
       "  0.7224748859445144,\n",
       "  0.7210097558940877,\n",
       "  0.721347502394066,\n",
       "  0.7211208350110252,\n",
       "  0.7219207772257585,\n",
       "  0.7226679046068165,\n",
       "  0.7220337390899658,\n",
       "  0.7216097108214846,\n",
       "  0.7211815343339028,\n",
       "  0.7210059299693543,\n",
       "  0.7208803093004095,\n",
       "  0.7223405383960693,\n",
       "  0.7212364880001776,\n",
       "  0.7210766055577349,\n",
       "  0.7221702931660364,\n",
       "  0.721441136833043,\n",
       "  0.7216713937035558,\n",
       "  0.7211896182757666,\n",
       "  0.7219392069489011,\n",
       "  0.7220353215024742,\n",
       "  0.7211080596387551,\n",
       "  0.722845967126355,\n",
       "  0.7218316699329176,\n",
       "  0.7215366522030817,\n",
       "  0.7212540061520077,\n",
       "  0.7221111082304218,\n",
       "  0.7215358304515109,\n",
       "  0.721215145904932,\n",
       "  0.7211943882984467,\n",
       "  0.7222722767132471,\n",
       "  0.722457311325126,\n",
       "  0.721707814453051,\n",
       "  0.7215481867750596,\n",
       "  0.7212661263685147,\n",
       "  0.7226984664343731,\n",
       "  0.7219670180468678,\n",
       "  0.7213738080207001,\n",
       "  0.7212129733899294,\n",
       "  0.722089773565118,\n",
       "  0.7219962594911993,\n",
       "  0.7228609303688409,\n",
       "  0.7212944276775349,\n",
       "  0.721633738923271,\n",
       "  0.7213560228862921,\n",
       "  0.7213061481631694,\n",
       "  0.7213288177413624,\n",
       "  0.720482673836547,\n",
       "  0.7232328263676397,\n",
       "  0.7212356981148019,\n",
       "  0.722320821641885],\n",
       " 'train_VAL_loss': [1.7758707395881166,\n",
       "  1.7428227521045716,\n",
       "  1.7124897588323027,\n",
       "  1.6927034600619795,\n",
       "  1.6797415810608798,\n",
       "  1.6711814978116106,\n",
       "  1.6656361468280783,\n",
       "  1.659131530909657,\n",
       "  1.6546998938695217,\n",
       "  1.6514367977998263,\n",
       "  1.6482215290914943,\n",
       "  1.6455565730620618,\n",
       "  1.6433244174207016,\n",
       "  1.6409862540105042,\n",
       "  1.6392990073338771,\n",
       "  1.6377273573439537,\n",
       "  1.6354925094218795,\n",
       "  1.6347447551188377,\n",
       "  1.6334716174080761,\n",
       "  1.6317895505567006,\n",
       "  1.6310361798780446,\n",
       "  1.6310638460095899,\n",
       "  1.6286732589108792,\n",
       "  1.628753691498923,\n",
       "  1.6280739059738836,\n",
       "  1.6268813653005458,\n",
       "  1.625857249191263,\n",
       "  1.62579251590528,\n",
       "  1.624944516496315,\n",
       "  1.6251381470556074,\n",
       "  1.624137774068563,\n",
       "  1.6229454529912848,\n",
       "  1.6228367545928322,\n",
       "  1.6227996897499317,\n",
       "  1.6221300112573724,\n",
       "  1.62099075152273,\n",
       "  1.6208025520858342,\n",
       "  1.620362944550131,\n",
       "  1.6204808227243186,\n",
       "  1.6198382562547509,\n",
       "  1.6196873937617378,\n",
       "  1.6190741755625548,\n",
       "  1.6190398304085982,\n",
       "  1.6180249703557867,\n",
       "  1.6176407832518178,\n",
       "  1.6181083590700356,\n",
       "  1.6179985445292042,\n",
       "  1.617613014421965,\n",
       "  1.6172667619594245,\n",
       "  1.6174028609928333,\n",
       "  1.6173750802750733,\n",
       "  1.616569122118963,\n",
       "  1.6166273768258557,\n",
       "  1.6156158942595082,\n",
       "  1.6153988607042054,\n",
       "  1.6163927465264487,\n",
       "  1.6148685768370483,\n",
       "  1.615122168678326,\n",
       "  1.614966167637516,\n",
       "  1.6151992706710943,\n",
       "  1.614638035317207,\n",
       "  1.6138938328565984,\n",
       "  1.613600639425156,\n",
       "  1.6134457423085982,\n",
       "  1.613471773852932,\n",
       "  1.6131900895665556,\n",
       "  1.613028224155183,\n",
       "  1.6129666540431185,\n",
       "  1.613344001307712,\n",
       "  1.6119373247564004,\n",
       "  1.6121636932906682,\n",
       "  1.6114488381097851,\n",
       "  1.6113818801340964,\n",
       "  1.6108931071870545,\n",
       "  1.6107580384389186,\n",
       "  1.6101786601576449,\n",
       "  1.6102526686528382,\n",
       "  1.6103554085681313,\n",
       "  1.6104265213673135,\n",
       "  1.610195662836619,\n",
       "  1.610289893982483,\n",
       "  1.610065450298489,\n",
       "  1.610213727501951,\n",
       "  1.6103424375407254,\n",
       "  1.6118923009597694,\n",
       "  1.6110994376634296,\n",
       "  1.60985328385044,\n",
       "  1.6107204109017539,\n",
       "  1.6117943138957354,\n",
       "  1.6119560379731028,\n",
       "  1.612251270180594,\n",
       "  1.6127703810662775,\n",
       "  1.6108232449296438,\n",
       "  1.6100197102554616,\n",
       "  1.6087180177921074,\n",
       "  1.6083129357102834,\n",
       "  1.607711265291864,\n",
       "  1.6074693117115306,\n",
       "  1.6074209972762006,\n",
       "  1.6072743262610607,\n",
       "  1.6071082545119308,\n",
       "  1.6082995773352415,\n",
       "  1.609252419828378,\n",
       "  1.609627823089959,\n",
       "  1.6090323079656035,\n",
       "  1.6092627348992303,\n",
       "  1.6090166565453907,\n",
       "  1.608052344533545,\n",
       "  1.6064310773918173,\n",
       "  1.6060149032323314,\n",
       "  1.6055999848981313,\n",
       "  1.6055262844978608,\n",
       "  1.6060091752424794,\n",
       "  1.6057913957210128,\n",
       "  1.6058949555716686,\n",
       "  1.6056955015890486,\n",
       "  1.6064602285210776,\n",
       "  1.6070594470586803,\n",
       "  1.6058582031165465,\n",
       "  1.6049364930705021,\n",
       "  1.6043888386596934,\n",
       "  1.6041149657188691,\n",
       "  1.6043428532634745,\n",
       "  1.6047498265131688,\n",
       "  1.6051898811662626,\n",
       "  1.6051539265217873,\n",
       "  1.604959081414664,\n",
       "  1.6040174089310242,\n",
       "  1.6050255651949514,\n",
       "  1.6056989705463525,\n",
       "  1.6052968584599587,\n",
       "  1.6050451396244714,\n",
       "  1.6047350957453086,\n",
       "  1.6041410837807484,\n",
       "  1.604165303409925,\n",
       "  1.6048010373049495,\n",
       "  1.6046450210740362,\n",
       "  1.6039653012627049,\n",
       "  1.603794827685792,\n",
       "  1.6034592439593371,\n",
       "  1.6035914853669269,\n",
       "  1.602825854623747,\n",
       "  1.6026830554338705,\n",
       "  1.6030841256773043,\n",
       "  1.6034766739425237,\n",
       "  1.603108295113096,\n",
       "  1.6035232808121023,\n",
       "  1.6036172758509248,\n",
       "  1.6034879070239714,\n",
       "  1.603102876869265,\n",
       "  1.6039973972907027,\n",
       "  1.6038646427217944,\n",
       "  1.6047854793368945,\n",
       "  1.6038669968575983,\n",
       "  1.6036929836563787,\n",
       "  1.6048304720928794,\n",
       "  1.6037880958282387,\n",
       "  1.6016996808329447,\n",
       "  1.6016314904775646,\n",
       "  1.6019068120919435,\n",
       "  1.6015508151780866,\n",
       "  1.6020830684751686,\n",
       "  1.6020236728594244,\n",
       "  1.601901337050335,\n",
       "  1.602178488081512,\n",
       "  1.6027914364912503,\n",
       "  1.6024363169709732,\n",
       "  1.6040057752931547,\n",
       "  1.6045880410149487,\n",
       "  1.603733564017552,\n",
       "  1.6026150797212553,\n",
       "  1.601755038192728,\n",
       "  1.6010716186005653,\n",
       "  1.6020575986344399,\n",
       "  1.6012257809784274,\n",
       "  1.6009844125472938,\n",
       "  1.6011224096831853,\n",
       "  1.600961561678519,\n",
       "  1.600458359454147,\n",
       "  1.6008433410665666,\n",
       "  1.6005987252555065,\n",
       "  1.601173344411348,\n",
       "  1.6014016585997266,\n",
       "  1.5998884888567093,\n",
       "  1.600278108734173,\n",
       "  1.6001506330564081,\n",
       "  1.6000469440898737,\n",
       "  1.6006272149548306,\n",
       "  1.600988569352105,\n",
       "  1.6016371425168996,\n",
       "  1.6026765963377385,\n",
       "  1.6017025124663462,\n",
       "  1.6012975426591995,\n",
       "  1.6007120473893395,\n",
       "  1.6013293395082047,\n",
       "  1.6012169466124346,\n",
       "  1.6013522448632196,\n",
       "  1.6022312799649225,\n",
       "  1.603356598156641,\n",
       "  1.6016213560368546,\n",
       "  1.6019798596479886,\n",
       "  1.6021209314589355,\n",
       "  1.601713755454383,\n",
       "  1.6032841644789044,\n",
       "  1.6042027724416632,\n",
       "  1.6019729976838977,\n",
       "  1.600405203338475,\n",
       "  1.6016521272236621,\n",
       "  1.6020665726833396,\n",
       "  1.6003349913784672,\n",
       "  1.5992984695777999,\n",
       "  1.600088839715868,\n",
       "  1.6006277700540432,\n",
       "  1.6005851664371438,\n",
       "  1.600885522332548,\n",
       "  1.5998380035574746,\n",
       "  1.5992742347585198,\n",
       "  1.5997621894212972,\n",
       "  1.5993765008086314,\n",
       "  1.5991132599495124,\n",
       "  1.5992403912081943,\n",
       "  1.599314651660972,\n",
       "  1.599366080397714,\n",
       "  1.6011642283680037,\n",
       "  1.6027024819249922,\n",
       "  1.602727498374157,\n",
       "  1.6060624868916011,\n",
       "  1.6088722009738066,\n",
       "  1.604661158577557,\n",
       "  1.607061830584032,\n",
       "  1.6041846635268997,\n",
       "  1.600826942689531,\n",
       "  1.5998032571866572,\n",
       "  1.59919303110762,\n",
       "  1.6000368261601456,\n",
       "  1.60046463494816,\n",
       "  1.600304592018973,\n",
       "  1.5995513643254204,\n",
       "  1.5989274291780846,\n",
       "  1.5979250814776012,\n",
       "  1.5977220512162946,\n",
       "  1.5981575987015404,\n",
       "  1.5980775828506808,\n",
       "  1.598152651681134,\n",
       "  1.5977686055809508,\n",
       "  1.597939811585022,\n",
       "  1.5974416395964055,\n",
       "  1.5973694783498706,\n",
       "  1.5980200239165667,\n",
       "  1.5984308019569375,\n",
       "  1.5981486210202247,\n",
       "  1.5987057002297398,\n",
       "  1.5981620054826182,\n",
       "  1.5983737660907311,\n",
       "  1.5978416154919568,\n",
       "  1.5974783088361788,\n",
       "  1.5970898727961194,\n",
       "  1.5974570156134398,\n",
       "  1.597866355547284,\n",
       "  1.597443023546911,\n",
       "  1.5971941515349286,\n",
       "  1.5976540890426847,\n",
       "  1.5983647253374644,\n",
       "  1.5983038189668735,\n",
       "  1.5980626958889315,\n",
       "  1.5973338892585354,\n",
       "  1.597912061247469,\n",
       "  1.5973229649324496,\n",
       "  1.596900919137569,\n",
       "  1.5967769236445757,\n",
       "  1.596757813173648,\n",
       "  1.5968829720304283,\n",
       "  1.596811448107796,\n",
       "  1.5972808239532639,\n",
       "  1.598531656318094,\n",
       "  1.5995863230274654,\n",
       "  1.6003106462658276,\n",
       "  1.5997589945462931,\n",
       "  1.598099845598279,\n",
       "  1.5975001125784791,\n",
       "  1.5969024781044832,\n",
       "  1.596979024667819,\n",
       "  1.597612252526006,\n",
       "  1.5982029728612082,\n",
       "  1.5989263741263393,\n",
       "  1.5980867781467385,\n",
       "  1.5972102249758395,\n",
       "  1.5965909789473727,\n",
       "  1.596414329272558,\n",
       "  1.596781813206765,\n",
       "  1.5968560888496461,\n",
       "  1.5964764224525303,\n",
       "  1.5969267172166186,\n",
       "  1.5967847270648565,\n",
       "  1.5969358619890714,\n",
       "  1.596909530275086,\n",
       "  1.5969570462393299,\n",
       "  1.5969229672424021,\n",
       "  1.5969369804429876,\n",
       "  1.5967022259809964,\n",
       "  1.5964894007447683,\n",
       "  1.5966839740811292,\n",
       "  1.5967670405010108,\n",
       "  1.5967109834058133,\n",
       "  1.596923507481731,\n",
       "  1.5973089675824068,\n",
       "  1.5971179913285696,\n",
       "  1.5965449453390868,\n",
       "  1.5973691246846375,\n",
       "  1.5969700955287902,\n",
       "  1.5968745966697333,\n",
       "  1.5964113335199963,\n",
       "  1.5961921251381532,\n",
       "  1.596440118765897,\n",
       "  1.5972687808430426,\n",
       "  1.5967459041326,\n",
       "  1.5974680825944092,\n",
       "  1.5974239622787094,\n",
       "  1.597443425754431,\n",
       "  1.5975332781878866,\n",
       "  1.5978580079250388,\n",
       "  1.598780402516394,\n",
       "  1.6016111284742065,\n",
       "  1.6005090049099064,\n",
       "  1.6010097172782032,\n",
       "  1.5989593023078263,\n",
       "  1.599010815580796,\n",
       "  1.597919738523848,\n",
       "  1.5967337124235412,\n",
       "  1.5962504769296197,\n",
       "  1.5975645179563611,\n",
       "  1.5993857991332163,\n",
       "  1.5985506138973289,\n",
       "  1.5972606990476064,\n",
       "  1.596027992108522,\n",
       "  1.5962139764320817,\n",
       "  1.5958310907237088,\n",
       "  1.5962796128687766,\n",
       "  1.596903442345828,\n",
       "  1.5966080129972124,\n",
       "  1.5968037741336136,\n",
       "  1.5961433873612465,\n",
       "  1.595892269195282,\n",
       "  1.595892289999119,\n",
       "  1.5962090231375021,\n",
       "  1.59575232607506,\n",
       "  1.5956602588584878,\n",
       "  1.59612779405969,\n",
       "  1.5968720704894976,\n",
       "  1.5986231004102078,\n",
       "  1.5975831340884898,\n",
       "  1.5964636594634967,\n",
       "  1.5961029093681611,\n",
       "  1.5965988177011547,\n",
       "  1.596512036970778,\n",
       "  1.5965714623062894,\n",
       "  1.5963411344385543,\n",
       "  1.5959943542850314,\n",
       "  1.5965253273205744,\n",
       "  1.597368952640206,\n",
       "  1.596569698933419,\n",
       "  1.5970771230158713,\n",
       "  1.5971125559133175,\n",
       "  1.5961653816402783,\n",
       "  1.5961672922911077,\n",
       "  1.5962321642693391,\n",
       "  1.596543201449175,\n",
       "  1.5979713961027997,\n",
       "  1.5970779095990506,\n",
       "  1.5974850657904247,\n",
       "  1.5978532210611571,\n",
       "  1.5968664914287027,\n",
       "  1.597082588150891,\n",
       "  1.5967028573609454,\n",
       "  1.5969816132595664,\n",
       "  1.5964310776824107,\n",
       "  1.5953701490180314,\n",
       "  1.5953307277277897,\n",
       "  1.5952969198411853,\n",
       "  1.5952559963818072,\n",
       "  1.595227731562057,\n",
       "  1.5955970739723904,\n",
       "  1.595637369023796,\n",
       "  1.597141446499283,\n",
       "  1.5990584121186318,\n",
       "  1.6004713367557262,\n",
       "  1.6023359014717165,\n",
       "  1.6003349563751854,\n",
       "  1.5987440582127452,\n",
       "  1.5986006524093923,\n",
       "  1.5973247989723227,\n",
       "  1.5957595146263734,\n",
       "  1.5961120372994124,\n",
       "  1.5958045945603432,\n",
       "  1.5960188780464954,\n",
       "  1.5962273998603926,\n",
       "  1.5959561065953853,\n",
       "  1.5959361504319631,\n",
       "  1.595635888649156,\n",
       "  1.5950776104121327,\n",
       "  1.594969346912944,\n",
       "  1.5952909633393433,\n",
       "  1.5956430851257408,\n",
       "  1.595247130644949,\n",
       "  1.5953745967463444,\n",
       "  1.5956393790707364,\n",
       "  1.5952447372130079,\n",
       "  1.595364222235957,\n",
       "  1.5950619490853306,\n",
       "  1.5950672814720555,\n",
       "  1.5950221236062512,\n",
       "  1.5948383259971386,\n",
       "  1.5949317943356374,\n",
       "  1.59500159087934,\n",
       "  1.595032822062104,\n",
       "  1.5949006925989717,\n",
       "  1.594531420855641,\n",
       "  1.5947387822777281,\n",
       "  1.594705760974303,\n",
       "  1.5946951308739152,\n",
       "  1.5947220242915061,\n",
       "  1.5945531529402799,\n",
       "  1.5947586327708658,\n",
       "  1.5946491751974639,\n",
       "  1.5956780821332641,\n",
       "  1.5947700979967196,\n",
       "  1.5952982222274399,\n",
       "  1.595186192243053,\n",
       "  1.5949710719803363,\n",
       "  1.5945905285198603,\n",
       "  1.5947847650321898,\n",
       "  1.5955363393160116,\n",
       "  1.5959894112272606,\n",
       "  1.5953939928572594,\n",
       "  1.5951092834287732,\n",
       "  1.5946429290929989,\n",
       "  1.59488790946654,\n",
       "  1.5950852163610696,\n",
       "  1.5949180545569126,\n",
       "  1.5949155244140414,\n",
       "  1.5954686231560324,\n",
       "  1.5957921736788552],\n",
       " 'train_ATT_acc': [42.659279778393355,\n",
       "  40.99722991689751,\n",
       "  54.57063711911357,\n",
       "  61.772853185595565,\n",
       "  67.03601108033241,\n",
       "  72.29916897506925,\n",
       "  74.51523545706371,\n",
       "  81.16343490304709,\n",
       "  82.54847645429363,\n",
       "  84.48753462603878,\n",
       "  86.98060941828255,\n",
       "  89.19667590027701,\n",
       "  92.24376731301939,\n",
       "  93.07479224376732,\n",
       "  95.29085872576178,\n",
       "  95.8448753462604,\n",
       "  96.1218836565097,\n",
       "  96.39889196675901,\n",
       "  96.67590027700831,\n",
       "  97.22991689750693,\n",
       "  97.78393351800554,\n",
       "  97.78393351800554,\n",
       "  98.06094182825485,\n",
       "  98.06094182825485,\n",
       "  98.61495844875347,\n",
       "  99.16897506925208,\n",
       "  98.61495844875347,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0],\n",
       " 'train_VAL_acc': [88.6426592797784,\n",
       "  94.45983379501385,\n",
       "  94.73684210526316,\n",
       "  96.67590027700831,\n",
       "  97.50692520775624,\n",
       "  96.95290858725762,\n",
       "  98.61495844875347,\n",
       "  98.33795013850416,\n",
       "  98.33795013850416,\n",
       "  98.89196675900277,\n",
       "  98.89196675900277,\n",
       "  98.89196675900277,\n",
       "  99.16897506925208,\n",
       "  99.16897506925208,\n",
       "  99.16897506925208,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.16897506925208,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0],\n",
       " 'train_VAL_jac': [0.008310249307479225,\n",
       "  0.2880886426592798,\n",
       "  0.45337026733440705,\n",
       "  0.5115420151285187,\n",
       "  0.565096956871223,\n",
       "  0.6126500549739088,\n",
       "  0.6191135839742307,\n",
       "  0.6389658418058358,\n",
       "  0.6602031464722018,\n",
       "  0.6680517170237702,\n",
       "  0.6694367480079884,\n",
       "  0.6809787644573856,\n",
       "  0.686057251906461,\n",
       "  0.6975992577888298,\n",
       "  0.7059095176633375,\n",
       "  0.7063712017027625,\n",
       "  0.7248384153413641,\n",
       "  0.7202216172152279,\n",
       "  0.7206832959711387,\n",
       "  0.7331486699323575,\n",
       "  0.731763628381111,\n",
       "  0.7243767418689675,\n",
       "  0.740535556444501,\n",
       "  0.7331486699323575,\n",
       "  0.7396121989326794,\n",
       "  0.7451523598541513,\n",
       "  0.749307484507891,\n",
       "  0.7423822767516582,\n",
       "  0.7530009251222056,\n",
       "  0.7516158888544733,\n",
       "  0.7548476507128771,\n",
       "  0.7590027753666168,\n",
       "  0.7617728584691098,\n",
       "  0.7590027753666168,\n",
       "  0.7571560550594594,\n",
       "  0.7640812575321778,\n",
       "  0.7686980662253424,\n",
       "  0.7770083155328217,\n",
       "  0.7765466367769109,\n",
       "  0.7751615952256644,\n",
       "  0.7626962212644456,\n",
       "  0.7682363821859175,\n",
       "  0.7719298280837463,\n",
       "  0.788088647942794,\n",
       "  0.7922437725965336,\n",
       "  0.7746999111862394,\n",
       "  0.7751615899421501,\n",
       "  0.7825484817378079,\n",
       "  0.7797783986353147,\n",
       "  0.7797783986353147,\n",
       "  0.7783933570840682,\n",
       "  0.7774699942887324,\n",
       "  0.7742382324303286,\n",
       "  0.7857802435962117,\n",
       "  0.7894736894940405,\n",
       "  0.782086802981897,\n",
       "  0.788088647942794,\n",
       "  0.7867036063915475,\n",
       "  0.790858731045287,\n",
       "  0.7811634401865614,\n",
       "  0.7839335232890544,\n",
       "  0.7936288141477802,\n",
       "  0.7963988972502732,\n",
       "  0.7936288141477802,\n",
       "  0.7922437725965336,\n",
       "  0.7936288141477802,\n",
       "  0.7894736894940405,\n",
       "  0.797322260045609,\n",
       "  0.796860576006184,\n",
       "  0.8010157006599236,\n",
       "  0.8019390634552593,\n",
       "  0.8139427586605674,\n",
       "  0.8070175509043348,\n",
       "  0.815327800211814,\n",
       "  0.818559567353732,\n",
       "  0.8231763654798682,\n",
       "  0.815327800211814,\n",
       "  0.8185595620702178,\n",
       "  0.8162511577236355,\n",
       "  0.814866116172389,\n",
       "  0.8190212461096428,\n",
       "  0.824561412314629,\n",
       "  0.8199446089049786,\n",
       "  0.8217913344956501,\n",
       "  0.8084025977390955,\n",
       "  0.8148661267394174,\n",
       "  0.8199446141884928,\n",
       "  0.8264081379053005,\n",
       "  0.8111726808415886,\n",
       "  0.807017556187849,\n",
       "  0.8111726808415886,\n",
       "  0.8084025977390955,\n",
       "  0.824099728275204,\n",
       "  0.8227146920074716,\n",
       "  0.8305632572755259,\n",
       "  0.8305632572755259,\n",
       "  0.8374884650317586,\n",
       "  0.8287165369683686,\n",
       "  0.8282548582124578,\n",
       "  0.8287165369683686,\n",
       "  0.8314866200708616,\n",
       "  0.809325955250917,\n",
       "  0.8125577171093209,\n",
       "  0.806094188108999,\n",
       "  0.806555872148424,\n",
       "  0.8051708305971774,\n",
       "  0.7996306643921913,\n",
       "  0.8037857890459309,\n",
       "  0.8134810799046567,\n",
       "  0.8231763707633825,\n",
       "  0.8291782157242793,\n",
       "  0.8259464485823613,\n",
       "  0.8190212408261286,\n",
       "  0.8185595620702178,\n",
       "  0.8222530079680467,\n",
       "  0.8407202268901625,\n",
       "  0.8337950191339297,\n",
       "  0.8167128364795463,\n",
       "  0.8337950191339297,\n",
       "  0.8388735012994909,\n",
       "  0.8337950191339297,\n",
       "  0.8462603930951486,\n",
       "  0.8365651022364228,\n",
       "  0.8227146867239574,\n",
       "  0.8167128417630605,\n",
       "  0.8171745205189713,\n",
       "  0.8264081326217863,\n",
       "  0.8434903099926555,\n",
       "  0.8254847698264505,\n",
       "  0.8102493127627386,\n",
       "  0.8116343543139851,\n",
       "  0.8194829248655536,\n",
       "  0.8282548529289436,\n",
       "  0.832871656338594,\n",
       "  0.8384118225435802,\n",
       "  0.8259464485823613,\n",
       "  0.8342566978898405,\n",
       "  0.8379501437876693,\n",
       "  0.8337950191339297,\n",
       "  0.836103423480512,\n",
       "  0.8351800606851763,\n",
       "  0.8490304761976416,\n",
       "  0.8536472848908062,\n",
       "  0.8393351906224301,\n",
       "  0.8444136780715055,\n",
       "  0.845798719622752,\n",
       "  0.8416435949690124,\n",
       "  0.8448753568274162,\n",
       "  0.8439519940320804,\n",
       "  0.8494921602370666,\n",
       "  0.8453370302998128,\n",
       "  0.8481071186858201,\n",
       "  0.8467220771345736,\n",
       "  0.8467220771345736,\n",
       "  0.8494921549535525,\n",
       "  0.8388735065830051,\n",
       "  0.8610341714029497,\n",
       "  0.8776546700179082,\n",
       "  0.8702677782222504,\n",
       "  0.8610341714029497,\n",
       "  0.8504155177488881,\n",
       "  0.8444136727879913,\n",
       "  0.8476454346463951,\n",
       "  0.8379501437876693,\n",
       "  0.836103423480512,\n",
       "  0.8370267862758478,\n",
       "  0.8384118278270943,\n",
       "  0.8379501437876693,\n",
       "  0.8291782104407651,\n",
       "  0.8324099775826832,\n",
       "  0.8402585481342516,\n",
       "  0.8499538389929774,\n",
       "  0.8568790467492101,\n",
       "  0.8536472848908062,\n",
       "  0.8578024095445459,\n",
       "  0.854570647686142,\n",
       "  0.8679593791591824,\n",
       "  0.8596491351352173,\n",
       "  0.8591874510957924,\n",
       "  0.851800564583649,\n",
       "  0.8734995453641685,\n",
       "  0.8647276173007785,\n",
       "  0.8725761825688327,\n",
       "  0.8845798777741408,\n",
       "  0.8721144985294078,\n",
       "  0.8698060994663397,\n",
       "  0.8841181937347159,\n",
       "  0.8758079444272366,\n",
       "  0.8665743376079359,\n",
       "  0.8573407255051209,\n",
       "  0.8518005593001348,\n",
       "  0.8573407255051209,\n",
       "  0.8670360163638466,\n",
       "  0.8554940104814778,\n",
       "  0.8568790520327243,\n",
       "  0.8490304814811559,\n",
       "  0.8444136780715055,\n",
       "  0.8227146920074716,\n",
       "  0.8199446089049786,\n",
       "  0.8374884703152727,\n",
       "  0.8356417447246013,\n",
       "  0.8384118331106085,\n",
       "  0.8462603983786627,\n",
       "  0.8180978885978213,\n",
       "  0.8097876340068279,\n",
       "  0.8393351906224301,\n",
       "  0.8499538442764917,\n",
       "  0.8425669524808339,\n",
       "  0.8430286312367448,\n",
       "  0.8554940051979636,\n",
       "  0.8808864371598262,\n",
       "  0.8638042545054427,\n",
       "  0.863342575749532,\n",
       "  0.8744229028759901,\n",
       "  0.8628808917101071,\n",
       "  0.863342575749532,\n",
       "  0.8771929912619973,\n",
       "  0.8753462656713259,\n",
       "  0.8758079497107508,\n",
       "  0.8679593791591824,\n",
       "  0.8591874510957924,\n",
       "  0.8582640883004566,\n",
       "  0.8610341714029497,\n",
       "  0.8370267809923336,\n",
       "  0.8365651022364228,\n",
       "  0.8310249360314367,\n",
       "  0.8107109968021636,\n",
       "  0.8060941933925132,\n",
       "  0.8319483041102866,\n",
       "  0.8361034287640262,\n",
       "  0.8388735118665194,\n",
       "  0.8559556892373885,\n",
       "  0.8684210631986073,\n",
       "  0.872576187852347,\n",
       "  0.8578024095445459,\n",
       "  0.8624192129541962,\n",
       "  0.8642659385448678,\n",
       "  0.8716528250570112,\n",
       "  0.8541089689302312,\n",
       "  0.8804247531204012,\n",
       "  0.8928901270816201,\n",
       "  0.8933518058375308,\n",
       "  0.8933518058375308,\n",
       "  0.8970452517353596,\n",
       "  0.9030471019797708,\n",
       "  0.8919667642862843,\n",
       "  0.9025854179403459,\n",
       "  0.8850415618135659,\n",
       "  0.8836565202623193,\n",
       "  0.8684210631986073,\n",
       "  0.8781163540573331,\n",
       "  0.8647276173007785,\n",
       "  0.8698061047498539,\n",
       "  0.8651893013402036,\n",
       "  0.883656514978805,\n",
       "  0.890120043979127,\n",
       "  0.8965835729794489,\n",
       "  0.8984302932866062,\n",
       "  0.8785780275297297,\n",
       "  0.8975069304912705,\n",
       "  0.9044321382475031,\n",
       "  0.9016620551450101,\n",
       "  0.8928901270816201,\n",
       "  0.8822714734275585,\n",
       "  0.8928901217981059,\n",
       "  0.9118190247596466,\n",
       "  0.8988919667590027,\n",
       "  0.8984302932866062,\n",
       "  0.8956602101841131,\n",
       "  0.9016620551450101,\n",
       "  0.899815340121367,\n",
       "  0.8988919773260312,\n",
       "  0.8868882874042373,\n",
       "  0.8698061047498539,\n",
       "  0.8614958554423747,\n",
       "  0.8462603983786627,\n",
       "  0.8397968693783409,\n",
       "  0.8499538442764917,\n",
       "  0.8767313125060866,\n",
       "  0.8850415618135659,\n",
       "  0.899815340121367,\n",
       "  0.9025854179403459,\n",
       "  0.8947368473887773,\n",
       "  0.8873499608766339,\n",
       "  0.8739612241200793,\n",
       "  0.880886431876312,\n",
       "  0.8924284483257093,\n",
       "  0.9090489522241819,\n",
       "  0.9030470966962566,\n",
       "  0.9030471019797708,\n",
       "  0.8961218942235382,\n",
       "  0.8975069357747847,\n",
       "  0.8868882874042373,\n",
       "  0.8970452570188739,\n",
       "  0.8942751739163808,\n",
       "  0.882271483994587,\n",
       "  0.8739612346871077,\n",
       "  0.8882733289554839,\n",
       "  0.8721145090964362,\n",
       "  0.8762696337501759,\n",
       "  0.8813481211992512,\n",
       "  0.8896583705067305,\n",
       "  0.8988919773260312,\n",
       "  0.8901200492626412,\n",
       "  0.8882733236719697,\n",
       "  0.8905817227350378,\n",
       "  0.8891966864673054,\n",
       "  0.8919667695697985,\n",
       "  0.8827331574669836,\n",
       "  0.8919667642862843,\n",
       "  0.8919667642862843,\n",
       "  0.9072022213499962,\n",
       "  0.8993536507984278,\n",
       "  0.8993536507984278,\n",
       "  0.8887350024278805,\n",
       "  0.9099723044524893,\n",
       "  0.8799630690809762,\n",
       "  0.8919667590027701,\n",
       "  0.8887350024278805,\n",
       "  0.8674976951197574,\n",
       "  0.869344420710429,\n",
       "  0.8587257723398817,\n",
       "  0.8287165369683686,\n",
       "  0.8434903152761697,\n",
       "  0.8287165369683686,\n",
       "  0.8504155177488881,\n",
       "  0.8670360110803325,\n",
       "  0.8818097893881336,\n",
       "  0.889658359939702,\n",
       "  0.9016620551450101,\n",
       "  0.8795013956085797,\n",
       "  0.8831948362228943,\n",
       "  0.8878116449160589,\n",
       "  0.8795013956085797,\n",
       "  0.893351811121045,\n",
       "  0.8873499661601482,\n",
       "  0.8915050908138877,\n",
       "  0.8758079549942651,\n",
       "  0.8711911463011005,\n",
       "  0.8665743376079359,\n",
       "  0.8762696284666616,\n",
       "  0.8965835676959347,\n",
       "  0.8998153348378527,\n",
       "  0.8993536613654562,\n",
       "  0.9002770188772777,\n",
       "  0.91135735128725,\n",
       "  0.9136657503503182,\n",
       "  0.907663900105907,\n",
       "  0.9016620551450101,\n",
       "  0.8744229028759901,\n",
       "  0.8951985261446882,\n",
       "  0.8975069304912705,\n",
       "  0.898891972042517,\n",
       "  0.8799630690809762,\n",
       "  0.8776546647343939,\n",
       "  0.8702677782222504,\n",
       "  0.8781163487738189,\n",
       "  0.9016620604285243,\n",
       "  0.8896583652232162,\n",
       "  0.8688827419545182,\n",
       "  0.8702677835057647,\n",
       "  0.8841181990182301,\n",
       "  0.8758079549942651,\n",
       "  0.89381349516047,\n",
       "  0.8915050908138877,\n",
       "  0.8910434067744628,\n",
       "  0.8813481106322227,\n",
       "  0.8734995400806543,\n",
       "  0.8831948309393801,\n",
       "  0.880424747836887,\n",
       "  0.8855032352859624,\n",
       "  0.893351811121045,\n",
       "  0.8711911463011005,\n",
       "  0.8822714787110728,\n",
       "  0.8771929965455116,\n",
       "  0.8790397168526689,\n",
       "  0.9030471019797708,\n",
       "  0.9136657503503182,\n",
       "  0.9053555010428389,\n",
       "  0.908125584145332,\n",
       "  0.9058171797987496,\n",
       "  0.8965835729794489,\n",
       "  0.9099723044524893,\n",
       "  0.8758079444272366,\n",
       "  0.8614958501588604,\n",
       "  0.8434903099926555,\n",
       "  0.8277931741730328,\n",
       "  0.8439519940320804,\n",
       "  0.8568790467492101,\n",
       "  0.8582640883004566,\n",
       "  0.8831948362228943,\n",
       "  0.8951985314282024,\n",
       "  0.8915050855303736,\n",
       "  0.8915050855303736,\n",
       "  0.8919667642862843,\n",
       "  0.8933518058375308,\n",
       "  0.9067405425940854,\n",
       "  0.8942751686328666,\n",
       "  0.8910434014909485,\n",
       "  0.9016620551450101,\n",
       "  0.9159741494133862,\n",
       "  0.9053554957593247,\n",
       "  0.9025854179403459,\n",
       "  0.9035087754521673,\n",
       "  0.8988919773260312,\n",
       "  0.8988919773260312,\n",
       "  0.908587268184757,\n",
       "  0.8984302932866062,\n",
       "  0.908125584145332,\n",
       "  0.9099723044524893,\n",
       "  0.9127423875549824,\n",
       "  0.9238227199649547,\n",
       "  0.9062788585546604,\n",
       "  0.9048938170034139,\n",
       "  0.9192059112717901,\n",
       "  0.9247460774767762,\n",
       "  0.9256694455556261,\n",
       "  0.9145891078621396,\n",
       "  0.9219759996577973,\n",
       "  0.926131124311537,\n",
       "  0.9219759943742831,\n",
       "  0.9228993677366474,\n",
       "  0.9192059165553043,\n",
       "  0.9224376836972223,\n",
       "  0.9058171850822639,\n",
       "  0.9247460827602905,\n",
       "  0.9155124759409896,\n",
       "  0.91135735128725,\n",
       "  0.9164358334528112,\n",
       "  0.9279778446186943,\n",
       "  0.9233610412090439,\n",
       "  0.9048938170034139,\n",
       "  0.8961218889400239,\n",
       "  0.8979686092471812,\n",
       "  0.9025854126568316,\n",
       "  0.9233610412090439,\n",
       "  0.9044321435310173,\n",
       "  0.8924284483257093,\n",
       "  0.902123739184435,\n",
       "  0.902123739184435,\n",
       "  0.8845798830576551,\n",
       "  0.8734995506476828],\n",
       " 'train_VAL_acc_1': [55.40166204986149,\n",
       "  61.49584487534626,\n",
       "  61.772853185595565,\n",
       "  65.09695290858726,\n",
       "  66.20498614958449,\n",
       "  66.20498614958449,\n",
       "  70.08310249307479,\n",
       "  69.52908587257618,\n",
       "  70.08310249307479,\n",
       "  73.13019390581718,\n",
       "  73.6842105263158,\n",
       "  72.85318559556787,\n",
       "  73.13019390581718,\n",
       "  75.34626038781164,\n",
       "  75.06925207756233,\n",
       "  74.51523545706371,\n",
       "  76.45429362880887,\n",
       "  75.90027700831025,\n",
       "  76.17728531855956,\n",
       "  76.73130193905817,\n",
       "  77.28531855955679,\n",
       "  76.73130193905817,\n",
       "  78.94736842105263,\n",
       "  77.5623268698061,\n",
       "  76.17728531855956,\n",
       "  77.5623268698061,\n",
       "  78.94736842105263,\n",
       "  77.5623268698061,\n",
       "  79.50138504155125,\n",
       "  78.39335180055402,\n",
       "  79.22437673130194,\n",
       "  79.77839335180056,\n",
       "  79.77839335180056,\n",
       "  80.33240997229917,\n",
       "  80.60941828254848,\n",
       "  80.88642659279779,\n",
       "  80.88642659279779,\n",
       "  81.4404432132964,\n",
       "  81.4404432132964,\n",
       "  81.16343490304709,\n",
       "  80.88642659279779,\n",
       "  81.4404432132964,\n",
       "  80.88642659279779,\n",
       "  81.16343490304709,\n",
       "  82.27146814404432,\n",
       "  80.33240997229917,\n",
       "  79.50138504155125,\n",
       "  81.4404432132964,\n",
       "  81.16343490304709,\n",
       "  79.50138504155125,\n",
       "  81.16343490304709,\n",
       "  83.10249307479225,\n",
       "  81.4404432132964,\n",
       "  83.93351800554017,\n",
       "  83.37950138504155,\n",
       "  81.16343490304709,\n",
       "  82.82548476454294,\n",
       "  82.82548476454294,\n",
       "  83.37950138504155,\n",
       "  80.88642659279779,\n",
       "  83.10249307479225,\n",
       "  82.82548476454294,\n",
       "  81.99445983379502,\n",
       "  83.10249307479225,\n",
       "  83.10249307479225,\n",
       "  83.10249307479225,\n",
       "  83.10249307479225,\n",
       "  84.21052631578948,\n",
       "  83.37950138504155,\n",
       "  84.7645429362881,\n",
       "  83.93351800554017,\n",
       "  84.7645429362881,\n",
       "  84.48753462603878,\n",
       "  84.48753462603878,\n",
       "  84.48753462603878,\n",
       "  86.42659279778394,\n",
       "  87.53462603878117,\n",
       "  86.70360110803324,\n",
       "  86.98060941828255,\n",
       "  86.42659279778394,\n",
       "  86.14958448753463,\n",
       "  86.42659279778394,\n",
       "  85.87257617728532,\n",
       "  85.0415512465374,\n",
       "  84.21052631578948,\n",
       "  84.48753462603878,\n",
       "  85.31855955678671,\n",
       "  84.21052631578948,\n",
       "  84.48753462603878,\n",
       "  82.54847645429363,\n",
       "  83.10249307479225,\n",
       "  83.10249307479225,\n",
       "  84.48753462603878,\n",
       "  83.93351800554017,\n",
       "  85.0415512465374,\n",
       "  84.21052631578948,\n",
       "  84.48753462603878,\n",
       "  83.93351800554017,\n",
       "  85.0415512465374,\n",
       "  84.21052631578948,\n",
       "  85.0415512465374,\n",
       "  82.54847645429363,\n",
       "  82.54847645429363,\n",
       "  82.27146814404432,\n",
       "  83.65650969529086,\n",
       "  83.65650969529086,\n",
       "  83.93351800554017,\n",
       "  84.7645429362881,\n",
       "  86.42659279778394,\n",
       "  86.70360110803324,\n",
       "  86.14958448753463,\n",
       "  86.70360110803324,\n",
       "  86.98060941828255,\n",
       "  87.25761772853186,\n",
       "  88.08864265927978,\n",
       "  86.98060941828255,\n",
       "  86.42659279778394,\n",
       "  86.70360110803324,\n",
       "  88.36565096952909,\n",
       "  88.36565096952909,\n",
       "  88.9196675900277,\n",
       "  88.08864265927978,\n",
       "  88.6426592797784,\n",
       "  87.25761772853186,\n",
       "  85.87257617728532,\n",
       "  85.0415512465374,\n",
       "  85.0415512465374,\n",
       "  88.08864265927978,\n",
       "  85.31855955678671,\n",
       "  84.7645429362881,\n",
       "  84.7645429362881,\n",
       "  85.59556786703601,\n",
       "  85.87257617728532,\n",
       "  87.25761772853186,\n",
       "  88.6426592797784,\n",
       "  87.25761772853186,\n",
       "  88.36565096952909,\n",
       "  88.36565096952909,\n",
       "  87.53462603878117,\n",
       "  88.6426592797784,\n",
       "  87.53462603878117,\n",
       "  88.6426592797784,\n",
       "  89.19667590027701,\n",
       "  89.75069252077563,\n",
       "  86.70360110803324,\n",
       "  86.42659279778394,\n",
       "  86.14958448753463,\n",
       "  86.98060941828255,\n",
       "  87.25761772853186,\n",
       "  86.98060941828255,\n",
       "  86.70360110803324,\n",
       "  86.14958448753463,\n",
       "  85.0415512465374,\n",
       "  86.98060941828255,\n",
       "  86.42659279778394,\n",
       "  85.87257617728532,\n",
       "  85.87257617728532,\n",
       "  88.6426592797784,\n",
       "  88.6426592797784,\n",
       "  88.6426592797784,\n",
       "  88.36565096952909,\n",
       "  87.81163434903047,\n",
       "  89.47368421052632,\n",
       "  88.36565096952909,\n",
       "  88.08864265927978,\n",
       "  86.70360110803324,\n",
       "  86.98060941828255,\n",
       "  83.93351800554017,\n",
       "  83.93351800554017,\n",
       "  83.93351800554017,\n",
       "  86.42659279778394,\n",
       "  88.36565096952909,\n",
       "  88.36565096952909,\n",
       "  87.81163434903047,\n",
       "  87.81163434903047,\n",
       "  87.81163434903047,\n",
       "  87.25761772853186,\n",
       "  89.19667590027701,\n",
       "  88.08864265927978,\n",
       "  87.81163434903047,\n",
       "  88.08864265927978,\n",
       "  87.81163434903047,\n",
       "  88.9196675900277,\n",
       "  89.47368421052632,\n",
       "  89.47368421052632,\n",
       "  90.85872576177286,\n",
       "  90.85872576177286,\n",
       "  89.47368421052632,\n",
       "  89.19667590027701,\n",
       "  87.53462603878117,\n",
       "  87.81163434903047,\n",
       "  88.9196675900277,\n",
       "  88.36565096952909,\n",
       "  90.85872576177286,\n",
       "  90.85872576177286,\n",
       "  88.36565096952909,\n",
       "  86.98060941828255,\n",
       "  86.14958448753463,\n",
       "  85.0415512465374,\n",
       "  86.98060941828255,\n",
       "  86.42659279778394,\n",
       "  86.98060941828255,\n",
       "  86.98060941828255,\n",
       "  86.70360110803324,\n",
       "  85.31855955678671,\n",
       "  87.53462603878117,\n",
       "  91.13573407202216,\n",
       "  88.6426592797784,\n",
       "  87.53462603878117,\n",
       "  89.19667590027701,\n",
       "  91.13573407202216,\n",
       "  88.6426592797784,\n",
       "  88.08864265927978,\n",
       "  88.08864265927978,\n",
       "  88.08864265927978,\n",
       "  88.9196675900277,\n",
       "  88.6426592797784,\n",
       "  88.36565096952909,\n",
       "  88.36565096952909,\n",
       "  88.6426592797784,\n",
       "  88.08864265927978,\n",
       "  87.81163434903047,\n",
       "  89.19667590027701,\n",
       "  87.53462603878117,\n",
       "  85.59556786703601,\n",
       "  85.59556786703601,\n",
       "  85.0415512465374,\n",
       "  83.93351800554017,\n",
       "  86.42659279778394,\n",
       "  84.7645429362881,\n",
       "  86.14958448753463,\n",
       "  87.53462603878117,\n",
       "  86.42659279778394,\n",
       "  87.81163434903047,\n",
       "  89.47368421052632,\n",
       "  90.58171745152355,\n",
       "  90.30470914127424,\n",
       "  90.58171745152355,\n",
       "  90.02770083102493,\n",
       "  92.797783933518,\n",
       "  93.07479224376732,\n",
       "  93.35180055401662,\n",
       "  93.62880886426593,\n",
       "  93.07479224376732,\n",
       "  91.68975069252078,\n",
       "  93.07479224376732,\n",
       "  92.797783933518,\n",
       "  91.68975069252078,\n",
       "  90.85872576177286,\n",
       "  90.02770083102493,\n",
       "  90.58171745152355,\n",
       "  89.19667590027701,\n",
       "  89.47368421052632,\n",
       "  88.9196675900277,\n",
       "  90.30470914127424,\n",
       "  91.96675900277009,\n",
       "  93.07479224376732,\n",
       "  93.62880886426593,\n",
       "  90.85872576177286,\n",
       "  94.18282548476455,\n",
       "  92.5207756232687,\n",
       "  90.58171745152355,\n",
       "  90.30470914127424,\n",
       "  91.13573407202216,\n",
       "  88.6426592797784,\n",
       "  89.75069252077563,\n",
       "  88.6426592797784,\n",
       "  89.47368421052632,\n",
       "  90.85872576177286,\n",
       "  92.24376731301939,\n",
       "  92.5207756232687,\n",
       "  91.68975069252078,\n",
       "  92.797783933518,\n",
       "  91.41274238227147,\n",
       "  88.08864265927978,\n",
       "  85.87257617728532,\n",
       "  85.59556786703601,\n",
       "  86.98060941828255,\n",
       "  89.19667590027701,\n",
       "  89.75069252077563,\n",
       "  92.24376731301939,\n",
       "  92.24376731301939,\n",
       "  90.58171745152355,\n",
       "  91.41274238227147,\n",
       "  89.19667590027701,\n",
       "  89.19667590027701,\n",
       "  90.30470914127424,\n",
       "  90.30470914127424,\n",
       "  91.68975069252078,\n",
       "  91.96675900277009,\n",
       "  91.41274238227147,\n",
       "  91.96675900277009,\n",
       "  92.5207756232687,\n",
       "  94.18282548476455,\n",
       "  93.07479224376732,\n",
       "  93.90581717451524,\n",
       "  91.96675900277009,\n",
       "  94.18282548476455,\n",
       "  91.68975069252078,\n",
       "  91.96675900277009,\n",
       "  91.68975069252078,\n",
       "  91.68975069252078,\n",
       "  91.96675900277009,\n",
       "  91.96675900277009,\n",
       "  91.68975069252078,\n",
       "  90.02770083102493,\n",
       "  90.58171745152355,\n",
       "  91.41274238227147,\n",
       "  91.68975069252078,\n",
       "  93.35180055401662,\n",
       "  93.35180055401662,\n",
       "  94.18282548476455,\n",
       "  92.797783933518,\n",
       "  92.5207756232687,\n",
       "  90.58171745152355,\n",
       "  92.24376731301939,\n",
       "  89.19667590027701,\n",
       "  88.6426592797784,\n",
       "  88.36565096952909,\n",
       "  86.70360110803324,\n",
       "  85.59556786703601,\n",
       "  85.31855955678671,\n",
       "  82.54847645429363,\n",
       "  84.48753462603878,\n",
       "  85.87257617728532,\n",
       "  88.36565096952909,\n",
       "  88.36565096952909,\n",
       "  89.47368421052632,\n",
       "  90.30470914127424,\n",
       "  91.68975069252078,\n",
       "  91.13573407202216,\n",
       "  89.19667590027701,\n",
       "  90.30470914127424,\n",
       "  90.58171745152355,\n",
       "  90.85872576177286,\n",
       "  91.13573407202216,\n",
       "  91.41274238227147,\n",
       "  91.13573407202216,\n",
       "  91.41274238227147,\n",
       "  91.68975069252078,\n",
       "  91.96675900277009,\n",
       "  94.18282548476455,\n",
       "  94.45983379501385,\n",
       "  93.62880886426593,\n",
       "  92.797783933518,\n",
       "  94.45983379501385,\n",
       "  93.90581717451524,\n",
       "  93.62880886426593,\n",
       "  89.75069252077563,\n",
       "  89.19667590027701,\n",
       "  89.75069252077563,\n",
       "  91.13573407202216,\n",
       "  91.13573407202216,\n",
       "  89.47368421052632,\n",
       "  90.30470914127424,\n",
       "  89.19667590027701,\n",
       "  89.19667590027701,\n",
       "  91.13573407202216,\n",
       "  89.19667590027701,\n",
       "  88.36565096952909,\n",
       "  90.85872576177286,\n",
       "  89.19667590027701,\n",
       "  88.6426592797784,\n",
       "  90.58171745152355,\n",
       "  90.02770083102493,\n",
       "  90.02770083102493,\n",
       "  90.02770083102493,\n",
       "  87.53462603878117,\n",
       "  90.30470914127424,\n",
       "  88.08864265927978,\n",
       "  90.58171745152355,\n",
       "  90.58171745152355,\n",
       "  91.96675900277009,\n",
       "  92.5207756232687,\n",
       "  93.07479224376732,\n",
       "  93.35180055401662,\n",
       "  93.62880886426593,\n",
       "  95.56786703601108,\n",
       "  93.07479224376732,\n",
       "  94.18282548476455,\n",
       "  93.07479224376732,\n",
       "  92.5207756232687,\n",
       "  91.96675900277009,\n",
       "  90.30470914127424,\n",
       "  85.59556786703601,\n",
       "  85.59556786703601,\n",
       "  85.59556786703601,\n",
       "  86.70360110803324,\n",
       "  87.25761772853186,\n",
       "  87.53462603878117,\n",
       "  90.30470914127424,\n",
       "  93.62880886426593,\n",
       "  92.5207756232687,\n",
       "  92.24376731301939,\n",
       "  93.90581717451524,\n",
       "  92.5207756232687,\n",
       "  91.41274238227147,\n",
       "  89.75069252077563,\n",
       "  91.68975069252078,\n",
       "  92.24376731301939,\n",
       "  92.797783933518,\n",
       "  93.07479224376732,\n",
       "  91.96675900277009,\n",
       "  93.90581717451524,\n",
       "  92.24376731301939,\n",
       "  91.96675900277009,\n",
       "  92.5207756232687,\n",
       "  93.07479224376732,\n",
       "  92.5207756232687,\n",
       "  91.68975069252078,\n",
       "  91.68975069252078,\n",
       "  93.62880886426593,\n",
       "  93.62880886426593,\n",
       "  94.73684210526316,\n",
       "  91.68975069252078,\n",
       "  95.01385041551247,\n",
       "  95.01385041551247,\n",
       "  94.45983379501385,\n",
       "  95.29085872576178,\n",
       "  95.01385041551247,\n",
       "  93.35180055401662,\n",
       "  94.45983379501385,\n",
       "  93.90581717451524,\n",
       "  93.90581717451524,\n",
       "  91.13573407202216,\n",
       "  92.5207756232687,\n",
       "  92.797783933518,\n",
       "  91.68975069252078,\n",
       "  92.797783933518,\n",
       "  95.29085872576178,\n",
       "  93.90581717451524,\n",
       "  91.13573407202216,\n",
       "  89.47368421052632,\n",
       "  93.07479224376732,\n",
       "  93.35180055401662,\n",
       "  93.35180055401662,\n",
       "  95.29085872576178,\n",
       "  92.797783933518,\n",
       "  93.90581717451524,\n",
       "  94.73684210526316,\n",
       "  92.797783933518,\n",
       "  92.24376731301939],\n",
       " 'val_loss': [7.186981657847199,\n",
       "  7.0083031193077785,\n",
       "  6.845330231235752,\n",
       "  6.705332025657013,\n",
       "  6.5955354201454215,\n",
       "  6.494900200791551,\n",
       "  6.41365717825106,\n",
       "  6.323318564802059,\n",
       "  6.2481527195061695,\n",
       "  6.189754681077994,\n",
       "  6.129014808640873,\n",
       "  6.088374039662883,\n",
       "  6.043437233661769,\n",
       "  6.013525094107229,\n",
       "  5.9763306839923995,\n",
       "  5.957338402201351,\n",
       "  5.924115278971721,\n",
       "  5.906646523874564,\n",
       "  5.887054011678065,\n",
       "  5.8710712272454675,\n",
       "  5.854206924711757,\n",
       "  5.8443403924438675,\n",
       "  5.826417006560873,\n",
       "  5.822154115972776,\n",
       "  5.809386988922508,\n",
       "  5.803127707711729,\n",
       "  5.7950822728244,\n",
       "  5.790141913441124,\n",
       "  5.784231079031017,\n",
       "  5.779940645520662,\n",
       "  5.779110797402178,\n",
       "  5.764152972129785,\n",
       "  5.760039452638694,\n",
       "  5.759615004519194,\n",
       "  5.758629125259875,\n",
       "  5.748032909087937,\n",
       "  5.745197593886505,\n",
       "  5.737205379531506,\n",
       "  5.738166933577655,\n",
       "  5.737211786592778,\n",
       "  5.735132825661538,\n",
       "  5.729359864054107,\n",
       "  5.728715052162552,\n",
       "  5.7224979662749345,\n",
       "  5.718586264472447,\n",
       "  5.717747831559208,\n",
       "  5.717906202505809,\n",
       "  5.714275229940537,\n",
       "  5.717328612568038,\n",
       "  5.713698753175425,\n",
       "  5.712974563101357,\n",
       "  5.71000431595292,\n",
       "  5.712093132662803,\n",
       "  5.707753750181889,\n",
       "  5.707015277007931,\n",
       "  5.708816469856127,\n",
       "  5.7033129979659485,\n",
       "  5.7039728131766045,\n",
       "  5.703155897119993,\n",
       "  5.702655673089701,\n",
       "  5.7060223424781755,\n",
       "  5.6999129707403915,\n",
       "  5.697795917147644,\n",
       "  5.696783028259223,\n",
       "  5.698815925801214,\n",
       "  5.6984266455673644,\n",
       "  5.698006541204088,\n",
       "  5.697923537397004,\n",
       "  5.694551099064763,\n",
       "  5.692269491400506,\n",
       "  5.690955188980119,\n",
       "  5.68822431227087,\n",
       "  5.6894330507612585,\n",
       "  5.687412149879829,\n",
       "  5.686871495263334,\n",
       "  5.688127459765359,\n",
       "  5.690459631963197,\n",
       "  5.68794987238775,\n",
       "  5.6884808624168866,\n",
       "  5.691675220340558,\n",
       "  5.6957120770466405,\n",
       "  5.695036549946348,\n",
       "  5.691700641834944,\n",
       "  5.692558037176131,\n",
       "  5.698813575737286,\n",
       "  5.695466747781154,\n",
       "  5.694760786371159,\n",
       "  5.6988392038904685,\n",
       "  5.700548464949997,\n",
       "  5.699420191500675,\n",
       "  5.701918273943985,\n",
       "  5.70388539044446,\n",
       "  5.698075289491768,\n",
       "  5.694748382532743,\n",
       "  5.688043532252188,\n",
       "  5.687167028894011,\n",
       "  5.6852604588753675,\n",
       "  5.679637795789608,\n",
       "  5.680394133708447,\n",
       "  5.6770525454759895,\n",
       "  5.675089003922771,\n",
       "  5.677875046545586,\n",
       "  5.679794770905786,\n",
       "  5.68149891577166,\n",
       "  5.680607973102349,\n",
       "  5.679528542561503,\n",
       "  5.68069127608923,\n",
       "  5.675216890148517,\n",
       "  5.674480006413851,\n",
       "  5.674664235535428,\n",
       "  5.673103836857803,\n",
       "  5.672375420515073,\n",
       "  5.677097722837266,\n",
       "  5.6778649101800305,\n",
       "  5.678342065131202,\n",
       "  5.677650140519566,\n",
       "  5.682920018140213,\n",
       "  5.685808134575386,\n",
       "  5.681177183875901,\n",
       "  5.674444410650209,\n",
       "  5.671725081003487,\n",
       "  5.670229216274045,\n",
       "  5.671353157961358,\n",
       "  5.672557037183021,\n",
       "  5.675879090945027,\n",
       "  5.672084436783196,\n",
       "  5.6746672577438355,\n",
       "  5.67113338188611,\n",
       "  5.672981879952634,\n",
       "  5.674843643271091,\n",
       "  5.675134544757438,\n",
       "  5.6742736489825605,\n",
       "  5.672972043166125,\n",
       "  5.671115342899277,\n",
       "  5.672878654550683,\n",
       "  5.673166201067652,\n",
       "  5.677609075155473,\n",
       "  5.670486117164596,\n",
       "  5.670022738006028,\n",
       "  5.667583899207184,\n",
       "  5.666444988061241,\n",
       "  5.6660568755588665,\n",
       "  5.663788944134876,\n",
       "  5.6639427503656625,\n",
       "  5.668600365002707,\n",
       "  5.667497016038634,\n",
       "  5.67149697026937,\n",
       "  5.66935571692408,\n",
       "  5.670666721463084,\n",
       "  5.671158125167891,\n",
       "  5.674428001069456,\n",
       "  5.676404923923315,\n",
       "  5.676977228289292,\n",
       "  5.6745724903708386,\n",
       "  5.672190320258925,\n",
       "  5.679692531330021,\n",
       "  5.678373826056035,\n",
       "  5.669492123638453,\n",
       "  5.669039682782624,\n",
       "  5.6702106100906144,\n",
       "  5.6703276727040475,\n",
       "  5.668309610896356,\n",
       "  5.670487823174781,\n",
       "  5.669768594241485,\n",
       "  5.668448748876786,\n",
       "  5.667749535036515,\n",
       "  5.666858977385185,\n",
       "  5.672770941793749,\n",
       "  5.676727587331129,\n",
       "  5.672687924013506,\n",
       "  5.66849453021043,\n",
       "  5.669196552856029,\n",
       "  5.666734382129436,\n",
       "  5.668751120450352,\n",
       "  5.665983540770279,\n",
       "  5.66824196885742,\n",
       "  5.669118536741939,\n",
       "  5.6660174929613225,\n",
       "  5.664731085617936,\n",
       "  5.666574576059992,\n",
       "  5.667170003850252,\n",
       "  5.669754972351753,\n",
       "  5.673442079725352,\n",
       "  5.666174311020013,\n",
       "  5.672409027611957,\n",
       "  5.6626448839809775,\n",
       "  5.665892478242272,\n",
       "  5.6708298117176605,\n",
       "  5.67229520560989,\n",
       "  5.674469231834705,\n",
       "  5.680883008696524,\n",
       "  5.674023047141497,\n",
       "  5.6691231244758065,\n",
       "  5.66690455678479,\n",
       "  5.667755865065187,\n",
       "  5.6669037605845185,\n",
       "  5.666226450315517,\n",
       "  5.67016363214103,\n",
       "  5.6748434721933,\n",
       "  5.6712290718515925,\n",
       "  5.667500047009658,\n",
       "  5.668663263261841,\n",
       "  5.666904674712184,\n",
       "  5.668590155361321,\n",
       "  5.670536585715814,\n",
       "  5.666042343991707,\n",
       "  5.665124006173418,\n",
       "  5.667015810339666,\n",
       "  5.67346491130912,\n",
       "  5.670925271064563,\n",
       "  5.667577982989052,\n",
       "  5.669314096944845,\n",
       "  5.672480241592137,\n",
       "  5.673080070821613,\n",
       "  5.671387229332448,\n",
       "  5.667174652162721,\n",
       "  5.669665226144309,\n",
       "  5.6730182241431955,\n",
       "  5.672143916999188,\n",
       "  5.667727797402317,\n",
       "  5.664269169224168,\n",
       "  5.666895886101126,\n",
       "  5.667378595493702,\n",
       "  5.676060493145823,\n",
       "  5.68270405708569,\n",
       "  5.683671161617742,\n",
       "  5.696030563700566,\n",
       "  5.7061889221408135,\n",
       "  5.693640281844447,\n",
       "  5.701651481824484,\n",
       "  5.68780093245464,\n",
       "  5.674890052102426,\n",
       "  5.664841804746953,\n",
       "  5.662775560796341,\n",
       "  5.662926017045091,\n",
       "  5.666575570911977,\n",
       "  5.669066861079693,\n",
       "  5.663306802658622,\n",
       "  5.659840749383255,\n",
       "  5.663721304618557,\n",
       "  5.663436516827579,\n",
       "  5.66807249589583,\n",
       "  5.6668050423084875,\n",
       "  5.663206793296546,\n",
       "  5.663937385399187,\n",
       "  5.661309208090865,\n",
       "  5.66044683429673,\n",
       "  5.663746213396386,\n",
       "  5.6656033665400125,\n",
       "  5.66488115592727,\n",
       "  5.665066824034689,\n",
       "  5.6628741538427,\n",
       "  5.660946564580515,\n",
       "  5.66620137852666,\n",
       "  5.663940557800843,\n",
       "  5.664675256594042,\n",
       "  5.660659379895776,\n",
       "  5.6631652990912515,\n",
       "  5.664987211988272,\n",
       "  5.664422380363963,\n",
       "  5.666297402949179,\n",
       "  5.663279099277789,\n",
       "  5.668293714812757,\n",
       "  5.668000485448616,\n",
       "  5.667006392241827,\n",
       "  5.664680546635768,\n",
       "  5.666762273529036,\n",
       "  5.665886321537696,\n",
       "  5.6612423823692755,\n",
       "  5.662726033872705,\n",
       "  5.664145117356816,\n",
       "  5.662364020487696,\n",
       "  5.661644993478294,\n",
       "  5.661388573953317,\n",
       "  5.66448114245785,\n",
       "  5.66636419608742,\n",
       "  5.665569889534221,\n",
       "  5.667371999702292,\n",
       "  5.6634886798763775,\n",
       "  5.665025052385754,\n",
       "  5.659855286035632,\n",
       "  5.663886482536022,\n",
       "  5.668170639246283,\n",
       "  5.669296649840679,\n",
       "  5.674294663457777,\n",
       "  5.672273133922084,\n",
       "  5.665157477886907,\n",
       "  5.661242101238092,\n",
       "  5.659220957110081,\n",
       "  5.660057626991379,\n",
       "  5.661601854424472,\n",
       "  5.660595671407709,\n",
       "  5.656711502732424,\n",
       "  5.659469038676555,\n",
       "  5.657663631408893,\n",
       "  5.661363470288304,\n",
       "  5.6584054186771295,\n",
       "  5.657321447126618,\n",
       "  5.660224938651406,\n",
       "  5.658039159171954,\n",
       "  5.658345087938976,\n",
       "  5.658533840496648,\n",
       "  5.660938060645552,\n",
       "  5.657979180617782,\n",
       "  5.66276186893996,\n",
       "  5.666901834419805,\n",
       "  5.6655480497462944,\n",
       "  5.663475946743724,\n",
       "  5.669436504316371,\n",
       "  5.668948068029007,\n",
       "  5.668538818654092,\n",
       "  5.664476363008718,\n",
       "  5.662938285288251,\n",
       "  5.665897311298942,\n",
       "  5.6682102340073826,\n",
       "  5.665707853186894,\n",
       "  5.669313567768917,\n",
       "  5.666919285386372,\n",
       "  5.667244074255738,\n",
       "  5.665875526115187,\n",
       "  5.667381465757293,\n",
       "  5.666828896353374,\n",
       "  5.675468516635534,\n",
       "  5.67166951268934,\n",
       "  5.673253720883824,\n",
       "  5.668511327321038,\n",
       "  5.671230709589408,\n",
       "  5.667456716162393,\n",
       "  5.664782294961982,\n",
       "  5.6636242556831675,\n",
       "  5.6670816340394765,\n",
       "  5.676529314351805,\n",
       "  5.671417616599266,\n",
       "  5.665335063973069,\n",
       "  5.662131462946061,\n",
       "  5.660491300541579,\n",
       "  5.660569830053159,\n",
       "  5.6606939412357145,\n",
       "  5.661530230526591,\n",
       "  5.65892533684272,\n",
       "  5.6580019819391065,\n",
       "  5.66061638623704,\n",
       "  5.66234272617871,\n",
       "  5.659316739701595,\n",
       "  5.659646214987052,\n",
       "  5.665346796181435,\n",
       "  5.665132194310468,\n",
       "  5.670211291024566,\n",
       "  5.6739591085104895,\n",
       "  5.675717749553485,\n",
       "  5.673461509855448,\n",
       "  5.6692407814945245,\n",
       "  5.664181183330301,\n",
       "  5.6653657977478336,\n",
       "  5.664413734742941,\n",
       "  5.665121349922226,\n",
       "  5.6624871358878766,\n",
       "  5.663179656259503,\n",
       "  5.664414354101659,\n",
       "  5.667105649004252,\n",
       "  5.66204436750355,\n",
       "  5.665196341654412,\n",
       "  5.6605960301282945,\n",
       "  5.661005756747971,\n",
       "  5.658085142126324,\n",
       "  5.658702671340472,\n",
       "  5.662157003292859,\n",
       "  5.665811789898631,\n",
       "  5.664515369598564,\n",
       "  5.664617777170315,\n",
       "  5.663571191421164,\n",
       "  5.658572877835319,\n",
       "  5.657299869206204,\n",
       "  5.658728623034274,\n",
       "  5.661078908656719,\n",
       "  5.658343331900776,\n",
       "  5.657072128522663,\n",
       "  5.658932380967573,\n",
       "  5.661633652715896,\n",
       "  5.659037895582121,\n",
       "  5.6607933130931825,\n",
       "  5.665339841746425,\n",
       "  5.660917115050235,\n",
       "  5.671139253908028,\n",
       "  5.678491792812895,\n",
       "  5.680653539918713,\n",
       "  5.683895448480195,\n",
       "  5.681640382622936,\n",
       "  5.677232762562712,\n",
       "  5.67696130716095,\n",
       "  5.667126524145944,\n",
       "  5.663052927768461,\n",
       "  5.666638574613134,\n",
       "  5.6590309151361735,\n",
       "  5.659933094344497,\n",
       "  5.658186615338753,\n",
       "  5.660174517570372,\n",
       "  5.658126423986642,\n",
       "  5.65888917156337,\n",
       "  5.659542178531408,\n",
       "  5.65848244643048,\n",
       "  5.655764623492928,\n",
       "  5.661442349665949,\n",
       "  5.662066575200782,\n",
       "  5.659228058056167,\n",
       "  5.661525297076115,\n",
       "  5.656209886824227,\n",
       "  5.656639619845332,\n",
       "  5.6556189641594585,\n",
       "  5.658063345137536,\n",
       "  5.657880967143726,\n",
       "  5.658602810873038,\n",
       "  5.65794405236472,\n",
       "  5.662192008514809,\n",
       "  5.6589950044793484,\n",
       "  5.656198392049146,\n",
       "  5.655633519032323,\n",
       "  5.657034123691413,\n",
       "  5.658996454279558,\n",
       "  5.657921652365582,\n",
       "  5.659906969858336,\n",
       "  5.65377824039259,\n",
       "  5.662460321907458,\n",
       "  5.65812449984026,\n",
       "  5.661720643748824,\n",
       "  5.66119945486611,\n",
       "  5.6654097387800135,\n",
       "  5.661993952124811,\n",
       "  5.660830351093361,\n",
       "  5.657240223521617,\n",
       "  5.661253194968493,\n",
       "  5.663735440660713,\n",
       "  5.667530115107316,\n",
       "  5.65755066322325,\n",
       "  5.658528107585655,\n",
       "  5.657706312581284,\n",
       "  5.657012734904565,\n",
       "  5.6557587052314,\n",
       "  5.654867879173568,\n",
       "  5.657264969248432,\n",
       "  5.65467785810151,\n",
       "  5.659511979236215],\n",
       " 'val_ATT_loss': [1.8956027428309123,\n",
       "  1.827802089898567,\n",
       "  1.7628037294721215,\n",
       "  1.6772709745701735,\n",
       "  1.6028099350812959,\n",
       "  1.52392482878716,\n",
       "  1.4560124660895122,\n",
       "  1.383106403719119,\n",
       "  1.318923213375293,\n",
       "  1.268027252782651,\n",
       "  1.2145568890784815,\n",
       "  1.1802398264408112,\n",
       "  1.1407981329574817,\n",
       "  1.1139601942727235,\n",
       "  1.0800207667961352,\n",
       "  1.0640645377277358,\n",
       "  1.035910650360875,\n",
       "  1.018367262027128,\n",
       "  0.9998118318920213,\n",
       "  0.9878513444729937,\n",
       "  0.9734542020937291,\n",
       "  0.9612109789518806,\n",
       "  0.9485339128631887,\n",
       "  0.9443158772660465,\n",
       "  0.9326569685121862,\n",
       "  0.9285525608111204,\n",
       "  0.9224605721429111,\n",
       "  0.9172424803662106,\n",
       "  0.9107649132488219,\n",
       "  0.9058958854132552,\n",
       "  0.905512240601749,\n",
       "  0.8956327486813553,\n",
       "  0.890705090228135,\n",
       "  0.8892017810083017,\n",
       "  0.8899182174506226,\n",
       "  0.8816986357778068,\n",
       "  0.8789806947475527,\n",
       "  0.8712885900726163,\n",
       "  0.8716934073746689,\n",
       "  0.8715541980615477,\n",
       "  0.8685487237403063,\n",
       "  0.865585713003709,\n",
       "  0.8644749542804269,\n",
       "  0.860316196471695,\n",
       "  0.8579545718867604,\n",
       "  0.8555910657576429,\n",
       "  0.8549278139825759,\n",
       "  0.8512882213524686,\n",
       "  0.8530675291530485,\n",
       "  0.8507757053627232,\n",
       "  0.8492858551624345,\n",
       "  0.8463860677509774,\n",
       "  0.8461338806200803,\n",
       "  0.8457036878519911,\n",
       "  0.8446947757790728,\n",
       "  0.8414801021174687,\n",
       "  0.8406082617073525,\n",
       "  0.8430786939655862,\n",
       "  0.8424770047024983,\n",
       "  0.8401628445561339,\n",
       "  0.8411283352510716,\n",
       "  0.837725265723903,\n",
       "  0.8368595344264332,\n",
       "  0.8361289900977437,\n",
       "  0.8363506205198241,\n",
       "  0.8364264027374547,\n",
       "  0.836018052527575,\n",
       "  0.8356069700747002,\n",
       "  0.8342136091333094,\n",
       "  0.8324235970896434,\n",
       "  0.8302526755061576,\n",
       "  0.8288373073668984,\n",
       "  0.8308354468365026,\n",
       "  0.8298659119906464,\n",
       "  0.8281593781903507,\n",
       "  0.8286020898722052,\n",
       "  0.8295369888466548,\n",
       "  0.8256301885940195,\n",
       "  0.8258495614296053,\n",
       "  0.8268354633232442,\n",
       "  0.829614764791194,\n",
       "  0.8296547805875297,\n",
       "  0.825430207741939,\n",
       "  0.8239990260785188,\n",
       "  0.8242126250170111,\n",
       "  0.8225044320996214,\n",
       "  0.8258575047177028,\n",
       "  0.8257522064495862,\n",
       "  0.8244067212430443,\n",
       "  0.8214882419603627,\n",
       "  0.8225153833627701,\n",
       "  0.8219929342347432,\n",
       "  0.8213436888243125,\n",
       "  0.8221497162570798,\n",
       "  0.8196256590810248,\n",
       "  0.8195209289953961,\n",
       "  0.8200388451417288,\n",
       "  0.8176850681382466,\n",
       "  0.8178088944859621,\n",
       "  0.8165313828039945,\n",
       "  0.8163098800715393,\n",
       "  0.8172266176803326,\n",
       "  0.8167298470086198,\n",
       "  0.8181678198459672,\n",
       "  0.8188747796343594,\n",
       "  0.816781451183606,\n",
       "  0.8189156147522655,\n",
       "  0.8166639210005117,\n",
       "  0.8171244986900469,\n",
       "  0.8169029435006584,\n",
       "  0.8170995564480138,\n",
       "  0.8159278823835093,\n",
       "  0.816315594000545,\n",
       "  0.8164409344516149,\n",
       "  0.8151748349996117,\n",
       "  0.8131921591797495,\n",
       "  0.8138784648683982,\n",
       "  0.8144080378418046,\n",
       "  0.8146761220887424,\n",
       "  0.8124833007653555,\n",
       "  0.8124130546319775,\n",
       "  0.8121497552327024,\n",
       "  0.8126636119635124,\n",
       "  0.8124979684750239,\n",
       "  0.8139799740014038,\n",
       "  0.8129063587847764,\n",
       "  0.8154405124061476,\n",
       "  0.8147068839005338,\n",
       "  0.8130881815421872,\n",
       "  0.8130911414216204,\n",
       "  0.8156162957108117,\n",
       "  0.8127015518948315,\n",
       "  0.8133788226338906,\n",
       "  0.8121704646969229,\n",
       "  0.8123523441514349,\n",
       "  0.8101654141172161,\n",
       "  0.8134597284280187,\n",
       "  0.8109403519126458,\n",
       "  0.8104244190745238,\n",
       "  0.8107465202730846,\n",
       "  0.8098704691340284,\n",
       "  0.8097275237484676,\n",
       "  0.8097884685285692,\n",
       "  0.8088866559470572,\n",
       "  0.8110870746819954,\n",
       "  0.8093689405094318,\n",
       "  0.8106105197978214,\n",
       "  0.8097040832527285,\n",
       "  0.808702408782835,\n",
       "  0.8097291620281654,\n",
       "  0.8091992488721522,\n",
       "  0.8103677319075034,\n",
       "  0.8075364249023965,\n",
       "  0.8081303032917705,\n",
       "  0.8083577983505358,\n",
       "  0.808618399185863,\n",
       "  0.81064732154695,\n",
       "  0.8092353307619328,\n",
       "  0.8087212264053221,\n",
       "  0.8097260258788985,\n",
       "  0.8108991847290257,\n",
       "  0.8085575014110503,\n",
       "  0.8100981371916407,\n",
       "  0.8101890956967827,\n",
       "  0.8090043385338977,\n",
       "  0.807576317855013,\n",
       "  0.8076348799030956,\n",
       "  0.8083733784231714,\n",
       "  0.8083405406252155,\n",
       "  0.8094633025367085,\n",
       "  0.8072534397365602,\n",
       "  0.809157264668767,\n",
       "  0.8079926559353262,\n",
       "  0.8080326934170917,\n",
       "  0.8069291262607264,\n",
       "  0.8095348180551839,\n",
       "  0.8090609654905351,\n",
       "  0.8088577010524951,\n",
       "  0.8068139330158389,\n",
       "  0.8072345243479179,\n",
       "  0.8082978615188986,\n",
       "  0.8075238270003621,\n",
       "  0.808498443263333,\n",
       "  0.807970605124303,\n",
       "  0.8120400118149393,\n",
       "  0.8055974612148796,\n",
       "  0.807766282461523,\n",
       "  0.808481885892589,\n",
       "  0.8088460977726836,\n",
       "  0.8087026566509309,\n",
       "  0.8104243417823218,\n",
       "  0.8090401015630583,\n",
       "  0.8075629083848581,\n",
       "  0.8074052056403664,\n",
       "  0.808080233330649,\n",
       "  0.8074979458640261,\n",
       "  0.8067773960954775,\n",
       "  0.8076763036774426,\n",
       "  0.8084433918319097,\n",
       "  0.8105365437705342,\n",
       "  0.8082158944713391,\n",
       "  0.8072862759595965,\n",
       "  0.8088300953793331,\n",
       "  0.8079071193933487,\n",
       "  0.8082324948494997,\n",
       "  0.8069254397376766,\n",
       "  0.8085763322386316,\n",
       "  0.8083843268999239,\n",
       "  0.8097811009341139,\n",
       "  0.8087944588283213,\n",
       "  0.8077692749296747,\n",
       "  0.8073140073840211,\n",
       "  0.8076799857180293,\n",
       "  0.8079181245672025,\n",
       "  0.8071693079016073,\n",
       "  0.8058704095400446,\n",
       "  0.8076995551828446,\n",
       "  0.8096433073524537,\n",
       "  0.8096620762735848,\n",
       "  0.8084915617132574,\n",
       "  0.8065267151933375,\n",
       "  0.8070975457265125,\n",
       "  0.8073681307032825,\n",
       "  0.8066631794218125,\n",
       "  0.8064966648817062,\n",
       "  0.8063362427358705,\n",
       "  0.8060157038574296,\n",
       "  0.8072991233046461,\n",
       "  0.807679924901908,\n",
       "  0.8077859845830173,\n",
       "  0.8047985702268476,\n",
       "  0.8066260258599025,\n",
       "  0.8061530215711128,\n",
       "  0.8066006055692347,\n",
       "  0.8071729534767508,\n",
       "  0.8083042839193731,\n",
       "  0.8107298410278025,\n",
       "  0.8070721799522881,\n",
       "  0.8038344799987669,\n",
       "  0.8064013887953952,\n",
       "  0.806474126693679,\n",
       "  0.8072997481842351,\n",
       "  0.8061637149109104,\n",
       "  0.8046283955738797,\n",
       "  0.8044673227440051,\n",
       "  0.8042903145154318,\n",
       "  0.8042417484812621,\n",
       "  0.8051672466402131,\n",
       "  0.8053381068677437,\n",
       "  0.8044657454015763,\n",
       "  0.8063006241147112,\n",
       "  0.804079364833793,\n",
       "  0.8043648842873612,\n",
       "  0.810086818366516,\n",
       "  0.8073351683655405,\n",
       "  0.808114315193843,\n",
       "  0.8057300231563367,\n",
       "  0.8042170243050025,\n",
       "  0.8043233082304156,\n",
       "  0.8048159681200012,\n",
       "  0.8074289584547524,\n",
       "  0.8035738460901307,\n",
       "  0.8047974089054557,\n",
       "  0.8048280688320718,\n",
       "  0.8053306817281537,\n",
       "  0.8058659577272772,\n",
       "  0.8034103876206933,\n",
       "  0.8047350291314164,\n",
       "  0.8051272390819177,\n",
       "  0.8056617867413575,\n",
       "  0.8074857930584651,\n",
       "  0.8047970648461241,\n",
       "  0.8051088076781451,\n",
       "  0.8076661971768713,\n",
       "  0.8069077917957693,\n",
       "  0.8068587687926564,\n",
       "  0.8044318399293636,\n",
       "  0.8056347912889186,\n",
       "  0.8049742200267993,\n",
       "  0.8057845744902525,\n",
       "  0.8031345307826996,\n",
       "  0.8063477407383725,\n",
       "  0.8061006272711405,\n",
       "  0.8027564621553188,\n",
       "  0.8034030289911642,\n",
       "  0.8041329508632179,\n",
       "  0.8030765609043401,\n",
       "  0.804210570890729,\n",
       "  0.8056760430335999,\n",
       "  0.8042086962519622,\n",
       "  0.8055931943945769,\n",
       "  0.8050237930402523,\n",
       "  0.8011671881365582,\n",
       "  0.8038063756818694,\n",
       "  0.8047369977807611,\n",
       "  0.8049312608271111,\n",
       "  0.8036832570787368,\n",
       "  0.8025750813445425,\n",
       "  0.803679086328522,\n",
       "  0.8029931717044939,\n",
       "  0.80352958370515,\n",
       "  0.8020755294861832,\n",
       "  0.801917770529181,\n",
       "  0.8004682280910693,\n",
       "  0.8022421737754248,\n",
       "  0.8030230129152779,\n",
       "  0.8029194758917259,\n",
       "  0.802718413191113,\n",
       "  0.8039569809911696,\n",
       "  0.8045724468017982,\n",
       "  0.8043539628265349,\n",
       "  0.8034957207557631,\n",
       "  0.8041958790726778,\n",
       "  0.8050157848170133,\n",
       "  0.8040078086097066,\n",
       "  0.8047972619533539,\n",
       "  0.8046332687381806,\n",
       "  0.8054046959169512,\n",
       "  0.8046430077979235,\n",
       "  0.803270166361235,\n",
       "  0.8052074880134769,\n",
       "  0.8030145852788677,\n",
       "  0.8043626525053164,\n",
       "  0.8030631819633933,\n",
       "  0.8044889522034947,\n",
       "  0.8035381204955946,\n",
       "  0.8056988862714147,\n",
       "  0.8040403092537469,\n",
       "  0.803694459480968,\n",
       "  0.8026389818850571,\n",
       "  0.8038052164199876,\n",
       "  0.8043743867457398,\n",
       "  0.8023174916341053,\n",
       "  0.803119074644112,\n",
       "  0.8025031490781442,\n",
       "  0.8020761696303763,\n",
       "  0.8035195233618341,\n",
       "  0.8033370364729951,\n",
       "  0.8037218232222689,\n",
       "  0.8020882503530844,\n",
       "  0.8017283795568032,\n",
       "  0.804486659484181,\n",
       "  0.8052642496863032,\n",
       "  0.8035025340997106,\n",
       "  0.8010002449760593,\n",
       "  0.8041681285069241,\n",
       "  0.8053122641352134,\n",
       "  0.8046563769259104,\n",
       "  0.8042050248723689,\n",
       "  0.8010536449226907,\n",
       "  0.8032002887590145,\n",
       "  0.8035725592597713,\n",
       "  0.8012870117658522,\n",
       "  0.8009147364191893,\n",
       "  0.8022123464966208,\n",
       "  0.8022667711100927,\n",
       "  0.8020403897616921,\n",
       "  0.802810855391549,\n",
       "  0.8029921348017406,\n",
       "  0.803396358964889,\n",
       "  0.802472759674235,\n",
       "  0.8018827148811604,\n",
       "  0.7998687769097041,\n",
       "  0.8013309424485617,\n",
       "  0.8008518920439046,\n",
       "  0.7997459015710567,\n",
       "  0.8009873978733048,\n",
       "  0.8012081994031502,\n",
       "  0.800801814450481,\n",
       "  0.8005485478940049,\n",
       "  0.8008117521923732,\n",
       "  0.8002229708481611,\n",
       "  0.7988705917344829,\n",
       "  0.8001628074704147,\n",
       "  0.8008634702461522,\n",
       "  0.7999281569467327,\n",
       "  0.7986384115325726,\n",
       "  0.8006290431187405,\n",
       "  0.8011896284372826,\n",
       "  0.7995317160356336,\n",
       "  0.8004758767480773,\n",
       "  0.802360123129395,\n",
       "  0.7996480027592279,\n",
       "  0.8029188846911841,\n",
       "  0.8016784627263139,\n",
       "  0.8002121684754767,\n",
       "  0.7990259921889964,\n",
       "  0.8000164697083031,\n",
       "  0.8026843549517112,\n",
       "  0.8015785044044014,\n",
       "  0.7996011315807094,\n",
       "  0.8012089718405794,\n",
       "  0.8024387794539212,\n",
       "  0.7997796537914896,\n",
       "  0.8005832210788882,\n",
       "  0.7992374878831026,\n",
       "  0.8010253986207451,\n",
       "  0.800356349809383,\n",
       "  0.8005950809494267,\n",
       "  0.7999549953918147,\n",
       "  0.8006809978950314,\n",
       "  0.7985268687087346,\n",
       "  0.7998785308706082,\n",
       "  0.8023236636223832,\n",
       "  0.7975540740218588,\n",
       "  0.79868650145647,\n",
       "  0.7972306816316233,\n",
       "  0.7990341866161765,\n",
       "  0.7989972753495704,\n",
       "  0.7996837021858711,\n",
       "  0.7999012631371738,\n",
       "  0.7993965965460955,\n",
       "  0.7994522869102354,\n",
       "  0.801262255122022,\n",
       "  0.8004342626507689,\n",
       "  0.7985097337059859,\n",
       "  0.7979531002238514,\n",
       "  0.7987817328392974,\n",
       "  0.7999725121308149,\n",
       "  0.7985218838462985,\n",
       "  0.7982196073706557,\n",
       "  0.7979735568529223,\n",
       "  0.8026520912724782,\n",
       "  0.7990117919881169,\n",
       "  0.7998741949961438,\n",
       "  0.79985284126871,\n",
       "  0.8013104810947325,\n",
       "  0.7991826233825063,\n",
       "  0.7991043612724398,\n",
       "  0.7970897571100453,\n",
       "  0.7997250209252039,\n",
       "  0.8004349809351975,\n",
       "  0.8027354509122973,\n",
       "  0.7984995889227565,\n",
       "  0.7995064750434907,\n",
       "  0.8006607713253517,\n",
       "  0.7993659718734462,\n",
       "  0.7998038937405842,\n",
       "  0.7996278583276563,\n",
       "  0.800958902007196,\n",
       "  0.7986578966786222,\n",
       "  0.8017724602203059],\n",
       " 'val_VAL_loss': [1.7637929716720957,\n",
       "  1.7268336764697372,\n",
       "  1.6941755005878767,\n",
       "  1.6760203503622797,\n",
       "  1.6642418283547087,\n",
       "  1.6569917906681304,\n",
       "  1.6525482373871827,\n",
       "  1.6467373870276465,\n",
       "  1.6430765020436253,\n",
       "  1.6405758094317808,\n",
       "  1.6381526398541304,\n",
       "  1.6360447377406906,\n",
       "  1.6342130335680958,\n",
       "  1.6331882999448353,\n",
       "  1.6321033057320882,\n",
       "  1.6310912881578719,\n",
       "  1.629401542870282,\n",
       "  1.6294264206158116,\n",
       "  1.6290807265953478,\n",
       "  1.6277399609241578,\n",
       "  1.6269175742060094,\n",
       "  1.6277098044973288,\n",
       "  1.6259610312325614,\n",
       "  1.6259460795689098,\n",
       "  1.6255766734701071,\n",
       "  1.624858382300203,\n",
       "  1.6242072335604965,\n",
       "  1.6242998110249711,\n",
       "  1.6244887219273985,\n",
       "  1.6246815867024689,\n",
       "  1.6245328522668097,\n",
       "  1.6228400744828098,\n",
       "  1.6231114541368532,\n",
       "  1.623471074503631,\n",
       "  1.6229036359364175,\n",
       "  1.62211142443671,\n",
       "  1.6220722997129844,\n",
       "  1.6219722631529634,\n",
       "  1.6221578420676621,\n",
       "  1.6218858628437436,\n",
       "  1.6221947006404107,\n",
       "  1.6212580503501328,\n",
       "  1.6214133659607084,\n",
       "  1.62072725660108,\n",
       "  1.620210564195229,\n",
       "  1.6207189219338554,\n",
       "  1.620992796174411,\n",
       "  1.6209956695293557,\n",
       "  1.6214203611383298,\n",
       "  1.6209743492709006,\n",
       "  1.6212295693129741,\n",
       "  1.6212060827339811,\n",
       "  1.6219864173475744,\n",
       "  1.6206833541099661,\n",
       "  1.6207735004096195,\n",
       "  1.622445455912886,\n",
       "  1.6209015787528653,\n",
       "  1.6202980397370061,\n",
       "  1.6202262974724981,\n",
       "  1.6208309428445224,\n",
       "  1.621631335742368,\n",
       "  1.6207292350054963,\n",
       "  1.6203121275737369,\n",
       "  1.6202180127204933,\n",
       "  1.62082176842713,\n",
       "  1.62066674760997,\n",
       "  1.6206628295588377,\n",
       "  1.6207721891074345,\n",
       "  1.620112496643818,\n",
       "  1.619948631436954,\n",
       "  1.6202341711579873,\n",
       "  1.619795668301324,\n",
       "  1.6195325346415854,\n",
       "  1.619182079296394,\n",
       "  1.6195707056909947,\n",
       "  1.6198417899643847,\n",
       "  1.620307547705514,\n",
       "  1.6207732279312435,\n",
       "  1.6208771003290938,\n",
       "  1.6216132523391047,\n",
       "  1.622032437418482,\n",
       "  1.6217939231196061,\n",
       "  1.6220901446976685,\n",
       "  1.622853003699204,\n",
       "  1.624866983573425,\n",
       "  1.6243207718938442,\n",
       "  1.6229677605511519,\n",
       "  1.6243623324802943,\n",
       "  1.6253805812356508,\n",
       "  1.6259773165134375,\n",
       "  1.6264676301937384,\n",
       "  1.6272974854032394,\n",
       "  1.625577200222485,\n",
       "  1.624199555425221,\n",
       "  1.6228059577237208,\n",
       "  1.622548699966205,\n",
       "  1.621740537911213,\n",
       "  1.6206509092171204,\n",
       "  1.6208617464074948,\n",
       "  1.6201737208906652,\n",
       "  1.6195930412837438,\n",
       "  1.6202161429550848,\n",
       "  1.6210216412990552,\n",
       "  1.6211103653085643,\n",
       "  1.6205777311559968,\n",
       "  1.6209156971259657,\n",
       "  1.6205918871123215,\n",
       "  1.6195176563826688,\n",
       "  1.619118502574601,\n",
       "  1.6192537640115898,\n",
       "  1.6186680934699298,\n",
       "  1.6188158460438544,\n",
       "  1.6202607096122403,\n",
       "  1.6204746585761385,\n",
       "  1.621055743377197,\n",
       "  1.6214859937799389,\n",
       "  1.6230138510906051,\n",
       "  1.6238000322445272,\n",
       "  1.6221670205957197,\n",
       "  1.6206537032949513,\n",
       "  1.61977067545717,\n",
       "  1.6193598203471142,\n",
       "  1.6195631819992817,\n",
       "  1.6200196895693324,\n",
       "  1.6206330389812076,\n",
       "  1.6197260259994732,\n",
       "  1.619742248445896,\n",
       "  1.6188088326618588,\n",
       "  1.6199645661368158,\n",
       "  1.620584167283157,\n",
       "  1.6198394163488754,\n",
       "  1.6205240323625762,\n",
       "  1.619864406844078,\n",
       "  1.619648292734118,\n",
       "  1.6201754367997494,\n",
       "  1.6210002623168118,\n",
       "  1.6213831155758185,\n",
       "  1.6198485884173164,\n",
       "  1.6198661063105015,\n",
       "  1.618945792978033,\n",
       "  1.6188581729757374,\n",
       "  1.6187764506034663,\n",
       "  1.6180001585354358,\n",
       "  1.6183520314728685,\n",
       "  1.6191710967735704,\n",
       "  1.6193760251764007,\n",
       "  1.6202954834905163,\n",
       "  1.6198838778904505,\n",
       "  1.6206547708934165,\n",
       "  1.6204763210465756,\n",
       "  1.6217429173991011,\n",
       "  1.622012397338604,\n",
       "  1.6231469344622984,\n",
       "  1.6221473956930226,\n",
       "  1.6212775073027963,\n",
       "  1.623691377381386,\n",
       "  1.6225755015030283,\n",
       "  1.6200855976255069,\n",
       "  1.6201061521257674,\n",
       "  1.620161528070572,\n",
       "  1.619809495991674,\n",
       "  1.6199173698284355,\n",
       "  1.6201298953277137,\n",
       "  1.619859832848234,\n",
       "  1.6198148034476294,\n",
       "  1.6200577390605007,\n",
       "  1.6197413658273632,\n",
       "  1.6214658544568592,\n",
       "  1.6227956822353045,\n",
       "  1.6210748738255993,\n",
       "  1.6204136968246234,\n",
       "  1.6200130960624206,\n",
       "  1.6195805753980363,\n",
       "  1.6202394756777534,\n",
       "  1.6196848048365176,\n",
       "  1.619569050267412,\n",
       "  1.6200191904171346,\n",
       "  1.6190532639696094,\n",
       "  1.6193057175340324,\n",
       "  1.619780017237358,\n",
       "  1.6196240474437844,\n",
       "  1.6207437151171304,\n",
       "  1.621647878820673,\n",
       "  1.61940123529857,\n",
       "  1.6201230052656728,\n",
       "  1.6190158075886993,\n",
       "  1.619375398593583,\n",
       "  1.6207826419416906,\n",
       "  1.6211497026124024,\n",
       "  1.6219221917279247,\n",
       "  1.6234862223047342,\n",
       "  1.6216609818594796,\n",
       "  1.6205200720303163,\n",
       "  1.6198331170481415,\n",
       "  1.619891877244846,\n",
       "  1.619801938240164,\n",
       "  1.61981635140668,\n",
       "  1.6208291094878624,\n",
       "  1.6221333601204633,\n",
       "  1.620230842693686,\n",
       "  1.6197613841794394,\n",
       "  1.620458995767415,\n",
       "  1.61935819311095,\n",
       "  1.6202276786559908,\n",
       "  1.6207680302887715,\n",
       "  1.6197056347513434,\n",
       "  1.6188492246449286,\n",
       "  1.6195438278132472,\n",
       "  1.6212279367916689,\n",
       "  1.6207102707454137,\n",
       "  1.6199362360197922,\n",
       "  1.6206666965202745,\n",
       "  1.6216000852913692,\n",
       "  1.6217206487514702,\n",
       "  1.6214059738102804,\n",
       "  1.620434747540892,\n",
       "  1.6206552236538214,\n",
       "  1.6211249722635805,\n",
       "  1.6208272802418675,\n",
       "  1.6197454118963532,\n",
       "  1.6192474846769436,\n",
       "  1.619932780124871,\n",
       "  1.620003488263473,\n",
       "  1.6231324379080034,\n",
       "  1.6254024640679947,\n",
       "  1.6257783062939573,\n",
       "  1.6300049532810454,\n",
       "  1.6329632662787226,\n",
       "  1.6286534523141796,\n",
       "  1.6312884990804888,\n",
       "  1.6276674540759308,\n",
       "  1.6227546754141746,\n",
       "  1.6195629277252799,\n",
       "  1.6187249850757017,\n",
       "  1.6185843545227803,\n",
       "  1.619423762330868,\n",
       "  1.6194456733506302,\n",
       "  1.6187448742354444,\n",
       "  1.6186687564614959,\n",
       "  1.6191066386077204,\n",
       "  1.6189874633779666,\n",
       "  1.6202575825705317,\n",
       "  1.6202137757991921,\n",
       "  1.6195261325742223,\n",
       "  1.6198233542183937,\n",
       "  1.6190062978584778,\n",
       "  1.6187350286051558,\n",
       "  1.6195263222520575,\n",
       "  1.6200884198907561,\n",
       "  1.620138470175231,\n",
       "  1.6195887333066592,\n",
       "  1.6195982630029688,\n",
       "  1.618860560097718,\n",
       "  1.6187048533867145,\n",
       "  1.618868463145101,\n",
       "  1.6188536471333996,\n",
       "  1.618309785579813,\n",
       "  1.6196494249287496,\n",
       "  1.6202213012526188,\n",
       "  1.6198688040813203,\n",
       "  1.6196228148314753,\n",
       "  1.6199017510625529,\n",
       "  1.6211654353024336,\n",
       "  1.6210574722055144,\n",
       "  1.6205585701712246,\n",
       "  1.6196048629694972,\n",
       "  1.621117295302781,\n",
       "  1.6203837641354264,\n",
       "  1.6187050477624527,\n",
       "  1.6190214157104492,\n",
       "  1.6188864414327837,\n",
       "  1.6191889852138575,\n",
       "  1.6188453952667161,\n",
       "  1.617907458925482,\n",
       "  1.6191911168873603,\n",
       "  1.619835142431588,\n",
       "  1.620379349868286,\n",
       "  1.6205790694711244,\n",
       "  1.6195048199498594,\n",
       "  1.6197468259651673,\n",
       "  1.6189069184176441,\n",
       "  1.6191795805992164,\n",
       "  1.6206900039917143,\n",
       "  1.6221800625617868,\n",
       "  1.6236305448222044,\n",
       "  1.6227133943529553,\n",
       "  1.6206936389941888,\n",
       "  1.619010510115788,\n",
       "  1.6178483046921603,\n",
       "  1.6186163102464723,\n",
       "  1.6186695533432984,\n",
       "  1.618523959455819,\n",
       "  1.6185147715319554,\n",
       "  1.6185542209982284,\n",
       "  1.6176422112093771,\n",
       "  1.6188107364870645,\n",
       "  1.6182407205327978,\n",
       "  1.618248788594025,\n",
       "  1.6188486174409613,\n",
       "  1.6183486624891534,\n",
       "  1.6182718347446086,\n",
       "  1.618819437003488,\n",
       "  1.6196734300387905,\n",
       "  1.6191703175089043,\n",
       "  1.6201732317215116,\n",
       "  1.621292940501509,\n",
       "  1.6208761912848562,\n",
       "  1.6202525111842039,\n",
       "  1.621826507775067,\n",
       "  1.6214585404090693,\n",
       "  1.621394951942519,\n",
       "  1.620326880750985,\n",
       "  1.6195808020718578,\n",
       "  1.620293842160643,\n",
       "  1.621400808465892,\n",
       "  1.62030353041118,\n",
       "  1.621560099676912,\n",
       "  1.6205048631564738,\n",
       "  1.6208670221526047,\n",
       "  1.6208684532513171,\n",
       "  1.6207246592479387,\n",
       "  1.6212714370248353,\n",
       "  1.6237019547100724,\n",
       "  1.622868776908649,\n",
       "  1.6229215895601095,\n",
       "  1.6216577356084814,\n",
       "  1.6218439411059977,\n",
       "  1.621138802302882,\n",
       "  1.6203626118270047,\n",
       "  1.6203284245993703,\n",
       "  1.6210921392064963,\n",
       "  1.6240516425353553,\n",
       "  1.6230333749883867,\n",
       "  1.6207386631096525,\n",
       "  1.6198761046226389,\n",
       "  1.6194717103037342,\n",
       "  1.6190167688971082,\n",
       "  1.6191189682542397,\n",
       "  1.6192694691014407,\n",
       "  1.618945695496545,\n",
       "  1.6187578674607677,\n",
       "  1.6187099089176196,\n",
       "  1.6190261588308024,\n",
       "  1.6186047352006283,\n",
       "  1.6195486566703308,\n",
       "  1.620392889224837,\n",
       "  1.619939976725085,\n",
       "  1.6218516380328851,\n",
       "  1.6232513612127069,\n",
       "  1.6248880348769315,\n",
       "  1.6234204070321445,\n",
       "  1.6218894074115846,\n",
       "  1.6209647238548166,\n",
       "  1.621483687109548,\n",
       "  1.6207337960821067,\n",
       "  1.6209515262707113,\n",
       "  1.6201489153753947,\n",
       "  1.6201229336226515,\n",
       "  1.6204740730999725,\n",
       "  1.621236430013121,\n",
       "  1.6198572026097715,\n",
       "  1.6211045422577506,\n",
       "  1.6202424177395298,\n",
       "  1.6198916047664698,\n",
       "  1.6190777500274733,\n",
       "  1.619652256589805,\n",
       "  1.6203898684731846,\n",
       "  1.6215345301651602,\n",
       "  1.6212378517160275,\n",
       "  1.6213564097587698,\n",
       "  1.6209198130762636,\n",
       "  1.6194499689957191,\n",
       "  1.619476425823907,\n",
       "  1.6195219385212865,\n",
       "  1.620071812803522,\n",
       "  1.619471724984681,\n",
       "  1.6194779056633635,\n",
       "  1.6194344459496108,\n",
       "  1.620148008092871,\n",
       "  1.6198353931821625,\n",
       "  1.6201058121150351,\n",
       "  1.62099323953901,\n",
       "  1.6204230374303357,\n",
       "  1.6227401230722813,\n",
       "  1.6256044433621937,\n",
       "  1.6268137904810789,\n",
       "  1.628289818763733,\n",
       "  1.6272079709715443,\n",
       "  1.624849469203667,\n",
       "  1.6251276009188498,\n",
       "  1.6225084641884113,\n",
       "  1.6206146519759606,\n",
       "  1.6213999317197376,\n",
       "  1.619750420448228,\n",
       "  1.6197832910885364,\n",
       "  1.6196497091518833,\n",
       "  1.619716372983209,\n",
       "  1.6192566913924193,\n",
       "  1.619431363537981,\n",
       "  1.6198623943798647,\n",
       "  1.6192671495118165,\n",
       "  1.6190792515947314,\n",
       "  1.6205212729317802,\n",
       "  1.6199143038594663,\n",
       "  1.6205579946781028,\n",
       "  1.6209462652065483,\n",
       "  1.6196597350642012,\n",
       "  1.619201811076385,\n",
       "  1.6188738962699627,\n",
       "  1.6194598809838883,\n",
       "  1.619326568002184,\n",
       "  1.6197354047756476,\n",
       "  1.619497255151495,\n",
       "  1.620309917797596,\n",
       "  1.6195202472761934,\n",
       "  1.6192295527810534,\n",
       "  1.6192268062694906,\n",
       "  1.6194174636173717,\n",
       "  1.6196746473829147,\n",
       "  1.6197999228397613,\n",
       "  1.62056245416256,\n",
       "  1.6186015611798892,\n",
       "  1.6199360768783269,\n",
       "  1.6197042359507143,\n",
       "  1.62061548291756,\n",
       "  1.6204488711991334,\n",
       "  1.621366419228427,\n",
       "  1.620937109580768,\n",
       "  1.6205753299403074,\n",
       "  1.620050155470524,\n",
       "  1.620509391347763,\n",
       "  1.6211001532418388,\n",
       "  1.6215982213983395,\n",
       "  1.6196836914334978,\n",
       "  1.6196738775140547,\n",
       "  1.6190151804186441,\n",
       "  1.6192155876770395,\n",
       "  1.6186516038302718,\n",
       "  1.6184133402819705,\n",
       "  1.6187686890804123,\n",
       "  1.6186733204742958,\n",
       "  1.6192465063386363],\n",
       " 'val_ATT_acc': [37.19512195121951,\n",
       "  37.19512195121951,\n",
       "  46.13821138211382,\n",
       "  50.0,\n",
       "  54.0650406504065,\n",
       "  60.77235772357724,\n",
       "  64.4308943089431,\n",
       "  70.1219512195122,\n",
       "  73.17073170731707,\n",
       "  75.20325203252033,\n",
       "  78.2520325203252,\n",
       "  79.8780487804878,\n",
       "  82.72357723577235,\n",
       "  82.92682926829268,\n",
       "  84.7560975609756,\n",
       "  85.5691056910569,\n",
       "  86.78861788617886,\n",
       "  87.1951219512195,\n",
       "  87.1951219512195,\n",
       "  88.41463414634147,\n",
       "  89.02439024390245,\n",
       "  89.02439024390245,\n",
       "  89.63414634146342,\n",
       "  89.83739837398375,\n",
       "  90.44715447154472,\n",
       "  90.44715447154472,\n",
       "  90.44715447154472,\n",
       "  91.66666666666667,\n",
       "  91.46341463414635,\n",
       "  91.46341463414635,\n",
       "  91.869918699187,\n",
       "  92.27642276422765,\n",
       "  93.08943089430895,\n",
       "  92.47967479674797,\n",
       "  92.6829268292683,\n",
       "  92.88617886178862,\n",
       "  93.90243902439025,\n",
       "  94.3089430894309,\n",
       "  94.10569105691057,\n",
       "  93.69918699186992,\n",
       "  94.3089430894309,\n",
       "  94.10569105691057,\n",
       "  94.3089430894309,\n",
       "  94.91869918699187,\n",
       "  94.91869918699187,\n",
       "  94.91869918699187,\n",
       "  94.71544715447155,\n",
       "  95.1219512195122,\n",
       "  95.32520325203252,\n",
       "  94.71544715447155,\n",
       "  95.32520325203252,\n",
       "  95.1219512195122,\n",
       "  95.1219512195122,\n",
       "  95.73170731707317,\n",
       "  95.73170731707317,\n",
       "  95.52845528455285,\n",
       "  95.9349593495935,\n",
       "  95.73170731707317,\n",
       "  95.52845528455285,\n",
       "  95.1219512195122,\n",
       "  95.9349593495935,\n",
       "  95.52845528455285,\n",
       "  95.9349593495935,\n",
       "  96.13821138211382,\n",
       "  96.13821138211382,\n",
       "  95.1219512195122,\n",
       "  95.73170731707317,\n",
       "  95.52845528455285,\n",
       "  96.13821138211382,\n",
       "  96.13821138211382,\n",
       "  95.9349593495935,\n",
       "  96.54471544715447,\n",
       "  96.13821138211382,\n",
       "  96.13821138211382,\n",
       "  96.13821138211382,\n",
       "  96.13821138211382,\n",
       "  96.34146341463415,\n",
       "  96.34146341463415,\n",
       "  96.13821138211382,\n",
       "  95.73170731707317,\n",
       "  95.1219512195122,\n",
       "  95.52845528455285,\n",
       "  96.13821138211382,\n",
       "  96.34146341463415,\n",
       "  96.34146341463415,\n",
       "  96.34146341463415,\n",
       "  95.1219512195122,\n",
       "  95.52845528455285,\n",
       "  94.71544715447155,\n",
       "  95.73170731707317,\n",
       "  96.34146341463415,\n",
       "  96.13821138211382,\n",
       "  96.54471544715447,\n",
       "  95.52845528455285,\n",
       "  96.13821138211382,\n",
       "  95.9349593495935,\n",
       "  95.9349593495935,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  96.95121951219512,\n",
       "  96.34146341463415,\n",
       "  96.34146341463415,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  96.95121951219512,\n",
       "  96.54471544715447,\n",
       "  96.54471544715447,\n",
       "  96.54471544715447,\n",
       "  96.34146341463415,\n",
       "  96.13821138211382,\n",
       "  96.54471544715447,\n",
       "  96.54471544715447,\n",
       "  96.34146341463415,\n",
       "  96.7479674796748,\n",
       "  96.95121951219512,\n",
       "  96.54471544715447,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  97.15447154471545,\n",
       "  96.7479674796748,\n",
       "  97.5609756097561,\n",
       "  97.35772357723577,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  96.95121951219512,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  96.54471544715447,\n",
       "  96.95121951219512,\n",
       "  96.13821138211382,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  97.15447154471545,\n",
       "  95.73170731707317,\n",
       "  96.7479674796748,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  96.13821138211382,\n",
       "  96.34146341463415,\n",
       "  96.7479674796748,\n",
       "  96.13821138211382,\n",
       "  96.95121951219512,\n",
       "  97.15447154471545,\n",
       "  97.15447154471545,\n",
       "  96.7479674796748,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  96.7479674796748,\n",
       "  97.35772357723577,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  96.34146341463415,\n",
       "  97.15447154471545,\n",
       "  96.7479674796748,\n",
       "  96.54471544715447,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  96.34146341463415,\n",
       "  96.7479674796748,\n",
       "  97.15447154471545,\n",
       "  96.34146341463415,\n",
       "  96.95121951219512,\n",
       "  96.54471544715447,\n",
       "  96.95121951219512,\n",
       "  97.15447154471545,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  96.34146341463415,\n",
       "  96.95121951219512,\n",
       "  96.34146341463415,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  97.15447154471545,\n",
       "  96.7479674796748,\n",
       "  96.7479674796748,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  97.35772357723577,\n",
       "  96.95121951219512,\n",
       "  97.5609756097561,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  96.54471544715447,\n",
       "  97.76422764227642,\n",
       "  96.7479674796748,\n",
       "  96.54471544715447,\n",
       "  96.54471544715447,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  96.54471544715447,\n",
       "  97.76422764227642,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  96.54471544715447,\n",
       "  97.35772357723577,\n",
       "  96.54471544715447,\n",
       "  97.5609756097561,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  97.15447154471545,\n",
       "  97.76422764227642,\n",
       "  97.5609756097561,\n",
       "  97.15447154471545,\n",
       "  97.35772357723577,\n",
       "  97.15447154471545,\n",
       "  97.15447154471545,\n",
       "  97.5609756097561,\n",
       "  97.35772357723577,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  97.35772357723577,\n",
       "  97.35772357723577,\n",
       "  97.15447154471545,\n",
       "  97.35772357723577,\n",
       "  97.35772357723577,\n",
       "  97.15447154471545,\n",
       "  97.35772357723577,\n",
       "  97.15447154471545,\n",
       "  97.15447154471545,\n",
       "  97.35772357723577,\n",
       "  97.5609756097561,\n",
       "  97.35772357723577,\n",
       "  96.95121951219512,\n",
       "  97.5609756097561,\n",
       "  96.95121951219512,\n",
       "  97.5609756097561,\n",
       "  97.15447154471545,\n",
       "  97.15447154471545,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  96.7479674796748,\n",
       "  96.95121951219512,\n",
       "  96.54471544715447,\n",
       "  97.35772357723577,\n",
       "  96.54471544715447,\n",
       "  97.35772357723577,\n",
       "  97.35772357723577,\n",
       "  97.15447154471545,\n",
       "  97.35772357723577,\n",
       "  96.54471544715447,\n",
       "  97.35772357723577,\n",
       "  97.15447154471545,\n",
       "  97.15447154471545,\n",
       "  97.5609756097561,\n",
       "  96.95121951219512,\n",
       "  97.35772357723577,\n",
       "  96.7479674796748,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  97.35772357723577,\n",
       "  96.95121951219512,\n",
       "  96.54471544715447,\n",
       "  96.54471544715447,\n",
       "  97.5609756097561,\n",
       "  97.35772357723577,\n",
       "  96.95121951219512,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  96.54471544715447,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  97.5609756097561,\n",
       "  96.54471544715447,\n",
       "  96.95121951219512,\n",
       "  97.35772357723577,\n",
       "  96.7479674796748,\n",
       "  97.35772357723577,\n",
       "  96.7479674796748,\n",
       "  96.95121951219512,\n",
       "  97.15447154471545,\n",
       "  97.5609756097561,\n",
       "  96.13821138211382,\n",
       "  97.96747967479675,\n",
       "  96.7479674796748,\n",
       "  97.15447154471545,\n",
       "  97.76422764227642,\n",
       "  96.95121951219512,\n",
       "  97.35772357723577,\n",
       "  96.54471544715447,\n",
       "  97.96747967479675,\n",
       "  97.96747967479675,\n",
       "  97.35772357723577,\n",
       "  97.5609756097561,\n",
       "  97.35772357723577,\n",
       "  97.5609756097561,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  97.15447154471545,\n",
       "  97.15447154471545,\n",
       "  97.15447154471545,\n",
       "  96.7479674796748,\n",
       "  97.76422764227642,\n",
       "  97.35772357723577,\n",
       "  96.95121951219512,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  97.5609756097561,\n",
       "  96.13821138211382,\n",
       "  96.95121951219512,\n",
       "  97.5609756097561,\n",
       "  96.95121951219512,\n",
       "  97.35772357723577,\n",
       "  97.5609756097561,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  97.35772357723577,\n",
       "  96.34146341463415,\n",
       "  96.7479674796748,\n",
       "  97.5609756097561,\n",
       "  97.15447154471545,\n",
       "  97.35772357723577,\n",
       "  96.7479674796748,\n",
       "  97.15447154471545,\n",
       "  97.76422764227642,\n",
       "  96.54471544715447,\n",
       "  97.35772357723577,\n",
       "  97.96747967479675,\n",
       "  97.5609756097561,\n",
       "  96.54471544715447,\n",
       "  97.76422764227642,\n",
       "  97.35772357723577,\n",
       "  98.17073170731707,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  97.5609756097561,\n",
       "  97.35772357723577,\n",
       "  97.5609756097561,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  97.76422764227642,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  97.35772357723577,\n",
       "  96.95121951219512,\n",
       "  97.76422764227642,\n",
       "  96.34146341463415,\n",
       "  96.54471544715447,\n",
       "  96.95121951219512,\n",
       "  97.5609756097561,\n",
       "  96.7479674796748,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  97.5609756097561,\n",
       "  96.95121951219512,\n",
       "  96.54471544715447,\n",
       "  97.35772357723577,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  97.5609756097561,\n",
       "  96.95121951219512,\n",
       "  96.54471544715447,\n",
       "  97.35772357723577,\n",
       "  97.5609756097561,\n",
       "  96.7479674796748,\n",
       "  96.95121951219512,\n",
       "  97.15447154471545,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  97.35772357723577,\n",
       "  97.35772357723577,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  97.15447154471545,\n",
       "  97.5609756097561,\n",
       "  97.15447154471545,\n",
       "  97.35772357723577,\n",
       "  97.76422764227642,\n",
       "  97.5609756097561,\n",
       "  97.15447154471545,\n",
       "  97.15447154471545,\n",
       "  97.96747967479675,\n",
       "  97.35772357723577,\n",
       "  97.5609756097561,\n",
       "  97.76422764227642,\n",
       "  97.5609756097561,\n",
       "  97.35772357723577,\n",
       "  97.76422764227642,\n",
       "  97.5609756097561,\n",
       "  97.5609756097561,\n",
       "  97.35772357723577,\n",
       "  97.5609756097561,\n",
       "  97.35772357723577,\n",
       "  97.76422764227642,\n",
       "  97.15447154471545,\n",
       "  97.5609756097561,\n",
       "  97.76422764227642,\n",
       "  96.7479674796748,\n",
       "  96.7479674796748,\n",
       "  97.35772357723577,\n",
       "  97.35772357723577,\n",
       "  97.35772357723577,\n",
       "  97.35772357723577,\n",
       "  97.15447154471545,\n",
       "  97.76422764227642,\n",
       "  97.35772357723577,\n",
       "  96.95121951219512,\n",
       "  97.15447154471545,\n",
       "  97.35772357723577,\n",
       "  97.76422764227642,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  97.35772357723577,\n",
       "  97.35772357723577,\n",
       "  96.7479674796748,\n",
       "  96.34146341463415,\n",
       "  97.76422764227642,\n",
       "  97.76422764227642,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  97.35772357723577,\n",
       "  97.5609756097561,\n",
       "  96.54471544715447,\n",
       "  97.15447154471545,\n",
       "  97.35772357723577],\n",
       " 'val_VAL_acc': [86.69950738916256,\n",
       "  92.11822660098522,\n",
       "  94.08866995073892,\n",
       "  98.0295566502463,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.50738916256158,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.50738916256158,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158],\n",
       " 'val_VAL_jac': [0.019704433497536946,\n",
       "  0.3210180639633404,\n",
       "  0.5459770169751398,\n",
       "  0.5862069000751514,\n",
       "  0.634646967714056,\n",
       "  0.6781609211061975,\n",
       "  0.6674876870780155,\n",
       "  0.7101806278886467,\n",
       "  0.7101806302375981,\n",
       "  0.7331691375506922,\n",
       "  0.7380952459250765,\n",
       "  0.7389162608555385,\n",
       "  0.7569786597942484,\n",
       "  0.7725780045457662,\n",
       "  0.7528735710482292,\n",
       "  0.7651888419841898,\n",
       "  0.7725780045457662,\n",
       "  0.7709359676379876,\n",
       "  0.778325130199564,\n",
       "  0.7758620760123718,\n",
       "  0.778325130199564,\n",
       "  0.7684729134507955,\n",
       "  0.780788184386756,\n",
       "  0.778325130199564,\n",
       "  0.7733990218251797,\n",
       "  0.7857142927611402,\n",
       "  0.7832512385739482,\n",
       "  0.7758620760123718,\n",
       "  0.7857142927611402,\n",
       "  0.7742200391045932,\n",
       "  0.7816092016661695,\n",
       "  0.7865353100405538,\n",
       "  0.7766830932917853,\n",
       "  0.770935969986939,\n",
       "  0.7758620783613233,\n",
       "  0.7848932778306783,\n",
       "  0.779146149827929,\n",
       "  0.7857142951100918,\n",
       "  0.7865353123895054,\n",
       "  0.7717569872663526,\n",
       "  0.7766830956407369,\n",
       "  0.7865353123895054,\n",
       "  0.7931034576716681,\n",
       "  0.7931034576716681,\n",
       "  0.7931034576716681,\n",
       "  0.7963875267893222,\n",
       "  0.7873563296689189,\n",
       "  0.7865353123895054,\n",
       "  0.7865353123895054,\n",
       "  0.7857142951100918,\n",
       "  0.7758620783613233,\n",
       "  0.7750410563840068,\n",
       "  0.7627257854480461,\n",
       "  0.7766830956407369,\n",
       "  0.779967164758391,\n",
       "  0.7594417139814404,\n",
       "  0.779967164758391,\n",
       "  0.7865353123895054,\n",
       "  0.7857142951100918,\n",
       "  0.7816092040151211,\n",
       "  0.7742200391045932,\n",
       "  0.7717569872663526,\n",
       "  0.7668308788919683,\n",
       "  0.7627257854480461,\n",
       "  0.7701149480096225,\n",
       "  0.7824302189455831,\n",
       "  0.7701149480096225,\n",
       "  0.7783251325485154,\n",
       "  0.7832512362249966,\n",
       "  0.7766830932917853,\n",
       "  0.7717569872663526,\n",
       "  0.7733990241741312,\n",
       "  0.770935969986939,\n",
       "  0.7717569849174011,\n",
       "  0.7692939330791605,\n",
       "  0.7660098616125548,\n",
       "  0.7668308765430168,\n",
       "  0.7512315364894021,\n",
       "  0.7594417139814404,\n",
       "  0.7643678223558248,\n",
       "  0.7627257830990947,\n",
       "  0.7561576425148349,\n",
       "  0.7643678223558248,\n",
       "  0.7627257877969976,\n",
       "  0.7577996794226134,\n",
       "  0.7643678223558248,\n",
       "  0.7668308765430168,\n",
       "  0.7610837508892191,\n",
       "  0.7668308765430168,\n",
       "  0.7643678223558248,\n",
       "  0.7536945883276427,\n",
       "  0.7545156056070562,\n",
       "  0.7348111744584709,\n",
       "  0.749589497232672,\n",
       "  0.7619047681686326,\n",
       "  0.7430213496015576,\n",
       "  0.7454844037887498,\n",
       "  0.7520525514198642,\n",
       "  0.7397372804839035,\n",
       "  0.73891626320449,\n",
       "  0.7463054257660664,\n",
       "  0.7536945883276427,\n",
       "  0.7553366252354213,\n",
       "  0.7602627312608541,\n",
       "  0.7586206943530754,\n",
       "  0.7545156056070562,\n",
       "  0.7594417139814404,\n",
       "  0.7545156056070562,\n",
       "  0.7545156056070562,\n",
       "  0.7619047681686326,\n",
       "  0.7619047681686326,\n",
       "  0.7635468050764112,\n",
       "  0.7709359676379876,\n",
       "  0.7709359676379876,\n",
       "  0.769293930730209,\n",
       "  0.7619047681686326,\n",
       "  0.750410512163134,\n",
       "  0.7413793173916822,\n",
       "  0.7561576425148349,\n",
       "  0.7561576425148349,\n",
       "  0.7594417139814404,\n",
       "  0.7553366205375183,\n",
       "  0.7553366205375183,\n",
       "  0.7528735663503262,\n",
       "  0.7561576378169318,\n",
       "  0.7536945836297397,\n",
       "  0.7635468003785082,\n",
       "  0.7676518914734789,\n",
       "  0.7577996747247104,\n",
       "  0.7528735663503262,\n",
       "  0.7553366205375183,\n",
       "  0.7569786597942484,\n",
       "  0.7602627289119025,\n",
       "  0.7553366205375183,\n",
       "  0.750410512163134,\n",
       "  0.7495894948837205,\n",
       "  0.7619047658196811,\n",
       "  0.7545156032581047,\n",
       "  0.759441711632489,\n",
       "  0.7577996747247104,\n",
       "  0.7561576378169318,\n",
       "  0.7577996747247104,\n",
       "  0.7561576378169318,\n",
       "  0.7504105145120855,\n",
       "  0.747947457975942,\n",
       "  0.7397372804839035,\n",
       "  0.7545156056070562,\n",
       "  0.7627257854480461,\n",
       "  0.7651888396352383,\n",
       "  0.7651888396352383,\n",
       "  0.7701149480096225,\n",
       "  0.7668308765430168,\n",
       "  0.7553366252354213,\n",
       "  0.7577996770736619,\n",
       "  0.7635468027274597,\n",
       "  0.7536945883276427,\n",
       "  0.7545156079560078,\n",
       "  0.7594417139814404,\n",
       "  0.7577996747247104,\n",
       "  0.750410512163134,\n",
       "  0.747947457975942,\n",
       "  0.749589497232672,\n",
       "  0.7619047634707296,\n",
       "  0.7545156009091533,\n",
       "  0.7577996723757588,\n",
       "  0.7536945836297397,\n",
       "  0.7553366205375183,\n",
       "  0.7536945836297397,\n",
       "  0.7651888349373352,\n",
       "  0.7602627312608541,\n",
       "  0.7643678200068732,\n",
       "  0.7643678247047763,\n",
       "  0.7586206943530754,\n",
       "  0.7643678247047763,\n",
       "  0.7635468027274597,\n",
       "  0.7643678200068732,\n",
       "  0.7586206943530754,\n",
       "  0.7553366205375183,\n",
       "  0.7577996747247104,\n",
       "  0.7528735663503262,\n",
       "  0.7553366205375183,\n",
       "  0.7619047681686326,\n",
       "  0.749589497232672,\n",
       "  0.7676518914734789,\n",
       "  0.7454844037887498,\n",
       "  0.7577996770736619,\n",
       "  0.7643678223558248,\n",
       "  0.7627257830990947,\n",
       "  0.7717569849174011,\n",
       "  0.7709359652890361,\n",
       "  0.7651888396352383,\n",
       "  0.7586206943530754,\n",
       "  0.7676518938224304,\n",
       "  0.7586206943530754,\n",
       "  0.7602627312608541,\n",
       "  0.7635468027274597,\n",
       "  0.7775041082222474,\n",
       "  0.7775041082222474,\n",
       "  0.7750410540350552,\n",
       "  0.7742200367556417,\n",
       "  0.7651888396352383,\n",
       "  0.7586206943530754,\n",
       "  0.7742200391045932,\n",
       "  0.7733990218251797,\n",
       "  0.7577996770736619,\n",
       "  0.7577996770736619,\n",
       "  0.7594417139814404,\n",
       "  0.759441716330392,\n",
       "  0.7487684846511615,\n",
       "  0.7364532090172979,\n",
       "  0.749589497232672,\n",
       "  0.750410512163134,\n",
       "  0.73891626320449,\n",
       "  0.7463054234171148,\n",
       "  0.7487684799532585,\n",
       "  0.7438423715788742,\n",
       "  0.750410512163134,\n",
       "  0.7577996747247104,\n",
       "  0.7528735663503262,\n",
       "  0.7446633841603848,\n",
       "  0.749589492534769,\n",
       "  0.747947457975942,\n",
       "  0.7446633865093363,\n",
       "  0.7528735686992777,\n",
       "  0.7512315341404506,\n",
       "  0.7495894948837205,\n",
       "  0.7298850637351351,\n",
       "  0.7430213566484123,\n",
       "  0.757799681771565,\n",
       "  0.7545156103049593,\n",
       "  0.7553366252354213,\n",
       "  0.7619047681686326,\n",
       "  0.7602627312608541,\n",
       "  0.759441716330392,\n",
       "  0.7619047681686326,\n",
       "  0.7586206967020269,\n",
       "  0.7561576425148349,\n",
       "  0.7553366228864697,\n",
       "  0.7742200367556417,\n",
       "  0.7692939283812574,\n",
       "  0.7651888396352383,\n",
       "  0.7660098545657003,\n",
       "  0.7602627312608541,\n",
       "  0.7619047681686326,\n",
       "  0.7536945836297397,\n",
       "  0.7577996770736619,\n",
       "  0.761083746191316,\n",
       "  0.7701149480096225,\n",
       "  0.7651888396352383,\n",
       "  0.759441711632489,\n",
       "  0.7602627312608541,\n",
       "  0.759441711632489,\n",
       "  0.7553366228864697,\n",
       "  0.7635468003785082,\n",
       "  0.7512315294425476,\n",
       "  0.7487684752553555,\n",
       "  0.7553366228864697,\n",
       "  0.7619047681686326,\n",
       "  0.7635468003785082,\n",
       "  0.7676518938224304,\n",
       "  0.7561576401658834,\n",
       "  0.7602627289119025,\n",
       "  0.7463054210681633,\n",
       "  0.7405582954143656,\n",
       "  0.7528735663503262,\n",
       "  0.7561576401658834,\n",
       "  0.748768477604307,\n",
       "  0.7553366205375183,\n",
       "  0.7536945859786912,\n",
       "  0.758620692004124,\n",
       "  0.7553366228864697,\n",
       "  0.7643678200068732,\n",
       "  0.7528735686992777,\n",
       "  0.759441711632489,\n",
       "  0.7610837508892191,\n",
       "  0.7553366228864697,\n",
       "  0.7479474603248935,\n",
       "  0.7528735686992777,\n",
       "  0.7602627312608541,\n",
       "  0.7692939283812574,\n",
       "  0.7717569849174011,\n",
       "  0.7561576425148349,\n",
       "  0.7545156032581047,\n",
       "  0.749589497232672,\n",
       "  0.7389162608555385,\n",
       "  0.7463054257660664,\n",
       "  0.769293930730209,\n",
       "  0.7619047681686326,\n",
       "  0.7561576401658834,\n",
       "  0.7627257854480461,\n",
       "  0.7569786574452969,\n",
       "  0.7602627312608541,\n",
       "  0.7619047658196811,\n",
       "  0.7610837508892191,\n",
       "  0.759441711632489,\n",
       "  0.7619047658196811,\n",
       "  0.7619047658196811,\n",
       "  0.7569786574452969,\n",
       "  0.7569786574452969,\n",
       "  0.759441711632489,\n",
       "  0.7791461451300259,\n",
       "  0.7758620736634203,\n",
       "  0.7668308741940654,\n",
       "  0.7668308741940654,\n",
       "  0.7668308765430168,\n",
       "  0.769293930730209,\n",
       "  0.7643678223558248,\n",
       "  0.7643678223558248,\n",
       "  0.7651888396352383,\n",
       "  0.7619047658196811,\n",
       "  0.7561576401658834,\n",
       "  0.7569786574452969,\n",
       "  0.7389162608555385,\n",
       "  0.751231531791499,\n",
       "  0.7438423668809713,\n",
       "  0.750410512163134,\n",
       "  0.7454844061377013,\n",
       "  0.7528735663503262,\n",
       "  0.7619047681686326,\n",
       "  0.761083746191316,\n",
       "  0.7561576378169318,\n",
       "  0.7545156032581047,\n",
       "  0.7339901548301058,\n",
       "  0.7339901548301058,\n",
       "  0.7348111721095193,\n",
       "  0.7479474603248935,\n",
       "  0.7610837485402676,\n",
       "  0.7512315341404506,\n",
       "  0.7528735686992777,\n",
       "  0.7602627312608541,\n",
       "  0.7536945859786912,\n",
       "  0.7602627312608541,\n",
       "  0.7758620736634203,\n",
       "  0.7709359652890361,\n",
       "  0.7709359652890361,\n",
       "  0.7692939283812574,\n",
       "  0.7717569825684496,\n",
       "  0.7643678200068732,\n",
       "  0.7725780021968146,\n",
       "  0.761083746191316,\n",
       "  0.7733990171272767,\n",
       "  0.7610837485402676,\n",
       "  0.7660098569146518,\n",
       "  0.7684729111018439,\n",
       "  0.7660098592636033,\n",
       "  0.7660098592636033,\n",
       "  0.7643678223558248,\n",
       "  0.7586206967020269,\n",
       "  0.7495894948837205,\n",
       "  0.7487684799532585,\n",
       "  0.7520525490709127,\n",
       "  0.7577996747247104,\n",
       "  0.758620692004124,\n",
       "  0.759441711632489,\n",
       "  0.7627257877969976,\n",
       "  0.7528735710482292,\n",
       "  0.769293930730209,\n",
       "  0.7676518938224304,\n",
       "  0.7619047681686326,\n",
       "  0.7775041105711988,\n",
       "  0.7684729111018439,\n",
       "  0.7660098569146518,\n",
       "  0.7750410540350552,\n",
       "  0.7725779998478631,\n",
       "  0.7602627312608541,\n",
       "  0.7528735686992777,\n",
       "  0.7528735710482292,\n",
       "  0.7438423715788742,\n",
       "  0.7504105168610371,\n",
       "  0.759441711632489,\n",
       "  0.7577996770736619,\n",
       "  0.7602627312608541,\n",
       "  0.7627257854480461,\n",
       "  0.7709359652890361,\n",
       "  0.7643678200068732,\n",
       "  0.7668308741940654,\n",
       "  0.7725779998478631,\n",
       "  0.7577996770736619,\n",
       "  0.7536945883276427,\n",
       "  0.7536945883276427,\n",
       "  0.7569786597942484,\n",
       "  0.7627257877969976,\n",
       "  0.7651888419841898,\n",
       "  0.7520525537688156,\n",
       "  0.7586206967020269,\n",
       "  0.7602627312608541,\n",
       "  0.7676518961713819,\n",
       "  0.760262735958757,\n",
       "  0.7660098569146518,\n",
       "  0.7610837485402676,\n",
       "  0.7627257854480461,\n",
       "  0.7651888396352383,\n",
       "  0.7602627312608541,\n",
       "  0.7676518914734789,\n",
       "  0.7619047658196811,\n",
       "  0.7610837508892191,\n",
       "  0.7512315341404506,\n",
       "  0.7594417139814404,\n",
       "  0.7454844061377013,\n",
       "  0.7619047681686326,\n",
       "  0.7643678223558248,\n",
       "  0.7651888396352383,\n",
       "  0.7651888396352383,\n",
       "  0.7610837508892191,\n",
       "  0.7775041105711988,\n",
       "  0.7807881820378045,\n",
       "  0.7775041105711988,\n",
       "  0.7676518938224304,\n",
       "  0.7766830909428338,\n",
       "  0.7586206943530754,\n",
       "  0.7577996794226134,\n",
       "  0.7602627312608541,\n",
       "  0.7602627312608541,\n",
       "  0.7651888419841898,\n",
       "  0.7528735710482292,\n",
       "  0.7545156056070562,\n",
       "  0.7651888396352383,\n",
       "  0.7651888396352383,\n",
       "  0.7676518938224304,\n",
       "  0.7692939283812574,\n",
       "  0.759441711632489,\n",
       "  0.7701149480096225,\n",
       "  0.7725780021968146,\n",
       "  0.7709359676379876,\n",
       "  0.7676518938224304,\n",
       "  0.7627257854480461,\n",
       "  0.7742200391045932,\n",
       "  0.7758620760123718,\n",
       "  0.7758620760123718,\n",
       "  0.7577996794226134,\n",
       "  0.7643678200068732,\n",
       "  0.7643678223558248,\n",
       "  0.7651888396352383,\n",
       "  0.7561576378169318,\n",
       "  0.7577996747247104,\n",
       "  0.770114945660671,\n",
       "  0.759441711632489,\n",
       "  0.7577996770736619,\n",
       "  0.7586206967020269,\n",
       "  0.7676518938224304,\n",
       "  0.7577996770736619,\n",
       "  0.7619047681686326],\n",
       " 'val_VAL_acc_1': [59.60591133004926,\n",
       "  64.03940886699507,\n",
       "  68.47290640394088,\n",
       "  72.9064039408867,\n",
       "  72.9064039408867,\n",
       "  74.38423645320196,\n",
       "  75.86206896551724,\n",
       "  77.33990147783251,\n",
       "  77.33990147783251,\n",
       "  78.32512315270937,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  79.3103448275862,\n",
       "  78.81773399014779,\n",
       "  79.3103448275862,\n",
       "  78.32512315270937,\n",
       "  78.32512315270937,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  78.32512315270937,\n",
       "  77.83251231527093,\n",
       "  78.32512315270937,\n",
       "  78.81773399014779,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  78.81773399014779,\n",
       "  78.81773399014779,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  78.81773399014779,\n",
       "  78.32512315270937,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  79.3103448275862,\n",
       "  77.83251231527093,\n",
       "  78.81773399014779,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  77.83251231527093,\n",
       "  78.81773399014779,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  79.3103448275862,\n",
       "  78.81773399014779,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  78.32512315270937,\n",
       "  78.32512315270937,\n",
       "  76.84729064039409,\n",
       "  76.84729064039409,\n",
       "  79.3103448275862,\n",
       "  78.32512315270937,\n",
       "  77.83251231527093,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  78.81773399014779,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  81.2807881773399,\n",
       "  82.26600985221675,\n",
       "  82.26600985221675,\n",
       "  82.26600985221675,\n",
       "  82.75862068965517,\n",
       "  83.2512315270936,\n",
       "  83.2512315270936,\n",
       "  82.26600985221675,\n",
       "  82.26600985221675,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  77.83251231527093,\n",
       "  77.33990147783251,\n",
       "  77.33990147783251,\n",
       "  76.84729064039409,\n",
       "  75.36945812807882,\n",
       "  76.84729064039409,\n",
       "  76.35467980295566,\n",
       "  76.84729064039409,\n",
       "  76.35467980295566,\n",
       "  77.33990147783251,\n",
       "  78.32512315270937,\n",
       "  78.81773399014779,\n",
       "  78.81773399014779,\n",
       "  77.33990147783251,\n",
       "  77.33990147783251,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  78.81773399014779,\n",
       "  79.3103448275862,\n",
       "  77.33990147783251,\n",
       "  78.32512315270937,\n",
       "  77.83251231527093,\n",
       "  76.35467980295566,\n",
       "  78.32512315270937,\n",
       "  76.84729064039409,\n",
       "  78.32512315270937,\n",
       "  77.83251231527093,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  78.32512315270937,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  81.77339901477832,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  83.2512315270936,\n",
       "  83.2512315270936,\n",
       "  83.74384236453201,\n",
       "  81.77339901477832,\n",
       "  81.2807881773399,\n",
       "  81.77339901477832,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  81.2807881773399,\n",
       "  80.78817733990148,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  78.81773399014779,\n",
       "  78.81773399014779,\n",
       "  79.80295566502463,\n",
       "  81.2807881773399,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  78.81773399014779,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  78.81773399014779,\n",
       "  78.81773399014779,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  81.2807881773399,\n",
       "  79.3103448275862,\n",
       "  77.83251231527093,\n",
       "  79.3103448275862,\n",
       "  78.32512315270937,\n",
       "  77.83251231527093,\n",
       "  78.81773399014779,\n",
       "  78.32512315270937,\n",
       "  80.29556650246306,\n",
       "  81.2807881773399,\n",
       "  81.77339901477832,\n",
       "  80.29556650246306,\n",
       "  78.81773399014779,\n",
       "  78.32512315270937,\n",
       "  81.77339901477832,\n",
       "  81.2807881773399,\n",
       "  81.77339901477832,\n",
       "  80.78817733990148,\n",
       "  81.77339901477832,\n",
       "  81.77339901477832,\n",
       "  83.2512315270936,\n",
       "  83.74384236453201,\n",
       "  82.75862068965517,\n",
       "  83.2512315270936,\n",
       "  83.74384236453201,\n",
       "  81.2807881773399,\n",
       "  81.77339901477832,\n",
       "  82.75862068965517,\n",
       "  81.2807881773399,\n",
       "  83.2512315270936,\n",
       "  82.26600985221675,\n",
       "  81.2807881773399,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  77.83251231527093,\n",
       "  76.35467980295566,\n",
       "  76.35467980295566,\n",
       "  75.86206896551724,\n",
       "  76.35467980295566,\n",
       "  76.35467980295566,\n",
       "  78.32512315270937,\n",
       "  78.81773399014779,\n",
       "  78.81773399014779,\n",
       "  81.77339901477832,\n",
       "  83.2512315270936,\n",
       "  83.74384236453201,\n",
       "  83.2512315270936,\n",
       "  83.2512315270936,\n",
       "  82.75862068965517,\n",
       "  82.26600985221675,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  80.78817733990148,\n",
       "  81.2807881773399,\n",
       "  79.3103448275862,\n",
       "  81.77339901477832,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  81.2807881773399,\n",
       "  79.80295566502463,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  78.81773399014779,\n",
       "  78.32512315270937,\n",
       "  78.81773399014779,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  81.77339901477832,\n",
       "  81.2807881773399,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  80.78817733990148,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  80.78817733990148,\n",
       "  79.80295566502463,\n",
       "  80.78817733990148,\n",
       "  81.77339901477832,\n",
       "  80.78817733990148,\n",
       "  78.81773399014779,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  78.32512315270937,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  78.32512315270937,\n",
       "  78.32512315270937,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  81.2807881773399,\n",
       "  80.78817733990148,\n",
       "  81.2807881773399,\n",
       "  80.78817733990148,\n",
       "  81.2807881773399,\n",
       "  81.2807881773399,\n",
       "  80.78817733990148,\n",
       "  81.77339901477832,\n",
       "  79.80295566502463,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  81.77339901477832,\n",
       "  81.77339901477832,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  80.78817733990148,\n",
       "  78.81773399014779,\n",
       "  78.32512315270937,\n",
       "  76.35467980295566,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  78.32512315270937,\n",
       "  77.83251231527093,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  81.2807881773399,\n",
       "  81.2807881773399,\n",
       "  81.77339901477832,\n",
       "  82.75862068965517,\n",
       "  80.29556650246306,\n",
       "  81.2807881773399,\n",
       "  81.2807881773399,\n",
       "  80.29556650246306,\n",
       "  81.2807881773399,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  79.80295566502463,\n",
       "  78.32512315270937,\n",
       "  78.32512315270937,\n",
       "  78.32512315270937,\n",
       "  79.3103448275862,\n",
       "  76.84729064039409,\n",
       "  78.81773399014779,\n",
       "  77.33990147783251,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  78.32512315270937,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  78.32512315270937,\n",
       "  78.32512315270937,\n",
       "  80.29556650246306,\n",
       "  82.75862068965517,\n",
       "  82.75862068965517,\n",
       "  80.29556650246306,\n",
       "  82.75862068965517,\n",
       "  80.78817733990148,\n",
       "  81.77339901477832,\n",
       "  81.2807881773399,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  81.2807881773399,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  78.32512315270937,\n",
       "  77.83251231527093,\n",
       "  76.84729064039409,\n",
       "  74.8768472906404,\n",
       "  78.81773399014779,\n",
       "  78.32512315270937,\n",
       "  77.33990147783251,\n",
       "  79.3103448275862,\n",
       "  78.32512315270937,\n",
       "  79.80295566502463,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  82.75862068965517,\n",
       "  82.75862068965517,\n",
       "  81.77339901477832,\n",
       "  81.77339901477832,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  78.32512315270937,\n",
       "  78.81773399014779,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  81.2807881773399,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  78.81773399014779,\n",
       "  78.81773399014779,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  78.81773399014779,\n",
       "  80.29556650246306,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  78.32512315270937,\n",
       "  80.78817733990148,\n",
       "  78.81773399014779,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  81.2807881773399,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  82.26600985221675],\n",
       " 'test_loss': 5.713684047955582,\n",
       " 'test_ATT_loss': 0.8162068346622687,\n",
       " 'test_VAL_loss': 1.6324924044311047,\n",
       " 'test_ATT_acc': 97.21669980119285,\n",
       " 'test_VAL_acc': 99.47916666666667,\n",
       " 'test_VAL_jac': 0.7647569527228674,\n",
       " 'test_VAL_acc_1': 82.8125,\n",
       " 'model_filename': 'model_storage/GAT/model.pth'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.save_dir+'GAT_best_model/best_config.p', 'wb') as fp:\n",
    "    pickle.dump(train_state,fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.save_dir+'GAT_best_model/best_config.p', 'rb') as fp:\n",
    "    train_state = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.DataFrame(train_state['val_VAL_acc'], columns=['val_VAL'])\n",
    "states['train_VAL'] = pd.DataFrame(train_state['train_VAL_acc'])\n",
    "states['val_ATT'] = pd.DataFrame(train_state['val_ATT_acc'])\n",
    "states['train_ATT'] = pd.DataFrame(train_state['train_ATT_acc'])\n",
    "states['val_VAL_1'] = pd.DataFrame(train_state['val_VAL_acc_1'])\n",
    "states['train_VAL_1'] = pd.DataFrame(train_state['train_VAL_acc_1'])\n",
    "states['train_VAL_jac'] = pd.DataFrame(train_state['train_VAL_jac'])\n",
    "states['val_VAL_jac'] = pd.DataFrame(train_state['val_VAL_jac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_VAL</th>\n",
       "      <th>train_VAL</th>\n",
       "      <th>val_ATT</th>\n",
       "      <th>train_ATT</th>\n",
       "      <th>val_VAL_1</th>\n",
       "      <th>train_VAL_1</th>\n",
       "      <th>train_VAL_jac</th>\n",
       "      <th>val_VAL_jac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.699507</td>\n",
       "      <td>88.642659</td>\n",
       "      <td>37.195122</td>\n",
       "      <td>42.659280</td>\n",
       "      <td>59.605911</td>\n",
       "      <td>55.401662</td>\n",
       "      <td>0.008310</td>\n",
       "      <td>0.019704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.118227</td>\n",
       "      <td>94.459834</td>\n",
       "      <td>37.195122</td>\n",
       "      <td>40.997230</td>\n",
       "      <td>64.039409</td>\n",
       "      <td>61.495845</td>\n",
       "      <td>0.288089</td>\n",
       "      <td>0.321018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94.088670</td>\n",
       "      <td>94.736842</td>\n",
       "      <td>46.138211</td>\n",
       "      <td>54.570637</td>\n",
       "      <td>68.472906</td>\n",
       "      <td>61.772853</td>\n",
       "      <td>0.453370</td>\n",
       "      <td>0.545977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98.029557</td>\n",
       "      <td>96.675900</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>61.772853</td>\n",
       "      <td>72.906404</td>\n",
       "      <td>65.096953</td>\n",
       "      <td>0.511542</td>\n",
       "      <td>0.586207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98.522167</td>\n",
       "      <td>97.506925</td>\n",
       "      <td>54.065041</td>\n",
       "      <td>67.036011</td>\n",
       "      <td>72.906404</td>\n",
       "      <td>66.204986</td>\n",
       "      <td>0.565097</td>\n",
       "      <td>0.634647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>99.507389</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>97.357724</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>80.295567</td>\n",
       "      <td>92.797784</td>\n",
       "      <td>0.892428</td>\n",
       "      <td>0.757800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>99.507389</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>97.560976</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>79.802956</td>\n",
       "      <td>93.905817</td>\n",
       "      <td>0.902124</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>99.507389</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.544715</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>79.802956</td>\n",
       "      <td>94.736842</td>\n",
       "      <td>0.902124</td>\n",
       "      <td>0.767652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>99.507389</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>97.154472</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>80.295567</td>\n",
       "      <td>92.797784</td>\n",
       "      <td>0.884580</td>\n",
       "      <td>0.757800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>99.507389</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>97.357724</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>82.266010</td>\n",
       "      <td>92.243767</td>\n",
       "      <td>0.873500</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_VAL   train_VAL    val_ATT   train_ATT  val_VAL_1  train_VAL_1  \\\n",
       "0    86.699507   88.642659  37.195122   42.659280  59.605911    55.401662   \n",
       "1    92.118227   94.459834  37.195122   40.997230  64.039409    61.495845   \n",
       "2    94.088670   94.736842  46.138211   54.570637  68.472906    61.772853   \n",
       "3    98.029557   96.675900  50.000000   61.772853  72.906404    65.096953   \n",
       "4    98.522167   97.506925  54.065041   67.036011  72.906404    66.204986   \n",
       "..         ...         ...        ...         ...        ...          ...   \n",
       "437  99.507389  100.000000  97.357724  100.000000  80.295567    92.797784   \n",
       "438  99.507389  100.000000  97.560976  100.000000  79.802956    93.905817   \n",
       "439  99.507389  100.000000  96.544715  100.000000  79.802956    94.736842   \n",
       "440  99.507389  100.000000  97.154472  100.000000  80.295567    92.797784   \n",
       "441  99.507389  100.000000  97.357724  100.000000  82.266010    92.243767   \n",
       "\n",
       "     train_VAL_jac  val_VAL_jac  \n",
       "0         0.008310     0.019704  \n",
       "1         0.288089     0.321018  \n",
       "2         0.453370     0.545977  \n",
       "3         0.511542     0.586207  \n",
       "4         0.565097     0.634647  \n",
       "..             ...          ...  \n",
       "437       0.892428     0.757800  \n",
       "438       0.902124     0.758621  \n",
       "439       0.902124     0.767652  \n",
       "440       0.884580     0.757800  \n",
       "441       0.873500     0.761905  \n",
       "\n",
       "[442 rows x 8 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "states[['val_ATT','train_ATT']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "states[['val_VAL','train_VAL']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABgJ0lEQVR4nO2dd3hcxdWH31HvxZLci2zjXjHGdBswPYROsAkEEoJJgNASEgjJhyEQCBAgJIFAqKGD6R0bMKYajHHvxrItFzWrl5VWO98fc2f37u5daVVW0lrzPo+e3Z299+7s1d7fPXPmnDNCSonBYDAYoo+Y7u6AwWAwGNqHEXCDwWCIUoyAGwwGQ5RiBNxgMBiiFCPgBoPBEKXEdeWH5ebmyvz8/K78SIPBYIh6vvvuu1IpZV5ge5cKeH5+PsuWLevKjzQYDIaoRwix3anduFAMBoMhSjECbjAYDFGKEXCDwWCIUrrUB+5EU1MThYWFNDQ0dHdXeg1JSUkMHjyY+Pj47u6KwWDoAN0u4IWFhaSnp5Ofn48Qoru7s98jpaSsrIzCwkKGDx/e3d0xGAwdoNtdKA0NDeTk5Bjx7iKEEOTk5JgRj8GwH9DtAg4Y8e5izPk2GPYPeoSAGwwGQ9g0VMKql7q7Fz2CsARcCHG1EGKNEGKtEOIaq22+EGKXEGKF9XdKRHtqMBgMAG9eBa9eCkXruu4zXdWh3/N4oKnev625CdyNke0TYQi4EGIicCkwA5gCnCqEGGW9fZ+Ucqr1924E+9ljSEtLC/ne8OHD2bhxo1/bNddcw1133QXA999/jxCCDz74IOxj2lmyZAnTpk0jLi6OBQsWtLHnBsN+wr4f1KO7i+Zx6vbBHYPh49ud3//gj3B7f/A0+9r+ewzcFpT53umEY4GPA76WUtZJKd3Ap8CZke1WdDJnzhxeeOEF72uPx8OCBQs477zzAHj++ec58sgjef7559t1/KFDh/Lkk09y/vnnd0p/DYaoxONWj5EW8MY6ePZc+Pch6vWSu8DtCt5u2ePqsbbU17Z3dWT7ZhFOGOEa4HYhRA5QD5wCLAPKgCuFED+zXv9WSlkeuLMQYh4wD5QAtcQtb61l3e6qNn2B1hg/MIObfzwh5Pt/+MMfGDZsGJdffjkA8+fPRwjBkiVLKC8vp6mpidtuu43TTz+91c+aO3cu5513HjfffDOgLOb8/HyGDRuGlJIFCxawcOFCjjrqKBoaGkhKSmrTd9GFwGJizNSFoRejBbwlt0ZHqSmB9W/C5g/92ysLIWekf1tCCtS7oHoPpPfzf69qt2ofdFBEutmqEkgp1wN/AxYC7wMrATfwEDASmArsAf4eYv9HpJTTpZTT8/IiP6RoK3PmzOHFF1/0vn7ppZf4+c9/zmuvvcby5cv55JNP+O1vf0s4a4dOnjyZmJgYVq5cCcALL7zA3LlzAfjiiy8YPnw4I0eO5Oijj+bdd3uFx8nQG5FS/UUKLeANnWvs+fHI0fDOdcHtditbE5+qHmuKgt+7fxL899hO7ZqdsBJ5pJSPAY8BCCH+ChRKKb29FUL8F3i7o51pyVKOFAceeCDFxcXs3r2bkpISsrOzGTBgANdeey1LliwhJiaGXbt2UVRURP/+/Vs93ty5c3nhhReYMGECb7zxBrfeeiug3Cdz5swB1E3j6aef5qyzzorodzMYuoX3b4CN78LhV8G7v4Ob9kJ8cucd32uBR1DAqwqd2x8/AY6bD0de62tLsAS8ek/w9rqvUkIEwnfDjULpaz0OBc4CnhdCDLBtcibK1RKVnHPOOSxYsIAXX3yROXPm8Oyzz1JSUsJ3333HihUr6NevX9iJL3PnzuWll15i0aJFTJ48mb59+9Lc3Mwrr7zCrbfeSn5+Pr/5zW947733qK6O4BDQ0DP5/hnYvDAyx67aDe//UUVAFHwB3/y39X2++Afs/Kbz+lBbBkv/AxU7YNEtqq3w2847PvgmCz+4qWt8zac/CBe86nu9aL7/+/GWK7R6r3psctCKCN1swnWmviKEWAe8BVxh+brvEkKsFkKsAo4Brm3xCD0YPfm4YMECzjnnHCorK+nbty/x8fF88sknbN/uWIrXkZEjR5KTk8MNN9zgdZ8sWrSIKVOmsHPnTgoKCti+fTtnn302r7/+eoS+kaHH8sYV8Ow5kTn2+zfC1/+GpQ/Dk6co67clGutg4f/BY8d3Xh/2rvI9l5bQFnzeeccHX8ieux7+1/rcFLWlUF7gcJwG2Otgdwa6f4YeCvlH+rfZQxi1YGsLvKEi+Jh1+1rvZzsIS8CllEdJKcdLKadIKT+y2i6UUk6SUk6WUp4mpXQYP0QHEyZMoLq6mkGDBjFgwAB++tOfsmzZMqZPn86zzz7L2LFj23S8uXPnsmHDBs48UwXrPP/8897nmrPPPpvnnnsOgLq6OgYPHuz9u/feex2P++233zJ48GBefvllLrvsMiZM6HqXk6EHowXzw5tsbS34oks3hn6vvdjFq6lOPa59zT/EriN4PP6fEY4w/mOK+gvkravhP0cEHyPwdXwyxCX6t734U99zPZlaUxy6T/VB8R2dQrcXs+oprF7tG4rl5uby1VdfOW5XU1PT6rGuvfZarr3WNyB58skng7Y57bTTOO200wAVbhgOBx98MIWFIXxzhu7hq3/Dhnfh4rdb9nGWboaHZ0W2L/EpwW3uhtD+5+IN6jE20fn99tBQGdxWugk2vQ9jf9Tx49cUgQzveuH1K6CuDBqtazbQD61HBnVlkNLH1/7ESf7HCTyvw46EkvW+19o9ooXcSazru9ECNxgMDtTtg0/ugO2f+5JLQvH1g9BU63sdiSiNZofMv1CRGnX74J3fquedOcGoBXzggepxjJWgXbGjc46/9CEgzMnAFc/Apvd8rwPDDnU4rj2yREp1s7WjJyk1gw6ERut/6Wn23SBc1WqEsOjm4L7UV4TX5zZiLPB2sHr1ai688EK/tsTERJYuXdopx7/99tt5+eWX/drOPfdcbrrpphB7GLqFr/4FjZYobP0Y4pIgc5Dztru/93/dWAuJ4WXgho1TiJurOjg2GWDLR74bSkOFmviM7YT68A2VIGJh0HT1nTOHqPbAVPP2UFOiJmYnnQurdS0UqcQxOav1/av3QlKGel5b6utTXam60XnPgYTxp8O6N9T7+rzM+oPaLyFdjWya3f435cYa2PC286RthHzgRsDbwaRJk1ixYkXEjn/TTTcZse7p1JbB1/+BCWdBwWdqwvDd38E1qyHLIWGtaG3A/iVdJOAhLPDaEvV43C3KYqwpDn3zaQsNlZCUCX2teSN9zM7Imlz+lPKrz/o9bPoAXJa1X7YFBk9vff/qPZA3WmVT3m1Lxqktgf8cCRXb4WprElbfeOwc80f1+NW/1WNjNbhsLlVXjepj1jCYtxjustXbj5AP3LhQDIb2sPQ/SkyOvgGy833tX/4zeNvGumD3Rl1Z6GO/eVV4IYCB1JaoG8pIW+JISwIeEwd5ltBWhB9p1SL1FZaAj1evU/MgLtk3odkRtn0K/SZB7ii4ZiX86gvVvv2L8PbXYX4rngtoL/J9/xJrYjdzcOjjJFg33qUPw/Mq0oz0gcoCr69QmZpJmapdxEJihrLyI4ARcIOhPZRuVEKSNwbSbQleu5YHb1u5M7hNW8B2mhrg28eUFddaCGAgnmY1UZYzEtJtKRqh0s1rSyAlF4bMUILUnhuGEw2Vyp0xeAYcfSOMPknFSTvFRrcFt0vFqw8/Sr1Ozob+EyFnVPhhijrMb9P7/u1bFvmer7fcJk4WuEaPnBbfAUVW8EPOSCXgjbVq0jMmFn70d/j1l+o3sntFeH1sI0bADYb24KqBxHT13C6YTpas0wSek7tj4f85p297j7PTf8huZ982FZ2ROdg/0iKUgNeVKes4pQ9M/okStcZa523bgnahxMap0UlKH2WBuzvoAy9cptwwgfHYw4+C7V8pf3QQAZOdVbvUdtu/9G/ftcz3fJNV+6RFCzw9uK2fFdJbV+qbFD74l8qVlH8k7F4e+n/XAYyAGwztobHGN5S2W+C1JcplYscpicTJJ7rqheA2zdZP4P6J8Ohxzu8XLFGPw44AYbusQ0Wh1JZAao56njlYuTj+OlDFbHcELeB2OsMCL/gMEDDscP/2/KOUL3rPCv/2Zjdgi/SJS1Y30r0rlVtp1An+2+eNVZEztVYst/2mHIjT3EXOAeqxtiQ4qif/KJVSv/Pr0MdsJ71ewCsqKnjwwQfbvN8pp5xCRUVFm/Z58sknvdmZmtLSUvLy8nC5VJnK008/ncMOO8xvm/nz53PPPfeE9Rm/+MUv6Nu3LxMnTmxT3wxtxG6Bp+oibZbFF2hxV+wIjrUOjAt2u4JjqJubfM+1G8YefwxKoF+6CL55FNL6KyGJswnI+3+APSvh6bPgxQt9iwzUlvj6nZzt237rx45fNyykVN8rSMBTOh6FUvA5DJjs31dQ4giWwNvQPvfj5sPP31PzAhU7fO6WcT/23z5rKOSN870O/Bw7CQ4Cbv/OgXHjQw+F0/4F/R2SiTqIEfAQAt7c3HLm2LvvvktWVlabPuuss85i4cKF1NX5LLQFCxZw2mmnkZiYSEVFBcuXL6eiooJt27a16diaiy++mPfff7/1DQ0do7HaFx8cYwVz6eiTQAGv3utvpUOwBa4n2OzYq9vZLWn7Si9f/hPWvQ7Fa5U7QQjlupj+C982D8+ErR+p8qgrnlFttWXKBw6QbEticUoGslNbGuyC0GxeqG4Mg2f4t8clddyFUroZ+k8Kbk/LUzeusq3+7VrAEzOU1Z41VP1ftn0GuWNgylyYMQ+uWgEHX6qEXkfOAMQlhO6LkwVuF/VACzwhFaZdqPrayfSsMML3buj84jT9J8HJd4Z8+4YbbmDr1q1MnTqV+Ph40tLSGDBgACtWrGDdunWcccYZ7Ny5k4aGBq6++mrmzZsHqNrcy5Yto6amhpNPPpkjjzySL7/8kkGDBvHGG2+QnBycHJGRkcHMmTN56623vIs8vPDCC/zpT38C4JVXXuHHP/4x/fr144UXXuDGG29s89edOXMmBQUFbd7P0EZcNheKFqxZv1e1TvYFiIm2drV/PCYuOC5YC3jOKCjb7GvTvli7L7uuDDKsIf76N33t2hpN6QOn3qdivfVnTvoJFK1RLpKB09QNSEfP2K3NuFZq1L98sbJ2/7DdF3uty8cu/qsKoZsyx3+f+OSOuVCa3cq1kT7Q+f3UvOCoHu3P1zfZrKHK7bX1Y5g6V8V2n3K3eu9H1uhW+6j7T1aPsQkw3CF71u4Dj01U1rxd1Fu7CXYivd4Cv/POOxk5ciQrVqzg7rvv5ptvvuH2229n3TpVrObxxx/nu+++Y9myZTzwwAOUlQWHf23evJkrrriCtWvXkpWVxSuvvBLy83S5WYDdu3ezadMmjjnmGEDVTJk7dy5z585t96o9hi6iscZ30eYeAPMrYepPlRiWbPDftq4UUnN9rzMGOVjgVoTEuU/AvE/928A/HPDesb648irbNoETfFetgJsr4Pfb4MyHoc8IlQzz1b/UkH+q5c6zp5HHtmB5gi/jdIet1MSrl8Kt2SpxZ9bvgxOCOmqB15aoCdrAUYwmNSc4qkdb4FpM9ehINkOaQ2ITwNBD4IadcKnlRvpzCVzgsHShXaz/XAznPNayBR5BepYF3oKl3FXMmDGD4cN9AfgPPPAAr72mJnZ27tzJ5s2bycnJ8dtn+PDhTJ06FYCDDjqoRQv41FNP5fLLL6eqqoqXXnqJc845h9jYWIqKitiyZQtHHnkkQgji4uJYs2aN8WV3hPLtsOwxmD0ffvhE+ZEPurjjx3U3qrjuwGgEIZQftThAwGtLYYDN/5k5RFmMi+bDgReqEDRtgacP9CWo+CWJBExGFn6rLGhXpapNPfSw4JVidKq4FujUPNjxtQp1HD7L57e1W+CtxWtnDFLRHNs+gzEnq/C41bas4clzgveJT1Y+8Mpdav3IIYfAYZcrq3zRzSrD0X4TCUTfyEJNLKbmqf+1He1z1wJuF/+W/Ns6U7MlnEYpibbfQhcKeK+3wANJTfXVPVi8eDGLFi3iq6++YuXKlRx44IGOdcETE30TVLGxsbjdTiFNiuTkZE466SRee+01vxV7XnzxRcrLyxk+fDj5+fkUFBT4ra/ZqylaqzLv7BXt3C5VRCrQ92nnlV+qetdFa+CZs1T1uc5A175w8oX2HasmGnWtEymVgKfYLPDMQVC8Dj6/TyXtANTshZh4JWR6hRd7mrarWiWFaOJTfaKfOwZGn9h6v1Nz1Whg31boa5+ws4lna6GEeuSg3TyL71Q3slEnwvkvq/DBQLSAb/1Y+es/sFyDa15RCVGf3K5uBI6hgNhubqEs8LzgsEyvC8UScPsIKLmFm0U4CAGH/Bp+arPOk7J8z40LpetIT08PubBCZWUl2dnZpKSksGHDBr7+unPCgObOncu9995LUVERhx56KKDcJ++//z4FBQUUFBTw3XffGQHX/PdYeO4nyorWrH0dXpgLzztYfBqd/ehpCr1Ne9D+aKdohL7jVTSJtzZ0pfr81DyfdZpqm8zS1pqe6BTCJzr2cMSGKuUC8b6uaF3YArF/rs7AtPcBWrfA9WfW7VOW/Kb34Mir4acvwegTnPeJS1Ix3PaJWil9/5e9a+CRWc5FoKB1CzwlR/n07X72QBeK/bu3ZIGHy8l3wihbHXX7CKKnWeBCiKuFEGuEEGuFENdYbX2EEAuFEJutx044K11PTk4ORxxxBBMnTuT666/3e++kk07C7XYzefJk/vznP3vFtqOccMIJ7N69m/POOw8hBAUFBezYscPv+MOHDycjI8NbIOu2227zqxkeirlz53LYYYexceNGBg8ezGOPPdYpfe423I2+Ohr26A7t83Rah1CjfbH2qI2OVAHc9hncNQL+YU1yOVngWhjvHQfr3vRZhqm5cMaDcOMuJTgaLb7Ve3zPtejYxdRV7Z9cYr9JtBSzbMduhdotcHviT0sWuKvGV7yrvlxlIiZnw4zLWv5cPYlp9+nba3rrkMptn8K942GlzXCpLPQlN6WGiOLQ7fZ0dX3z05OY9oqCLblr2kuMfXTUdRZ4qz5wIcRE4FJgBtAIvC+EeMdq+0hKeacQ4gbgBuAPkexspNALKwSSmJjIe++95/ie9nPn5uayZo1vVY/f/a71FOi4uDhKSnyTLvn5+ezatStou+XLVVr2IYccwvz581s9LrD/TX7aBbra9lxbwS3psQ7va7T5kjtSBXDNAv9oB6eMPLswfnwbnPaAep6aqy7yxDSVRPKRtdyYdgtV71Wp+aC2i00MEPAqJeA/eRpeulDdwJb+R70XrgVud+PkjvZ/74JXVTy5/VwFov8XiRnKDbNvK8y+uXW/cbxVC8Vugb/3B18fdLx7xU4l7K9dpiJZ9qz0pfhPu8jZPQO+G1Ntie8mF2iB2+kMC7wlepgFPg74WkpZJ6V0A5+i1sA8HXjK2uYp4IyI9NDQu7Ff9E5RGY01oa1qLeD2BJlQqeXhEOijdbpQ7VZunxE+0bNbj/0nwqFXWP2xvkf1Hn9LOiHF34XiqlYTZeNPU1EUOuY6fWBw4kwo7H2wW4wAB8xWFf1assDLrdwEHWYHMOPS1j83Llm5S6oK8SY7rXpRRcOAzwK3W+Xbv1Lx698/3frnaP+z/f8c6AO3E3EB71k+8DXATCFEjhAiBTgFGAL008uoWY99I9fN6OOKK65g6tSpfn9PPPFEpxy7rKws6NhTp051DHGMeryiLfzFXAufbA6d5acF3Gm/9lCxXUVQHGdZz6GEQAtqU61v1BAYw3zSX2HIoao/jXVKfOyWdHyqvwXeUOU7blKWL9a8tZWA7OjjT7/E+f2E1JYFvNjKAh1mZQrHp/hHX4RCL/q7r8A/GUdPiDqt4hO4Kk5iC1a+jgqxu8r0SMJplBQpAdclDHpSGKGUcr0Q4m/AQqAGWAmEDrMIQAgxD5gHMHSoQ51k9RmIcH+EUcK///3viB07JyenQ/XIZUf8wG9epULWdAxxpNHiO/DAAB+qTYhdVc6WlhZwu++8IxZ4xXYlukdcrZI3AsP2NFetgJd+pvpevUf1IyUneLukDNj8ISywsiaDLHBLTJub1M1Ai6XX4hYtV80LJKUPXPW9SrZxolUB3wCpfX11P8IRb/BZpK5KJeD2hY+dOOUetaJ9o+1/1dJn6fUq7TXHXdVK2J3cLp2xcIUT8amqzz3MhYKU8jEp5TQp5UxgH7AZKBJCDACwHotD7PuIlHK6lHJ6Xl7wJERSUhJlZWUdExVD2EgpKSsrIymplYw7J2pLValTezRIOOxY6lxmtTUa61R2H0JVe/OzpKudn9vRFpG9QqCTtRcOzW4Vx5w1VFm8ocQblFD2Ha/WglzxnEr1jnG41LQo6WW//CxwWw3tPZbgaR+5FvCMgS2nfDvRZ0Sw+0TTqoCvUz5+bcGGK+D2czV8Zuvbjzkl2EoOS8BdvjZ7sTHNvE/hjIda//z2oidKW0uG6kTCSuQRQvSVUhYLIYYCZwGHAcOBi4A7rcc32tOBwYMHU1hY6DepZ4gsSUlJLUayhEQXAmpr2dHHrfCy+a2Ip5T+k4xrX1PD7AFTlCB+/zQUfgeDD/IX7VAV93T2nz3JI5TYS2llV4YQipoi5a4Jd9Ua7Quv2atS150I/KwM2/8kPtXnA9eVBnWqvBbwUJZ0e2lJwKVUK99MPs/X73AFfMghvucjj1UJSE4VGjUpfdToxP5zaclq1gLebBNwV03wZPXAqeovUpzwF5WV6jTaihDhZmK+IoTIAZqAK6SU5UKIO4GXhBCXADuAc9vTgfj4eL/MR0MPRld8aylSoSN8cb/KTrz+B5UeXfCZuhguXaxcCJ/eCd/+1xLwKjWZV1MU2q+tfeN2sQi17XdPwNvXqiW1sh2EUYctpoY51WOfzAy1InmgpdbHdh0kpPjqpez8RkVspFmfrR/tceGdQWKGuuk11gW7pBoq1LnLzvdZx/qG0hr2EL60vnD1Snj3evjmEeft45PDvzmAzQduc6E01jj7vyPJ5J+ovy4kLAGXUgb9p6SUZcDsTu+RIbJ89aCyZsKJHgikPRa43TXmdvmsJSfWWDVkygssAf9c1feIiVEXdN/xKtRs6cPKPTFwWngCbq/D4apWbpTXL4fjb/UN71dbWXXl25wFXMcYh4pFDmTqBcpSXvCL0Ocr0J1jtzLjU6CpUD2vLVEp7JqjfqciRsIV0HDRK8kXfgMjjvZ/T49isoYqN8oli3zbh8O1a/3L4866QZVZrdwFC/8cvH1Lk5aB6Buh3YXiqu78NUd7ID2rFoohsng8vjTmrKEtp19vWQRDD1eWWPl2Vc5TF2lqTcA9zSpMzFXtv9hsZWHLvmNtMVUVgmu0ql1ysC1iIjVX9eO936vXGQOtlU5CuEWcsgpd1aoc6oa31eTiaQ8oH70Wl1BV8+wJOeEQlwATz1ZrLB5wvPM2LS10G28LI2yo9Bfw1Bx17M5m6KEqXX/bZ8ECrieCdVGoIQe37diBK9zo7xBqqbF2WeABPvCUMP9XUYwR8N7E7u99z5/7CfxmubOglm6BZ85WNZNPuduXeQiQPbx1Ad/1Hbz+a/XcXvinYnvw5zVUqYtVCN9FW7HDJhg2azg1DwpsC9jqIXNIAbdZ3ik5ajRQsdNnsa17Xf2Brz6G01qV9vZwBVyjVzJ3YvJ5KgoFVEEnOwkpvlooTqvcRILEdOWvXrNA1RS3jwj0Yr9Oo5OOYE8o6jvBlxTUWllbO44WeI3/YtP7Kb2+FkqvobYUHj3Wvy1UGrp2F6x8Hu4IsJz6Twq/4NGMy/z9knvX+G/3xQNw5xDfEFp61GP5dmcBT8n19yenWVEboSJL7BZ4Sq4Kf9z+uRpJpOT6D9P1cUOtHl5bqoSiLUP71ph0jprYnV8ZLPSBFnhXCDjAEVcpF9bGd31te1fDJ7ep728v2tQZ2H3tv/4CfmEtRtKWqOKYGNW3IB/4/u9CMQK+v/PVv2HbErXoLfhHROi46i//pdwImpbqi2QPa13AtUU87UJfW1yySv22+0GLVc11NryjHrXw/7DYV6Qqy5Y7YPc/H3ypSoZJ7gM1jhGsygLXseCxCWrFmvICtYDvwKnw05fh5Lv9rT2nxYZ1e0pu+EkzHSUhVfnum+qVMOnFEyKN9qvr3wv4Kj4ed0tkvv9lS9Ro0H5s0UZpikvyFS8D/yXv9mOMgO/PNDep+stP/dhnWZ78NzjAWhhXx1V/eJMv1M/e7kRCuhIWe2nXQLTA22N5Z/5W1ZG2R4Rowd73Ayx/WpV9BV+pUvB3WdifH3yJskrTBzj3t9mtLmhdXKqpFsadpqzw7HzlHhp6KBwyz38FmVAulMBFGSKNth4rrRo5XWWBJ6Sqm619JKJvyONOjcxnDpji4MqzxDwxE+aEUd/HboHrkFBjgRuiGr16CviEKb2/qmMcl6QscI8neD97xmMgOiSsJSvcm8acBof8Sj3XEQv2Y9ft82UfvnmlugBHn+x7P3OIv1VmF1Btmaf3c+6vjjzRAt5Yp2K4f/G+svgmnePb9ghbnfBQFnhNcfgRKJ2BFmydhNTZrotQCBFcX1tH+XSlRast8OP+D8ae0vr2cUk+H3hjLSB7RRSKEfDuoG4f/HVQx1YADwdduwJ8F6R2A6T3V5arU6SG3aK1p3en5PoEvKW60S6bgJ94B9yww+fLth+7vlxZwdeuVWF3oLINf78NbiqCK5b6H1cLaL9Jvn6EssC1/1gLeEsJOH1GqD6OOiHYAq8pgVtzVLRLuGVbOwOvgO/wf90VBC5Rpi3wzvT/t4Ze9iwxzO8dl+AT8FrLpdYLXCgmCqU72LtKWakf36Yy0yKFfW3Giu0qu09PGmnhc7Kk7RbtWY+oDLzUPOg3US3lBWq/sq2w9lU48rf+qeKN1WoYrutQJGX6LCr7sevLlZslczDkH6FWTC/fFrpec95Y+NG9MPEsX1t6f+Wz93j8+6BFJ2sonP0YDDsi9HnSfUzJ9b/pAexaBh6377O6Cu3z7hYBz/OfV3BVq99OqBT8SHDUdWp0FW64pF40AmDpIyocMjAUcj/ECHh3oC+Oyl3KGo9Egflmt4rF1uxa7u+CSOunfM5OWZXVe5WYZQ2BwQf716/QE4+NNfDqZWr5sJwDYMKZ6vuk5jqnMSemK4u8cJmylGITLAG3vvvYH6nPOvK60N9JCP+4cFA3ItmsfLaNteq4Vbvw+lCTs8JbbgzUzSRwtXh7KGJXCnh3WuApuVC0zve6obLrrdm4RJj+i7Zt73Yp//eaBTD+9M7PVO2BGBdKd6Cz2mr2qqWkIsHql5QPXAvi3lX+Aq4tcEcXyh5l5c5bHFxZTbsuitYp8Qa1LmKzGx46DJbcE3oCye2C9W+qbVxVSnj1RGdSJvxyEQwKUTckFNqtsfIFeGAq3DceHjterTepjxsuydlqstMeT2yPyOkOF4qe9O0qHzio30ltiS+L1lUd3mK/3UlsoqqFUrJB9T2SI9sehBHw7sBeHa9ih//q4x1l57fw2Anw4Z/U7P6sP/jC5OyZaen9ldBWB4QMNtUH16a2o4VZp9UfdqW6aFY8q/bbssjZAgffGogV232WbkdrM+tEkMAKiRut0MS2CHiK1Zf6Cl+b3eXTpQKepR5LNir3U1dGwKTmKTHUbihXVc/3J2sLXP8uh3dymYEeihHw7qBihyqKpGNuv34QtraxRGsonjoVdi5VS39N/4Uqph9r1R8ZeYxvOy1GemEAjXeh3BBilTdWuT1WWqFds36vshz16z0roHq3cyGhC19Tj4kZvhDCjrqP+oxQbhN7lunBv/Q9b6sFDiqpx9MMy57wryWe3q9jfW0LesKwsVq5u7rS/5xhLT5RZYUwuqq7dgKzPcQlKh946SY18dkLsjDBCHj3ULUbhh0O57+kxOeT2+HpMzq24C4oq9aejaZ91yferiYgD7rY9562sMu2+No8ntZXOk9MU9l6slmF+SVlqoJTO75S70uPWsvQyQIfeazap6nOt3xWR10DsXHKCtc3hD4j/X32bTm+9sfXl6v5g7evUSVtQQlYWhcKeGyc7ybYlZ8LvoghffPSy7n1ZOIS1Yo85dsh23nhmP0RI+DdQb01cZmQ4j9Ro+tNtJfAGOZsqzzptAtVmrLdn60tbLuAN9WFt9L5wZcqd4xeHmvYkcHbhEqiSEhVrhtdw7szfKs6VDA2AX7znf/yZW1ZHUVb4HX7/N1a40+HG3dGbiWXUOjRQ1e6bsAXY6/nahqqosACt6JQKnZ0fp30HowR8K7G4/GPvjj+L3CGtbq4rrfdXnTs7pkPB6cmB+JkgTfWqIqB9vedSEyDSz5UIX0AebaCRFpsQiVR6EUDvLHFnWDZ6TrauiiWve9tSf1OsVngdrHuagHVeAW8C6NfQNXsjkvyzdVEyySm26X6nGUscEOkcFUpN4O29uISVCp3ap7yH3cELeD9JrRcthWU2CVn+/t4GyrVqjd541p3PeSMhAxL2OwXzOybYfRJMPEc5/3irbUevdl9nSAM+vN1yF97XQ76f/LNw77Yb+h6AdXo7FVd5KurEEKd04odqrxuY7XP4OipxCWqiKGmOmOBByKEuFYIsVYIsUYI8bwQIkkIMV8IsUsIscL6CyPf1eA4eSeEcgMUb3DeJ1zasuiAEGqBBDsFn6lJoCOvaZvlal8KbNK5cP6LMCJEeGRCWudb4PqC1SGRbV0n0t63+FRVfW/9m7727rLAj7JCQEef1PJ2kSBvrKqbrssxZHTTOQiXPiPUvAxCRV/1EloVcCHEIOAqYLqUciIQC+jqP/dJKadaf++GPIjBR32I8Lm+41Q43u4VMD9TLaPVVrzp8mGuyad9x5rtX6rHwW0s1m8XTKdVwO3YXSgJaZ0TXdFZQ2Yh4Lq16vm2Jb727rLAc0bC/5WHVwuksznsSmUQfPmAet1d5yBcDr8S/lAANxbCsMO6uzddRriZmHFAshCiCUgBdgP5kerUfo22wAOHpHljlQ96qeUP3/geDJnhe3/dG0r0DrwAPvu7ig0WMcGFmJKywp9s6zvO9kKoJc3ikiIbgqUFvDOz+wJXewH49Ze+8Mm2kJytFjXYaavD0l0WODivZt8V6Jv4ruXqsTvPQbh0NKcgCmlVwKWUu4QQ96AWLq4HPpRSfiiEOBy4UgjxM2AZ8FspZdAaUUKIecA8gKFDe8/kgh/F632z5HVawAMtcMudoS3vQBfGSz9Tj5N+Ah/d6mv3E/CStlXMO2A2DDlULY/1w2LlOsgd3T6r+Iz/qMzS1rBb4J0l4LHxcPhvIN8WPthvQvuPN+igAAHv4dZnJIiJUSMkXdo3GgS8F9KqgAshsoHTgeFABfCyEOIC4CHgL4C0Hv8OBBUvkFI+AjwCMH369A4GOkchUsKDh/peH/U79RiYwNLXcmfoxBpdBzoQnb6uaW7yWdzVe3wrlodDnxFwyQfq+Ue3KgEfdFD4+9uZOje87RJSVbq6q5ND0064rfOOZZ8EyxjU80PoIkVCmhoVxib0Sus2GghnfHYcsE1KWSKlbAJeBQ6XUhZJKZullB7gv8CMFo/SWwksjvTZPYAIjvJIzvZPdV/1glVXpA7+anMRPDzTfz89GSil8qHb1xhsC0f/Ea5cptbAjCQJqSqqoqak5yaH6HUfY+Lgym+7bhWenoYOBU3r33vPQQ8nHAHfARwqhEgRQghgNrBeCGEfU50JrHHcuzdTtw/esVXXS81TxaXOeMh5si9wPcaP/6KKGTWGWLQXfOF41XuVX9nPr90GYuNULe5IJ6vEW8Wwqnf33NhiPSnqcfuKd/VGdDJWapiT4oYuJxwf+FIhxAJgOeAGvke5RB4VQkxFuVAKgMsi180eSGWhqmfSUsjaR7f4Vj0HOOQymHl96O1P+6eqiTL+dHj5Ihgw1b8aXnxKcPVAndGoXSvtFfCuQgtiXVnPtcB7USJIi+j/T0+PAe/FhDXFLaW8WUo5Vko5UUp5oZTSZT1OklJOllKeJqVsYR2u/QxPM9w3AV66sOXtAqsM2hcUdmLaz+DcJ2DCGaqQvavaf7WZ3yz3+Wd/9ob1GZZ1rsPeAmO7I0xRVQNFVQ2tb6ix++jTIjs52NTsYe3uSjweyerCECvXO6GFa6h/ONra3ZU0urs4qaY70RZ4JOrVGzoFs6BDe9Ciuen9lrezZ9Bd/nXbrOPkbBVyqGuT/PorlUzx6y9UxmHlTl9f3rsBlj6kFlXoyrKjwCF//QghYNsdPwpvh5Gz4ZcfqYic1m5oHeTPr6/hhW93cv2JY7j7g40suf4YhuakhLfzbzf5jRC2l9Xyowc+5+LD85l/WgciXKIJPVoyE5g9FpNK3x5cNp+0q0Zlq61/O3g7XVcE2u7aSM5WFfuqdqvymP0syzoxXVmxeq1AVxVssD77uFva9hkdpLhaWd5tKqIYEwODp6sKhglhimk7eeFbdZN78ssCAEpqXC1sHUB6P7/+Ld6oyhS8vyaMUMn9BV0IzLhQeixGwNuKlEpUNaUb4YED4cWf+hYCqC5SlQXLt6nXuu53W0juoyz4zR86xyFb1uED736Hu3IP8ohrfNETXUBdo5tj7l7cZZ/XVuoafbVMSqqVcFc3qAUlbnx1NS9Z4h4un21WAr63qoE/vb66k3rZw/E0q0djgfdYjIC3laUPw+Mn+F7bV2/52zBYNF/5x/89QyXWzL4ZLnawzltDXzQV29XalIFYAp5Us5M43FTHd63rpLC8ntpGdYHHxwpkR2uZdzK7yuuD2mpcStTfWbWbd9e0bcpmW2ktU4ZkkZUSz0vfFvrdIPZb9ApKPTVayGAEvM3oAv+aZ87yf/35feqHf9Tv4NynYMal7fsc+8TRqfcFvx+fjJtYRgo1Gtgrs9r3Oe2kukEJ2OEjc2hqltQ3NXfp57eGtrrtVNQ18eryQqoa3Gwvc1gLtJXjTR2cyb/mTqOx2cPXP5R1Vld7Ls2N6jG29eJgjW4Pb67c3eEbeXVDE49+9gNfbintlONpPly7l/99VYC7ef+ahDaTmG0l3DTzyT+BvDHt/xwdAdBnhGNYW2OzpF4mMTtWLSW2ozGTdqbwtAvtjhicrfykFXVNpCT0nJ9TsSXgRx6QS1ysYPHGEh74aLO3fee+OtzNHuJiW7dhGpqaqWpwk5eeyMHDs0mOj2XxxhKOHdvFK+V0Nc3WKCMMAb934Sb+8+lW0pPiOGZMG7KBA/hwbRG3vePLNhbAj6cMDL1DGEgpmff0dwAckJfG4Qd07Wg1khgLvK04CfiwI4LbMh3cHm2hzwj1OPv/gt5q9ki+3FrKFun7Ya+pjuyEYCDaAh+SrT63vK6xSz+/NbQF/uAF03j8ooMRwifqAG6PZFdFsJulpWP1TU8iMS6Ww0fmsHhjSY9zG3U6U89Xj7oueQus2aXCNJubO3ZOKuub/F4XO4yk2kpDk8/q/qG0tsPHayvuZg81LndEfi9GwFti72pV2lWXWQUQDgKuV4Sx09EIi4wBcHOFCg0M4KT7l3DxE9/y0+abkZZ19NCyapZsKunYZ7YB7U8e0kd9z8q6ppY273JKalwkxceQnhhHTIwgLTF4dLAtzItZR6/kpavqhrPG5LFjXx2FDn72/Ypxp8L8Suc5mADKatUNvKmDLgptGGg8no6Lnh4tggoH7WrW7q5i4s0f8PGG4k4/thHwlthoxXnb4709DpNX2lrubBzqT1TUNbK5uIbjx/fjoQsPQVy3gRVHP4GLBDYVtZBy38noi2JIH8uFUt+zBLy4qoG89ESEdQ7THQQ8XD+4tsC1gA/PVfHRe9uSwLSfU2bd5KpdHZvcrXH5/47cnSHgtj4VtHHuozPQo9OslM4vU2EEvCW8y37ZUr51PW87dgE/4TaVEh8h9A/w3IMGc8zYvpCaw+SZZxIjgoefkaCp2cP9izaxt9KFEDAwSwn4O6v38O7qrk/GfeKLbWwprqGhqZl7F27ima+38+K3O3h9xW7y0nz1wNOTgi+ecC3wYq8LRR0vK1mNeiocRh17Kut5+NOtrQ6XP1pfxMcbikK+X1Bay+Ofbwurf4HUuNzct3ATjW6VifqHBav4cmtp6zt2AG2BB1rQbSVw/6e+LGDFzopOOWZcjGDhuiK+2+5wDUcQfV1mpbRzpagW6DmzTj0RLeAe27CwNQE//DcR7VKBJTraCgSIiRFkJsd3iR964boi7l+kakSnJ8WRm5ZIcnws76zawzur9lBwZ5gZmZ3A9rJabnlrHZMGZXLZrBE88NFmv/dnj/NNMgZGyQzKSg57OF1Rqy2oBOtR3QyczvfvF6zis82lHDUqj/EDQ4ffXfLUMgC23XGKd5Rg5+InvqGgrI7Tpg4kN61tC1Pc88FGnvyygBF5qSzdto8Xl+2kvK6Rw0dGZvJuR1kdzZalXNPJAr63qoG/f7iRpy85pAPHVAJ69rTBvLhsJ//7qoCDhnVdbLu+0WclGws8cjTVwz+mwF0jYcO7cNcIn++7vADuHQ/r3gwuDwuQ7eADjxAFZbUI4fM9e7uQkuBoEXY28baojYykeOJjYxiR57uZNDQ1h/SH1zc2e33nbaGqoQmXOzhMUWdHJsTFkBTnPzdxQN80rjjmAO/rHfv8h86TBmWGPZyubWwmITaGhDj13TMtAQ/8njUuN7XW9yuqaqChqdnP/6qx+3W3ltQEvQ+wu0K5Z8K5yRRXN1DrclPd0MTGvdWstiYUY2OE94Yfqd+GxyN57PMfvK8LymrZWlJDVUMTDe0ILXVywSz9YZ9f3P2+2sY2+cb1TeXiI/I5aFi2Y4hpKEprXG2efJRS+n2GPveZRsAjyM5vlFDXlcILc1W1vNJN6r1dy6Bqlype5a6H8WfAz9/z7duFVfUKSmsZmJlMUry/YGWmxHeJC8UupHpi0G7pXvDoUqbc+qHjBXbcvZ8y8eYP2vyZk+d/yEWPB68Rqt0CKQmxQRb20IAb3Ki+KixzdD/1OLJvKjv31YWVkFPX6CY10Xe+0xPjiI0RVNT7W+ATb/6A5TsqAHXDOPH+JUy+5cOg49kjK77Y0nI8eUFpyzeZbaW1zLj9Iybc/AGT5n/Iifcv8boI6hqbvX7+wL52Fu+u2cNTX21nUFYyfdMTee37Xcz++6dMnv8hf3q97RWmqxuagqZ+7HH3ZTUupv1lIQ98vNlh71DHVP/jtMQ48tISwxbwjXurmX7bIl5eVtj6xjbeXrWHw+74yFvkrbyukfTEuLBCVtuKEXAAdyN8+U8VYTLFYWWZfT/4vx4xC4YdDiffDRPP6dJi9wVldQxzKMiUlRzfJRa4fYibnqQE/OrZo/jlkWoUsswSj3V7qoL21WF7bbFo9ND86x+CRz5bS5R1WVLt8gr4KZNU2YGEgIvlpcsO44sbjuWVXx/OZ78/hkNH5OD2yLAScmpdzX4x7kKIoPO9t9J/QnNzcTXby+oc68TYfe8/OFjgHo+k2dqxoBUL/NONoSMbiqsa2F2pznmkfhvrrf/z05fMwBPwZT9cu7fNiTM1DW4GZib7tcUI32hLn4/Xvw+xYpUD2qrPSIonLz0x7Jo4G/aq77Zofei5CifeX7MXt0ey0xr1VdY3eUdtnc1+5QO/4931uKxyn22qGPfp32DLQrU+5Oyb1QLCuu72wGmwe7n/9qlWosIh89SfxsGV8ubK3ezcV+c3nO8IBWW1nDIpeH3CrJQENhc7D8c7kxoHAY+NEcwak8ejtkm3TzeVMHFQpuMx9tU24pFwy1tr+cNJY7nz/Q3kpiYwaXAW5xw0OGhbJ5o9kh2WdVlc7aLeSus/dEQO767eS3KC/wglOzUB7fVMty7k5PhY/vHRFv7z6Q80eyR9UhP41/kHkhjgjgm0wEGNeOyiGBjC+epyn8Bc+r9l3HX2ZLJTE2hoambe08r/nZkc73XjfLh2L+v3VDM9P5sXv93pvXFtK63F45H87uWVVNY38c/zD/S7mXzaQujoPR+qEeSAzCTKahuRUiKE4NXlhVTWN/HzI0K7/p5duh13s+Siw/MBWLSuiBU7K/jdif7JaQWldeTnpDAiL43SGv//VVWDm18/u5yq+iZu/vGEoDmByvom/vz6Gn57wmj+sWgzVx57ANUNbgZnJ/vF6M8ancery3exdneVd3QhgeteXMG+ukYemHsgGQ6T1KD88395ex0AqYmx5KUnUlGnXHL2//PLy3ayelclZbWNZCTFcdy4fhRXubyf1RpNzR5ueGU1vz56pLdujh5pVdQ1kh2BCUzYjwR8b2UDDy/xWcoXH55Pvm2iLyS1ZfD1QzB4Bpz3DKTlwZXfMvWOz0nAzdfZHxETJOAOk0HXb1ULFwdw1fMqU7IzBLyirpGKuibynSzwlPguicXWPt2jRuVy1jSf2PZN9333xLgYtgTcTOwulYKyOv73VQFvr9rD7op6r9sBtgcJuK54GMjeqgYamz30y0ikqMrl7depkweyfk8VV89uOS81KT6Ww0bm8PGGYpLiY8jPSeW77eXs3FfPAZa7RVPb2ByUZZqdkuDnlli727/e+LSh2Xy+Rbl4Fq4r4uG8H7jh5LFsK62lusHN0D4pTByUwdrdysrTmYKBFFe7KKpu4FXL4vyhpNZ7Y2xoauarH8o4bESOdx7i/EOG8tbKPfzn063eY5wyaQCPfb6N+ib1Pa57aSVASAH3eCQ3vabcH1rAf/k/ddP57Qmj/SZdC8pqW7zOFq5T1utH64uCBPzBxVt4c+VuVuysYMe+OuJjY6huaGJAVh+whPqGk8cyZXAWbs8Wvxvm9rI6r3voo/VFnHmg/+9G85DtPMTFxngjiUprGhlkRVBJKbl+wSq//Z7/ZifnH6IyoCvCCA5YsbOCV5YX8tr3heifunbVVNQ3RSSEEMJ0oQghrhVCrBVCrBFCPC+ESBJC9BFCLBRCbLYeu61k2fo9VbwTEMLWkmVip+rjvyOb6ig+9h4Km9QPsTyuLxWkU0w2BbUOd07byu8ej2ThuiJWlcf51hC0eGOFzworqXbx1srdIQVJs6momnLL6lxWsM9vCKqttWE5wRdMVnIC1S43W4ojGwte7XKTnhTH05cc4pfirGOkJw7KYPzAjCA/4z7bRVBQWsvyHeoC9Ym3YsXOClYXVnonwEL5K/Xk3PR8VTNGJ9VkJcdzx1mT6Z8ZfDMNZNZo9X8ckZvG709SlmWtwyRarSvYAs9KjueLLWWU1zaypbjG6zoCmDk6j2d+eQhPXHywt+3lZTsprXF5RejOsycxPDeVwvJ63lq5GycmDMygsq7Jz+VSZZsUXbptHw1NHubNGsHtZ07i9jMnMWFgJjecPJbYGCWyd50z2ev/f/brHd5hvRPfFuxjc1E1j3/hG0kVltfx6nKfD7ja5WZ7WS3F1Q1IKSkorSXf4fcYyDYHV9CyAnXO9ARz/8wkahubGWj73/1q1kgOG5nD05ccwv9+4bzs7n0LN3uvmUAC3c76d/rhWl9Z4FDhpDq79NuCcgrL/c/bpqJqvzknfUvzSDUijRFKg6oamqioa4rIBCaEIeBCiEHAVcB0KeVEIBaYA9wAfCSlHAV8ZL3uclzuZk7+x2feYZJm6bYwig1JScOyZ3m/eTq//aSBef9TVtCKwgrvJq9tdpjkslngn2ws5tL/LeO0f33hZ2XWNbq5+oUV3tc3v7mG3zz/Pbe85d/PwO9ywn1LmPPI13yzbR/n/OcrHlzssyA2WP7GkXlpQfsOzVHWxK+eWR70XmdS3eB2TIrJSo4nNy2RUyYNcJwosr9et6eKnfucsxjP+PcX/Phfn3PFs8uD9rOjL6iDhiq7YXtZHUnxMcTEhD8foWt2XHDoMK+FXeswqVnrcgdZ4DoK6P5Fmzju3k+9ljQony34xAJUnPS9CzdRaVntWckJTBiYSbNH8htrlBbIuAEZVNQ3+iUc2V1YizcWkxgXw2Ejgtes1C6Y/JxUr/V3+7vr+dUzPkvfPhfx2eYSzv3PVxx/3xK/WiSXPf2d12IH9f+YdfdijvrbJ96KlCMt6//oMeqGeNa0QQBcccxIQEX8OCVNbQ5IPCur9U+Y+sl0f6s6OzXBTwgnDcpk7owh7NhXx4OLtwQdH/D+zlIsl5oeLdzy1jqvAC/d5hBZBqyyreJkn5CVUnLCfUs47+GvvG32dP1JgzJJio/l4w3F/PLJZeytbPD7LXQm4U5ixgHJQog4IAXYDZwOPGW9/xRwRqf3LgzsFsXhI3NYefMJHDu2Lz+UtB5+5SnZRF9RwWLPVHaV17NuTxV7KuvZZ/PlbZYOQ7NE31DQnh5bZLOutf9symA13F25U/0YlmwqCTmx8+02ZZFsLKr2+tH0MBzUHb1/RpL3grFzxtRBXHTYMLYU17CnMnIp3tUNTY5JMTExgk+vP5rLZo50nCjSQpwQF+M3MgG4bNYI/n7uFL+2j6zzaj+OfTmzcsuSHTtARQDt2FdHcnyYhcYshuaksPLmE5g7YwiplkDXuYJD3+oam0kN8Kn/6UfjyEtP9Bv5ZVtCqcXTftHmpCawp6Le2+/s1HhOntifz35/jLcgWCA5qSo01D6RaZ9E/nRjCYeOyAmKSLKTn5tCZrJvFGm/0ehywOAfDXPd8aN58KfTvNsfeUCu9/+zxwpvdLk9LLZGubo41KM/m86Gv5zE3edMYd2tJ/Lb48ewev4JTByU4R0x2QkMVNKjqMzkeNbfehJ3nDU5aJ+lf5zNxttO4rPfH8ML8w7l1tMnEh8rQo/Uymo5cUI/lv/5eEAZP/+cq2q7fGldW9tKa0mIi2HlzSew4S/q2JqrZ49iwsAM7/cG5dsH2LDXdwOyh4sOzk6mzjq33xTso76p2fEm2xm0KuBSyl3APajV6fcAlVLKD4F+eh1M69GxBJkQYp4QYpkQYllJSefX6rCHWQ3LSSUzOZ5hOSlWBEAL0w91++DR2QB87Rnn/QF8urHELy18sxwUtOuBf1nIgbd+yMy7PuHlZYXeu/u20lpuem01B9++yHthH21ZeXpSprrBzfcBmWVSSs7/79f85nmf9fyQZXmv3aXWYbz7gw28t2Yvs0bnOSZ+CCE4/xC1oMMRd34cckjZUWpcbtKSnKdOUq3wur7pSeyrbeTyZ7/jwFs/5LR/fe69qZx70OCgya7+GUmMGxCc9PJtwT7uen+j97Xd51xR10RCbIw3XHBXRX27qiFmJscjhCDFcpE4WeB1jW5SAkYdcbExHDoix++79LeiJ7SA56Qq4UxJiGXy4ExKbC6UrOQEhBAM6ZPCESESbDJT4nG5PTz86Q/em8P3O8uZ/ffFfLe9nB9Ka71Wbyjy0hKD3D+airpGtpfVcvI/PuPZr7d728+YOojptkSXsf3TmWwZIit2+lxFn20qYXB2MiMsqzYuNoak+FhiYwQpCaoGTXpSPMNyUimrbfQTOSmDSxBrYyw7JYHkhFivG8hOUnwsiXGxDOmTQmpiHPGxMdZIJXj+x93sobBczWnYb3InT+xPelIcSywjqaC0lmF9UshMjicpPtbvhjo8N5WpQ7IoqXFx74cbOegvC3nGdq409vh1J2s7UhUQw3GhZKOs7eHAQCBVCHFBuB8gpXxESjldSjk9L6/lH1t7sFsnOrwuPyeV+qZm57uyx6MScjYvJKaxmvebD2a77Of9ByzeWEJFXSMxAv4xZyq7Y4JLWf54ykBOGN+fHfvqaGz2cLE10bO9rI5Xl++ipNrlDT2aYJu4OW5cP2JjBIsDQr8amjx8ubWM8rom74/H7ZEMzk6mtrGZbwv28dzSHaQkxHLZrNB1V0b3S2PujCF4JBEpnAOWCyWEgGv0D/jd1XtJiIthVWElX/+wDyFg7ozg0rh56YmMG5DOjSeP9Wt/bukOAO+kon2StrK+kcyUeL+LJSm+/VGxXgu8MdgCr3UFW+CAdzJ50iDld772uFGAr35HXGwMfz93Cm//5kj6pidRXOWior5RJR7Z+vqnU8fxh5PG8tczJ3HZrBHcf95U3rjiCG/KPviiqp75egdbS2p5+qsCAMcbH8C7Vx3Fgz+dhhCCiQMz+eMpY/nLGRP9tqmoa+LJLwtYv6eKapebOQcP4bYzJjI0J4W89ESvKyg/N9V7nr8t8Al4YXk9Y/qlOxoUdgZYPm374tcut8d7o9Nod0dbJ/wyQ4TQltY00uyR3nIPmrjYGEb3S/cafwVltX7zSvbvk5+b6jVI3lm9h7LaRv77mS9YQhfvso+M8tITefXyw/n7uVOYN3MEd50z2bGYWmcQzlGPA7ZJKUsAhBCvAocDRUKIAVLKPUKIAUBkFKMV7AKeap0k7ed6ZfkujhqVS7NH8snGYjKT47ko6TNi3lLp7rUxaVzecDV6CiItMY4vtpSSnRpPZnI8p08dxCcbiilZn8GKmAkcl1eJiE/m1tMnIqXkxWVqWa65M4by6OfbWLe7ymtVbC5SURj2mfeRfVOpqs9m8cYSrj9RidU7q/b4pcWfNW2wNyX8/EOGct/CTfz13fWU1zXxjzlTGeHg/9YIIbj9jEksXFfM4k0lnH2Q88y8nc1F1byzeg/J8bGM7p/OkOxkDujrn5i0YW8VH64tYsqQLFYVVrZan9kuqpceNYLb3lnP4o3FDMxMZsLADAZmJlFa00ij9ePPS1NFpy6bNZI73tvg3XfxxmLiYwV/+tE4Ln7iWz5cV4QERvdLp7y2iazkeBLjYsmyQvo6Uo/ca4EHTGI2e5SlmOpwAerJu+PG9eNXs0byjeVLtc+F6P9BXnoiZbWN7KtpJMuy+jXpSfH8+uiRQcffbQulO33qIK5fsMrrRtptxZ33DeFbHT8ww/vbi4kRzJs5klW2uR1QYXxrd/lcKj87LN+7jxCC/NxUfiipZVBWMpnJ8STExrCsYJ/f/qFuIHZ0hJIOqyutafROrNrRv4e2CnhWSkLQ5GxFXSNPWJOx9ggpX58S2Vxcw9IfythUVMPMUc7GZX5OCuusc6zzDuw3i7+9t4G0pDjetE1E56UlMm1oNtOGRj6uIxyTZQdwqBAiRahf3WxgPfAmcJG1zUXAG5HpYsvYJ0dmWf8EnW33t/c3cOo/P+ev767n/kWbueWtdezd6zvRXzaNxmM7BT+eMoBql5vFG0u8dS8OHZHDwa7/8O3B9yEu/wou/RhQP/ArjzmA6cOyGdInhcHZyXxh81fXuNzExQgGZiZ7k0r6pidx+AE5rN1dRX1jM/tqG7niueWc+x9fudqzp/lcNhMGZnLSxAGs3V1FblqCN2qiJWJiBIeM6BN0sYbiocVbuX/RZu54bwM/f+JbfvzPL4K2+ceizdy7cJM3G3JCCzU+QA25UxJiGZ6b6hX78romhuemIoTgJwcPYeZo35Cyb4bvAjt5Yn9mjs4jITaG8romhvRJ8dYCufuDjZxw3xJAuVN0bK0uWtVWH7gdbYHXBvjAdaZmqsPN4aBh2fTPSOJHk1XykP7dXTozeJSUl55Is0dSUFYbdkywTv7QKfz277fe8mW3ZXLsgL5ppCfFcaUV0lpR1+QNfxzTL51xA/xv3P936ngS4mKYNDgTIQQHDs3y85vvrWoIS2x1H0uqXRx3r5qkdxrpaOw++3DISo4PcqHc9Noab1ix0znKS1cT7Tp8cMZw/4Wbf3/SGHLTEshKSfDb/5gAl9Wjn2/j/kWb/ebc7L/nSNOqySKlXCqEWAAsB9zA98AjQBrwkhDiEpTInxvJjoZiW2ktp00ZyANzfUXnB9gyuYaIIk4veg73kMtI2/U56ete9L43YvIRvDT9MH5izSafMXUQLy8rZE9lA1OHZAEwZ8ZQzjt4iOMw0Z7U0Dc9MShbMDctkZgYwej+aazZpURY/xi276slJV5HPqgf83OXHuI3lMvPSeGBOVN5YM5UgFaHqt7vn5HEx+uLvYkbTlQ1NJGaEMe+ukYmDspgjWWJOS2NZh/6zhjeh1/NCrYW7Qzpk8LaW070vk5JiKWusdnr4rrmOBWjnX/DO4D/BfbQBQcBKu1+S3EN+TmpjlZmhSXuev/NxTUkObg5wiU2RpAUHxOUWq8nrFIc/Mj5ual8/cfZ3tdZKQkhi3np77ipqIYx/cMrvaAjLnQRJHvYWrXLTVJ8TJuG5ikJcayefyLFVQ3865Mt7KtrpLaxmatnj+La44Pj5o8e05dNt53sff3CvEMBFSP9x9dW0+yRYRVosgu4JnCuQcfzQ9st8GyrjITHI4mJEUG1SBwFPC2RyvomKuubmP/j8ZwwwX/h8MuPPoDLjz4gaP85M4Z6J3w3/OUkhFCVOK98zhdJpOc+uoKwnIZSypullGOllBOllBdKKV1SyjIp5Wwp5Sjr0TkWJ4K43M3srqh3TGw5apSy8I6LWc75nrf50RAXTyX8jfTaAu82g8dM8/rnAEb2TfNWKbP/iMIRzjzbMG1Mv3SrTf3jjx+nfhypCXEMtwS6oLTOL6YX8FpmOspkYFYyQgjvX7j0zUikvqnZz1qy88HavUye/yGXPPUt5XVN9ElN9E5EDXdIyrBHggwPI+YX8Ou3nmgMdP/oeF8n/7Lux4jcVPoEXBBnP/QlG/ZWe8VDC3xKByxw1Y+4IGG54NGlACEz/cJF97GyvinsqnT6Mw8OsA419nrnbUFb9jss92OoSc5A9P8z23ZthCO2GUlxJMbF+JUbOOn+z/y20Ss7pVkTk20hMyUBKZUf+vHPtzH8xndJtM0x5KYFC6qfVT225SXg+tss6gkDMzhkRB/6ZyYRE6POR2AcfI7D50WKqM7E3LmvHo90Tmz559wD+dv7G8n4TrlYxqUGhzElDpjIkD4p/Ov8A4mLEeSmJXL0mL4s3bavzaUf9TA+Ny2RQdnJbCyq9i52cOWxBzB2QDrHju3rnSzdXlYbFNyvL4YX5h1GQVltm3/I3r7YLB4nC+2dVSpCZv2eKpLjYxnWJ4V75h3KmQ9+GbSiSqA1MyhEyFtL3HPuFFbsrOC0qf6+89evOILCinpHEfrjKeM4alQuJ03oH1QESKdT64t02rBsXl+x2y+Msz2kJMb6hRHurqhnW2ktB+dnc5ytYFd7mDDQV1Zg6tCssPYZ0ieFJ39+cNDwfmz/dDbsrXb07YZDYlws4wdk8NF6NW3V1rmDTD8Bb12shBDkpSeyaldlyG0GZyezbHt5uxJe9LVaXtfIK1bS0UpbpFdgaQRQRg4oQ8FJP+z0z0zyhlUOzk7hr2dO8islbM9EfeaSQ/w8AJEmqotZ6VKbTqm8WSkJXHjoMDKEEvDBcQ4/HquO96mTB3LSRFVfRIdltbX4uhbN/JwUb5SG/mHExghOnNDfW7c7OyWeO97bwOaArEkddZCXnsjB+c5WV1h9SbMmjRxWjGn2SG+MeVlNI/tqG8lKiadvRhInTOhHYXk9//5kC7sr6vnHos387f2NfkkKTqFdrTFxUCYXHDosyIrtm5EUcqJneG4qPzssv0V/oo4iOHq0sqDsiRftIdAC19m8t585Kai2Slux76/7Gw5Hj+kbJLBjLRdMXhvrhPsfN8+7PmRbIyTs0THhujvy0hNZEZB1a0e7w9oTSWSvzz7Fcn1WtVKXXF8j4cwrgSpHoGsQDemTwuTBWd737OfvyFFdu2ByVFvgOrXcyYUCKuqjON0N9dAf/8zM3ac8ycDY4K8/tn86p00ZGPY/VqOHyPm5qd5Z6lDuhuPH9+OlZYVBFdU6EgZnx2uBO1RdKyyvo7xORQ+s31NFVYPba8HoC/PuDzbyxBfb/GKcbzh5LJ9tLuG8gzu4WHMnct0Jym87NCeFsw4cxAkTOmYlpyTE+k1ibtxbTVpinGPERHt49GfTeW3FrqDJwnC565zJrN1VyfT8PqzaVcnsce1f/d1+40xp483JPgrLCnPC8eSJ/akIKAsAMP/H4/l8SymzRufx/pq9nDq57SvQ67kVe2KN5ncnONfEOaBvGkeNyu203/NlM0eETMiKJNEt4KW1pCfFBflINYlxsRw9LBE2QHyJf23iAQef4biPEMJvQjRc7Bb4p2XKcgvlbrjjrMks+K4w6MfcHn+mE30dJo00+jMPGd7HWwo0cKUZICjZZtKgzFYnL7uSN688ws8Kuve8qR0+ZmpinN+CE5X1TWSnxnfa/+W48f04bnz7bzI/mT4EpivBaS2UszWyU33/a6cQyZawuznCtcDnzRzJvJkj+XhDEb94cpm3/UeTB3KxVVRr4XWz2tQPzci8NAZmJrF4Y7Hfdzlj6kCuPHaU4z7JCbEdWuUnkBtPGddpx2oLUe1CKShThXRavMAarGH1bv96E511UWr0RN3Y/hneAkuhqrTFxgj6pCZ606o7m6yUeBLjYoJWoQFf2KXdRaMvwpYuxn4ZkanlEC46fOsMy48eVqXJNpKaEOdXa6SirjFsCzPasLsI22qBA5w6WbkTQhlPoQgswxDuBGpLCCGYOTqPL7eW4bK5+yKxgEJPI7ot8LJapg5pJVi+vkI92hZlkDHxdK58K0FZdN0sRualMmtMHudNH+ItV+lEXnoipTUuEmJj+Pam43A1t335qVAIITg4vw+fbw5eyHZbaS2pCbF+CUZauDNCTCA9fOFBQck9Xc1DFxxEaY2LfhlJXH3c6A5HhTiRm57A0m2+UUt5XeTKgHY39kn6tlrgAPedN5XfnjCmzfsG+ts7ErtvZ0z/dF74dieFtuSnrljgpLuJ2lvUI0u2snOfcwihl22fQdFq/7Zj/oS4bElE+nRA3zSEEMTHxrRqIWqXS3pSHJkp8e2OKAjF0WPy2Fxcw3UvrvBr326lDdtjq3XiRKh1Bg8fGZlCPG1B1ahIIT42xjHUsTPIS0uivK7Jm+1YWR+5MqDdjf17tccCb+//IbAMQ2eNhPX1tnGvL7O02eMJtfl+Q9QK+Ovfq4zKFic9njo1oEHAkddAv/ER61e46AiCUIWhOsppUwfSJzWBV7/f5ReNUlBWx/DcVFIT45g3cwRnHjiI8VY69FGj8rjosGHMCZjYiVQdh56GvqnqsqaRXEmlu7G7F7ry/5ue6Ltx/OlHnec31rHYDU0eDh3Rh18eOZxbT5/Yyl7RT9QKeEmNizkHDwmd1VZeENyWmgexPcOi0nGoTinanXL89CSesSZp3ly5m9WFlSzZVMK20lrvrP0fTxnHfedN9Ya4JcTFcMvpE4NWTuns+YKein3y1+ORKulmP3Wh2OlIDZm2YjdYfnlU6MJsbWVwdrI3xDU1IY4/nTreG5q4PxOVplWzR1JW4wpZyAdQ7pNA0vsHt3UTB1hZiZEMPRo3IJ3slHi/Av1Aqyuo2OOL2zpJFc1oC7y4ykV1HzceyX7rQrGja610BVpkz57WeqG1thAfG8OAzCQKy+tbrI++vxGVAl5W68IjQxTyqd4LZVug4DNIzoYDL4DcMfDmlZAevBhwd3HWtEEcfkCOt1BTJBBCMDArOSjapTX/vI5QmDIki1d+dVjE+tfTsMfP69rj+6sLpTvZfPvJxEZgVNcnNcEIeDSg45sdBfyF82HXd5CYCQccCyfcBmXWsmQ9yAIXQnRJym3f9ETWBrS1OPGLz+rsl57YK0KxNPpmeuOrq72WYm9woXQ17S0R0Rra8OishLhoYP8T8CqrXKyrEvKPUs/T+4OIhcyek0XYVehzlBAbwz/PP5CKusZWy12OG5DOHWdN4qQJPeeG1xUkxMVw3fGjuXfhJpo9komDMjisB0TgRIr3rj6KvQ7lFqIVHRppLPAejhZwx9C7nAOg2lqncMKZ6jEhFS56E/pN6KIe9hy0gJ990GBODFOQhRCOK+f0Bq6aPYp7F24C4MHzD+rSCb6uZtyAjLAWZIgW9GjJWOA9HL2yh6P/2G1ZFKf/G1JsBaHyj+yCnvU8tA83PrZ3RJJ0Bn84aSyfbylhaCuuJkPPQicVJcQaC9yLEGIM8KKtaQTwf0AWcCmgVyr+o5Ty3c7uoBMl1S7SE+OcK8TVl8OEs9TkpaHXhAB2Jr8+eqTjEmeGno3O6gwsibw/E86KPBuBqQBCiFhgF/Aa8HPgPinlPZHsoBMlNS7yAmtzfPVvWPYE1Jaq6BMDgDedv7XQQYMh2tGuE6dVpfZX2upCmQ1slVJu707LrqTKFVwL+aO/gNuqg2AE3MuJE/rx1C9mcNQBXVun2GDoavTkZUMvEvC2evvnAM/bXl8phFglhHhcCOGomkKIeUKIZUKIZSUlJU6btJmSGldwBMpQW2nIlPYvhrC/IYRg1ug8YtqxEIPBEE0kWSvvuNy9x4UStoALIRKA04CXraaHgJEo98oe4O9O+0kpH5FSTpdSTs/La9siCaEoqXYQ8AZfERsS95+ZdYPBEB4nT+rPUaNyuXq2cw3w/ZG2uFBOBpZLKYsA9COAEOK/wNud3DdH6hrd1LjcwSGEDbbltJr9FyMwGAz7P+lJ8Z26SEM00BYBn4vNfSKEGCCltAKuORNY47hXJxMyiaehEqacr9a5nDK3K7piMBgM3UpYAi6ESAGOBy6zNd8lhJgKSKAg4L2I4SjgUioBT+8Hs67vim4YDAZDtxOWgEsp64CcgLYLI9KjVvBlYdoEvKkePE2QlNkdXTIYDIZuIeoyMYsDLfDvnoI9K9RzI+AGg6EXEXUCXlLtUosC6zKfb13le9MIuMFg6EVEXdWXkmoXuWkJKq65ao//m0lZ3dIng8Fg6A6iTsCLqxt87pMdX/m/mZzV5f0xGAyG7iKqBPz173fxycYSXwx4XZn/Bv2ndH2nDAaDoZuIKh/4+j0q2/Ka46xMq8Za9TjpXJh6PsRG1dcxGAyGDhFViudye8hIimPy4CzV0FQHCDjzEYiJqsGEwWAwdJioUj2Xu5lE+3JJjbUQn2LE22Aw9EqiSvlcTR4S42xdbqxRy6UZDAZDLyS6BNzt8V+wtLHOCLjBYOi1RJmANwdY4LWQkNZ9HTIYDIZuJMoE3MmFYhaeNRgMvZPoEvAmD4lxAZOYxoViMBh6KdEl4O5mEuNtXW4yPnCDwdB7iSoBb3CMQjE+cIPB0DuJKgFXk5jGhWIwGAwQhoALIcYIIVbY/qqEENcIIfoIIRYKITZbj46r0ncmwZOYViKPwWAw9EJaFXAp5UYp5VQp5VTgIKAOeA24AfhISjkK+Mh6HVFcbo/PB97cBO4G40IxGAy9lra6UGYDW6WU24HTgaes9qeAMzqxX464miwXyrIn4C+5qjHRCLjBYOidtFXA5+Bbmb6fXpXeeuzrtIMQYp4QYpkQYllJSUn7e4rOxIyBda/7GvtP6tAxDQaDIVoJW8CFEAnAacDLbfkAKeUjUsrpUsrpeXl5be2fF3ezB7dHKgvcLtqDZ7T7mAaDwRDNtMUCPxlYLqUssl4XCSEGAFiPxZ3dOTuNzR4ANYkppWrMHQ3xSZH8WIPBYOixtEXA5+JznwC8CVxkPb8IeKOzOuWEq8km4E31kJAOv/oikh9pMBgMPZqwBFwIkQIcD7xqa74TOF4Isdl6787O754Pl9sS8PhYFX2SlAlxCZH8SIPBYOjRhLUij5SyDsgJaCtDRaV0CS53M2CzwI3rxGAw9HKiJhOzwetCsSzwuORu7pHBYDB0L1Ej4MEWuBFwg8HQu4kiAdc+cONCMRgMBogmAfdzodQbF4rBYOj1RI+A+7lQGowFbjAYej1RJODKAk+KtyxwU4XQYDD0cqJIwAMmMeOMBW4wGHo30SPgTfZJzAYThWIwGHo90SPg7sBJTGOBGwyG3k0UCbjlQonxgMdtfOAGg6HXEzUC7s3ElC7VYKJQDAZDLydqBNzlbiY2RhDnsQTcuFAMBkMvJ3oEvMla0Pjln6sGsxamwWDo5USPgOsV6YvWKPEec1J3d8lgMBi6lSgS8GayY13QUAEzr4fk7O7uksFgMHQrUSTgHobFlaoXWUO7tzMGg8HQAwh3RZ4sIcQCIcQGIcR6IcRhQoj5QohdQogV1t8pkeyoq8nDUKEFfFgkP8pgMBiigrBW5AH+AbwvpTzHWp0+BTgRuE9KeU/EemfD5W5mkChRL7KNgBsMBkOrAi6EyABmAhcDSCkbgUYhRGR7FoDL7SGPcoiJg5Sc1ncwGAyG/ZxwXCgjgBLgCSHE90KIR4UQqdZ7VwohVgkhHhdCOM4qCiHmCSGWCSGWlZSUtLujLreHFOGC+FTo4puHwWAw9ETCEfA4YBrwkJTyQKAWuAF4CBgJTAX2AH932llK+YiUcrqUcnpeXl67O+pyN5OCCxJMCr3BYDBAeAJeCBRKKZdarxcA06SURVLKZimlB/gvMCNSnQSVSp+Ey9RAMRgMBotWBVxKuRfYKYQYYzXNBtYJIQbYNjsTWBOB/nlxuZtJNha4wWAweAk3CuU3wLNWBMoPwM+BB4QQUwEJFACXRaKDGleTh6SEBuUDNxgMBkN4Ai6lXAFMD2i+sNN70wIut4fE+AZIyOrKjzUYDIYeSxRlYjarUrLGB24wGAxAlAi4lBKX20OCxyxmbDAYDJqoEPCmZomUkOAxk5gGg8GgiQoB18upxXnqzSSmwWAwWESJgHsASXxzvbHADQaDwSJqBDyRJgTS+MANBoPBIioEvKGpmRQa1IsE40IxGAwGiBIBdzV5VB0UgPjk7u2MwWAw9BCiQ8DdzSQLLeDGhWIwGAwQNQLuYagoVi+Ssrq1LwaDwdBTiBoBnxf3Do0pA2D4Ud3dHYPBYOgRRIeANzUzUuymZujREJfY3d0xGAyGHkG41Qi7FZfbQzxuiE/q7q4YDAZDjyGqBNwTl9DdXTEYDIYeQ3S4UNzNxOMmxgi4wWAweAlLwIUQWUKIBUKIDUKI9UKIw4QQfYQQC4UQm61Hx0WNOwNXYzMJopnYeOP/NhgMBk24Fvg/gPellGOBKcB61MLGH0kpRwEfWa8jQlNTI4ARcIPBYLDRqoALITKAmcBjAFLKRillBXA68JS12VPAGZHpIjQ2qiQeI+AGg8HgIxwLfARQAjwhhPheCPGoECIV6Cel3ANgPfZ12lkIMU8IsUwIsaykpKRdnXQ3KQE3PnCDwWDwEY6AxwHTgIeklAcCtbTBXSKlfERKOV1KOT0vL69dnWy2BJzY+HbtbzAYDPsj4Qh4IVAopVxqvV6AEvQiIcQAAOuxODJdhBhPk3oSayxwg8Fg0LQq4FLKvcBOIcQYq2k2sA54E7jIarsIeCMiPQSuOzZfPTECbjAYDF7CTeT5DfCsECIB+AH4OUr8XxJCXALsAM6NTBeBZssCj4mKvCODwWDoEsJSRCnlCmC6w1uzO7U3oWhWYYTGAjcYDAYfUZGJaQTcYDAYgokSAXerRxOFYjAYDF6iRMCNBW4wGAyBGAE3GAyGKCVKBFzHgRsXisFgMGiiRMCNBW4wGAyBGAE3GAyGKCVKBFy7UEwij8FgMGiiRMCNBW4wGAyBGAE3GAyGKCU6BNxjEnkMBoMhkOgQcGOBGwwGQxBGwA0GgyFKiRIBN+VkDQaDIZAoEfBGZX0L0d09MRgMhh5DlAh4k3GfGAwGQwBhCbgQokAIsVoIsUIIscxqmy+E2GW1rRBCnBKxXjY3GveJwWAwBNAWVTxGSlka0HaflPKezuyQI/0mwri6iH+MwWAwRBPRYdYedJH6MxgMBoOXcH3gEvhQCPGdEGKerf1KIcQqIcTjQohspx2FEPOEEMuEEMtKSko63GGDwWAwKMIV8COklNOAk4ErhBAzgYeAkcBUYA/wd6cdpZSPSCmnSymn5+XldUKXDQaDwQBhCriUcrf1WAy8BsyQUhZJKZullB7gv8CMyHXTYDAYDIG0KuBCiFQhRLp+DpwArBFCDLBtdiawJjJdNBgMBoMT4Uxi9gNeEyqJJg54Tkr5vhDiaSHEVJR/vAC4LFKdNBgMBkMwrQq4lPIHYIpD+4UR6ZHBYDAYwiI6MjENBoPBEIQRcIPBYIhShJSy6z5MiBJgezt3zwUCM0F7O+acBGPOSTDmnAQTbedkmJQyKA67SwW8Iwghlkkpp3d3P3oS5pwEY85JMOacBLO/nBPjQjEYDIYoxQi4wWAwRCnRJOCPdHcHeiDmnARjzkkw5pwEs1+ck6jxgRsMBoPBn2iywA0Gg8Fgwwi4wWAwRClRIeBCiJOEEBuFEFuEEDd0d3+6CqvOerEQYo2trY8QYqEQYrP1mG1770brHG0UQpzYPb2OHEKIIUKIT4QQ64UQa4UQV1vtvfmcJAkhvhFCrLTOyS1We689JxohRKwQ4nshxNvW6/3vnEgpe/QfEAtsBUYACcBKYHx396uLvvtMYBqwxtZ2F3CD9fwG4G/W8/HWuUkEhlvnLLa7v0Mnn48BwDTreTqwyfrevfmcCCDNeh4PLAUO7c3nxHZurgOeA962Xu935yQaLPAZwBYp5Q9SykbgBeD0bu5TlyClXALsC2g+HXjKev4UcIat/QUppUtKuQ3Ywn5Wo11KuUdKudx6Xg2sBwbRu8+JlFLWWC/jrT9JLz4nAEKIwcCPgEdtzfvdOYkGAR8E7LS9LrTaeiv9pJR7QAka0Ndq71XnSQiRDxyIsjh79TmxXAUrgGJgoZSy158T4H7g94DH1rbfnZNoEHDh0GZiH4PpNedJCJEGvAJcI6WsamlTh7b97pxItTLWVGAwMEMIMbGFzff7cyKEOBUollJ+F+4uDm1RcU6iQcALgSG214OB3d3Ul55AkV4NyXosttp7xXkSQsSjxPtZKeWrVnOvPicaKWUFsBg4id59To4AThNCFKBcrscKIZ5hPzwn0SDg3wKjhBDDhRAJwBzgzW7uU3fyJnCR9fwi4A1b+xwhRKIQYjgwCvimG/oXMYRaFuoxYL2U8l7bW735nOQJIbKs58nAccAGevE5kVLeKKUcLKXMR+nFx1LKC9gfz0l3z6KGOZt8CiriYCtwU3f3pwu/9/PAHqAJZSVcAuQAHwGbrcc+tu1vss7RRuDk7u5/BM7Hkaih7SpghfV3Si8/J5OB761zsgb4P6u9156TgPNzNL4olP3unJhUeoPBYIhSosGFYjAYDAYHjIAbDAZDlGIE3GAwGKIUI+AGg8EQpRgBNxgMhijFCLjBYDBEKUbADQaDIUr5f/drKS4QENJHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "states[['val_VAL_1','train_VAL_1']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "states[['val_VAL_jac','train_VAL_jac']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.016014138857524,\n",
       " 3.580979287624359,\n",
       " 3.476141393184662,\n",
       " 3.359775106112162,\n",
       " 3.262439409891764,\n",
       " 3.1556219458580017,\n",
       " 3.0627198815345764,\n",
       " 2.9832192262013755,\n",
       " 2.894130051136017,\n",
       " 2.8221245805422464,\n",
       " 2.7742998798688254,\n",
       " 2.710253198941549,\n",
       " 2.663776616255442,\n",
       " 2.6352937618891397,\n",
       " 2.583137810230255,\n",
       " 2.5726837515830994,\n",
       " 2.550245145956675,\n",
       " 2.5174156427383423,\n",
       " 2.498042583465576,\n",
       " 2.5006632804870605,\n",
       " 2.483529508113861,\n",
       " 2.469638546307882,\n",
       " 2.4693893790245056,\n",
       " 2.454873959223429,\n",
       " 2.452928980191549,\n",
       " 2.445686995983124,\n",
       " 2.449835201104482,\n",
       " 2.4269318183263144,\n",
       " 2.441512187321981,\n",
       " 2.4307491381963096,\n",
       " 2.425937592983246,\n",
       " 2.4177355766296387,\n",
       " 2.412558356920878,\n",
       " 2.414771775404612,\n",
       " 2.420555830001831,\n",
       " 2.4095747470855713,\n",
       " 2.417506297429403,\n",
       " 2.4048957228660583,\n",
       " 2.4012410243352256,\n",
       " 2.4133909344673157,\n",
       " 2.4146655797958374,\n",
       " 2.4124674002329507,\n",
       " 2.395471533139547,\n",
       " 2.4041011730829873,\n",
       " 2.373327632745107,\n",
       " 2.4147808949152627,\n",
       " 2.40478777885437,\n",
       " 2.3814737200737,\n",
       " 2.4040048917134604,\n",
       " 2.393053491910299,\n",
       " 2.3750780622164407,\n",
       " 2.3751267790794373,\n",
       " 2.38983682791392,\n",
       " 2.3842284083366394,\n",
       " 2.3853240609169006,\n",
       " 2.3510618011156716,\n",
       " 2.4029164711634317,\n",
       " 2.396230181058248,\n",
       " 2.393138885498047,\n",
       " 2.3957826495170593,\n",
       " 2.380471487840017,\n",
       " 2.386141916116079,\n",
       " 2.3704294562339783,\n",
       " 2.365254561106364,\n",
       " 2.382726530234019,\n",
       " 2.370084822177887,\n",
       " 2.393240511417389,\n",
       " 2.3738020062446594,\n",
       " 2.378271222114563,\n",
       " 2.3794250090916953,\n",
       " 2.361500859260559,\n",
       " 2.376123607158661,\n",
       " 2.3814006447792053,\n",
       " 2.357136607170105,\n",
       " 2.353886822859446,\n",
       " 2.3945969343185425,\n",
       " 2.3695674737294516,\n",
       " 2.3480570713678994,\n",
       " 2.3620495994885764,\n",
       " 2.3731311162312827,\n",
       " 2.363567809263865,\n",
       " 2.3861716985702515,\n",
       " 2.3616434733072915,\n",
       " 2.382958730061849,\n",
       " 2.3751869002978006,\n",
       " 2.374918540318807,\n",
       " 2.3699220220247903,\n",
       " 2.370315968990326,\n",
       " 2.3803426027297974,\n",
       " 2.3490475018819175,\n",
       " 2.3744342724482217,\n",
       " 2.3611080646514893,\n",
       " 2.3741276264190674,\n",
       " 2.3455232779184976,\n",
       " 2.370332916577657,\n",
       " 2.352614084879557,\n",
       " 2.3653539617856345,\n",
       " 2.3722551266352334,\n",
       " 2.373527407646179,\n",
       " 2.3717002471288047,\n",
       " 2.3835099140803018,\n",
       " 2.344513158003489,\n",
       " 2.3756223320961,\n",
       " 2.378075281778971,\n",
       " 2.35638827085495,\n",
       " 2.3701693415641785,\n",
       " 2.3813953002293906,\n",
       " 2.3654622435569763,\n",
       " 2.367119789123535,\n",
       " 2.3470069567362466,\n",
       " 2.3602728247642517,\n",
       " 2.35102109114329,\n",
       " 2.3567301829655967,\n",
       " 2.3593870401382446,\n",
       " 2.351957162221273,\n",
       " 2.3552029530207315,\n",
       " 2.3550225297609964,\n",
       " 2.3517017563184104,\n",
       " 2.34880667924881,\n",
       " 2.326340893904368,\n",
       " 2.3484432101249695,\n",
       " 2.33596408367157,\n",
       " 2.3655043045679727,\n",
       " 2.3533435066541037,\n",
       " 2.362076759338379,\n",
       " 2.350716491540273,\n",
       " 2.3421265482902527,\n",
       " 2.36586731672287,\n",
       " 2.3265228470166526,\n",
       " 2.344319979349772,\n",
       " 2.3607678016026816,\n",
       " 2.3719481428464255,\n",
       " 2.351541817188263,\n",
       " 2.377408742904663,\n",
       " 2.3533135255177817,\n",
       " 2.361546834309896,\n",
       " 2.3601023157437644,\n",
       " 2.3594932357470193,\n",
       " 2.3442017634709678,\n",
       " 2.3446496725082397,\n",
       " 2.342009743054708,\n",
       " 2.332191546758016,\n",
       " 2.3377412954966226,\n",
       " 2.3603710730870566,\n",
       " 2.3483949502309165,\n",
       " 2.3711174925168357,\n",
       " 2.3486301501592,\n",
       " 2.33932759364446,\n",
       " 2.3672545552253723,\n",
       " 2.361992677052816,\n",
       " 2.360950549443563,\n",
       " 2.3473278482755027,\n",
       " 2.339685559272766,\n",
       " 2.3594000339508057,\n",
       " 2.3548028667767844,\n",
       " 2.346066474914551,\n",
       " 2.348200738430023,\n",
       " 2.365846574306488,\n",
       " 2.378258923689524,\n",
       " 2.34381635983785,\n",
       " 2.34015945593516,\n",
       " 2.3532375494639077,\n",
       " 2.363723119099935,\n",
       " 2.356558680534363,\n",
       " 2.3491830229759216,\n",
       " 2.3644002874692283,\n",
       " 2.3535417119661965,\n",
       " 2.3440672159194946,\n",
       " 2.3644270499547324,\n",
       " 2.3466598192850747,\n",
       " 2.353440006573995,\n",
       " 2.354663133621216,\n",
       " 2.358858565489451,\n",
       " 2.3360355695088706,\n",
       " 2.3566197554270425,\n",
       " 2.343908647696177,\n",
       " 2.342079281806946,\n",
       " 2.355863948663076,\n",
       " 2.3527610500653586,\n",
       " 2.3339257637659707,\n",
       " 2.3355759580930076,\n",
       " 2.353414257367452,\n",
       " 2.3423049251238504,\n",
       " 2.3546899358431497,\n",
       " 2.3722722927729287,\n",
       " 2.3433383901913962,\n",
       " 2.3614091873168945,\n",
       " 2.349238634109497,\n",
       " 2.355355183283488,\n",
       " 2.3364749352137246,\n",
       " 2.337398966153463,\n",
       " 2.341395318508148,\n",
       " 2.348423659801483,\n",
       " 2.3420873085657754,\n",
       " 2.355209986368815,\n",
       " 2.3347582817077637,\n",
       " 2.3461950620015464,\n",
       " 2.3514471650123596,\n",
       " 2.3524585564931235,\n",
       " 2.3533819715181985,\n",
       " 2.3327417969703674,\n",
       " 2.3412396709124246,\n",
       " 2.3482646346092224,\n",
       " 2.3258248368899026,\n",
       " 2.3464712699254355,\n",
       " 2.3454625010490417,\n",
       " 2.3443915247917175,\n",
       " 2.338489611943563,\n",
       " 2.3427399595578513,\n",
       " 2.33308607339859,\n",
       " 2.3355269034703574,\n",
       " 2.3439786235491433,\n",
       " 2.348806897799174,\n",
       " 2.3625837564468384,\n",
       " 2.3384679357210794,\n",
       " 2.352301855882009,\n",
       " 2.3518435756365457,\n",
       " 2.356831729412079,\n",
       " 2.3450879057248435,\n",
       " 2.354252338409424,\n",
       " 2.3316397865613303,\n",
       " 2.353452821572622,\n",
       " 2.341839094956716,\n",
       " 2.333790957927704,\n",
       " 2.326503574848175,\n",
       " 2.33182285229365,\n",
       " 2.3546353578567505,\n",
       " 2.351185659567515,\n",
       " 2.358602782090505,\n",
       " 2.327974776426951,\n",
       " 2.368851621945699,\n",
       " 2.3591676553090415,\n",
       " 2.3567435145378113,\n",
       " 2.3524892926216125,\n",
       " 2.345068355401357,\n",
       " 2.3501105904579163,\n",
       " 2.326898137728373,\n",
       " 2.3310365875562034,\n",
       " 2.325144648551941,\n",
       " 2.343954583009084,\n",
       " 2.3431854248046875,\n",
       " 2.3376614848772683,\n",
       " 2.3492897351582847,\n",
       " 2.347874959309896,\n",
       " 2.3289391001065574,\n",
       " 2.343879222869873,\n",
       " 2.3160665035247803,\n",
       " 2.350836932659149,\n",
       " 2.348975638548533,\n",
       " 2.3467960556348166,\n",
       " 2.342164178689321,\n",
       " 2.3382447163263955,\n",
       " 2.3284965554873147,\n",
       " 2.3246918519337973,\n",
       " 2.3569940527280173,\n",
       " 2.3582049012184143,\n",
       " 2.331792930761973,\n",
       " 2.354574223359426,\n",
       " 2.347904841105143,\n",
       " 2.336475888888041,\n",
       " 2.3510162631670632,\n",
       " 2.3373314142227173,\n",
       " 2.332490344842275,\n",
       " 2.3340445359547934,\n",
       " 2.3527621229489646,\n",
       " 2.3332653244336448,\n",
       " 2.339162290096283,\n",
       " 2.3371700843175254,\n",
       " 2.3342640002568564,\n",
       " 2.345694343249003,\n",
       " 2.3330258329709372,\n",
       " 2.334805130958557,\n",
       " 2.352565904458364,\n",
       " 2.3447222312291465,\n",
       " 2.3365224798520408,\n",
       " 2.3403624296188354,\n",
       " 2.347195327281952,\n",
       " 2.3541723092397056,\n",
       " 2.3352925380071006,\n",
       " 2.3585981329282126,\n",
       " 2.3527050813039145,\n",
       " 2.331176201502482,\n",
       " 2.3434005975723267,\n",
       " 2.3381160696347556,\n",
       " 2.34757928053538,\n",
       " 2.35615066687266,\n",
       " 2.3308611710866294,\n",
       " 2.3315317829449973,\n",
       " 2.336939255396525,\n",
       " 2.341226855913798,\n",
       " 2.3377283811569214,\n",
       " 2.3366944591204324,\n",
       " 2.34161506096522,\n",
       " 2.338366448879242,\n",
       " 2.3276671767234802,\n",
       " 2.3387354214986167,\n",
       " 2.3410407503445945,\n",
       " 2.33526748418808,\n",
       " 2.346993883450826,\n",
       " 2.349900722503662,\n",
       " 2.346663494904836,\n",
       " 2.33445543050766,\n",
       " 2.3330488999684653,\n",
       " 2.3246732155481973,\n",
       " 2.347734888394674,\n",
       " 2.3388441602389016,\n",
       " 2.343725879987081,\n",
       " 2.3512978752454123,\n",
       " 2.3514621456464133,\n",
       " 2.337273279825846,\n",
       " 2.347037196159363,\n",
       " 2.334003428618113,\n",
       " 2.336957593758901,\n",
       " 2.3508103092511496,\n",
       " 2.3240662217140198,\n",
       " 2.325262506802877,\n",
       " 2.335076371828715,\n",
       " 2.3389208714167276,\n",
       " 2.3398008743921914,\n",
       " 2.3493410547574363,\n",
       " 2.3438364466031394,\n",
       " 2.339883248011271,\n",
       " 2.327455143133799,\n",
       " 2.3383060495058694,\n",
       " 2.3283671935399375,\n",
       " 2.3406534592310586,\n",
       " 2.335781713326772,\n",
       " 2.3415627678235373,\n",
       " 2.350347022215525,\n",
       " 2.3511073191960654,\n",
       " 2.3411142230033875,\n",
       " 2.3267287214597068,\n",
       " 2.3461877504984536,\n",
       " 2.338520348072052,\n",
       " 2.3456786473592124,\n",
       " 2.346846640110016,\n",
       " 2.3434359033902488,\n",
       " 2.3368441661198935,\n",
       " 2.351143538951874,\n",
       " 2.3497967521349588,\n",
       " 2.3415074348449707,\n",
       " 2.3448121547698975,\n",
       " 2.330967823664347,\n",
       " 2.344583789507548,\n",
       " 2.345853865146637,\n",
       " 2.3340320587158203,\n",
       " 2.354803959528605,\n",
       " 2.3505939245224,\n",
       " 2.324528614679972,\n",
       " 2.341008643309275,\n",
       " 2.342706819375356,\n",
       " 2.340651591618856,\n",
       " 2.345959742863973,\n",
       " 2.334669212500254,\n",
       " 2.3426411151885986,\n",
       " 2.3402794003486633,\n",
       " 2.314810554186503,\n",
       " 2.3397565881411233,\n",
       " 2.3291311065355935,\n",
       " 2.3365612427393594,\n",
       " 2.338789085547129,\n",
       " 2.33844264348348,\n",
       " 2.3296879132588706,\n",
       " 2.3373787800470986,\n",
       " 2.335778295993805,\n",
       " 2.3319231271743774,\n",
       " 2.3401660323143005,\n",
       " 2.3352246085802713,\n",
       " 2.343611796696981,\n",
       " 2.341631293296814,\n",
       " 2.3541815280914307,\n",
       " 2.3327117959658303,\n",
       " 2.3396732409795127,\n",
       " 2.3253584702809653,\n",
       " 2.3409852584203086,\n",
       " 2.316529154777527,\n",
       " 2.3245728611946106,\n",
       " 2.348609904448191,\n",
       " 2.3354889154434204,\n",
       " 2.346611519654592,\n",
       " 2.3248634934425354,\n",
       " 2.3364249070485434,\n",
       " 2.343295991420746,\n",
       " 2.3471620678901672,\n",
       " 2.344647705554962,\n",
       " 2.3279197216033936,\n",
       " 2.3319934606552124,\n",
       " 2.342416266600291,\n",
       " 2.350442965825399,\n",
       " 2.3323514660199485,\n",
       " 2.327678660551707,\n",
       " 2.3262283404668174,\n",
       " 2.35122412443161,\n",
       " 2.3228691617647805,\n",
       " 2.348047693570455,\n",
       " 2.3212486505508423,\n",
       " 2.339704155921936,\n",
       " 2.343515157699585,\n",
       " 2.340518673261007,\n",
       " 2.3130542039871216,\n",
       " 2.341365655263265,\n",
       " 2.3362598220507302,\n",
       " 2.328787684440613,\n",
       " 2.3327929973602295,\n",
       " 2.3491464058558145,\n",
       " 2.3489977518717446,\n",
       " 2.3255539735158286,\n",
       " 2.3327155311902366,\n",
       " 2.33163845539093,\n",
       " 2.3310036857922873,\n",
       " 2.3505712151527405,\n",
       " 2.335424800713857,\n",
       " 2.323285222053528,\n",
       " 2.3169257044792175,\n",
       " 2.350167195002238,\n",
       " 2.3357329765955606,\n",
       " 2.343007961908976,\n",
       " 2.3223320841789246,\n",
       " 2.3037697076797485,\n",
       " 2.3163230617841086,\n",
       " 2.341017425060272,\n",
       " 2.334459145863851,\n",
       " 2.330294648806254,\n",
       " 2.3369978864987693,\n",
       " 2.341898262500763,\n",
       " 2.32839165131251,\n",
       " 2.332941552003225,\n",
       " 2.315711955229441,\n",
       " 2.3391212224960327,\n",
       " 2.343146582444509,\n",
       " 2.337365706761678,\n",
       " 2.3202757438023887,\n",
       " 2.323006272315979,\n",
       " 2.3393031557401023,\n",
       " 2.340575853983561,\n",
       " 2.3420414527257285,\n",
       " 2.342699646949768,\n",
       " 2.338997999827067,\n",
       " 2.345905820528666,\n",
       " 2.3282124598821006,\n",
       " 2.320876717567444,\n",
       " 2.3267672260602317]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state['train_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=2917, x=[2917, 1753], y=[2917, 20], node_type=[2917], att_lab=[2917], val_lab=[2917], train_mask=[2917], val_mask=[2917], test_mask=[2917], edge_index=[2, 104789], edge_attr=[104789], n_id=[2917], batch_size=32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=~(data.train_mask + data.val_mask + data.test_mask),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_Homo(model, loader):\n",
    "    model.eval()\n",
    "    seed_everything(args.seed)\n",
    "    all_preds = []\n",
    "    \n",
    "    for batch in tqdm(loader):\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch.batch_size\n",
    "        edge_index = to_undirected(batch.edge_index)\n",
    "        out = model(batch.x, edge_index)[:batch_size]\n",
    "        out_att = out[:,:9].softmax(axis=1)\n",
    "        out_val = out[:,9:].softmax(axis=1)\n",
    "        IDs = batch.n_id[:batch_size].unsqueeze(dim=-1).int()\n",
    "        \n",
    "        now = torch.hstack([IDs, out_att, out_val])\n",
    "        all_preds.append(now)\n",
    "    \n",
    "    final = torch.vstack(all_preds)\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 38.10it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict_Homo(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 40.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 39.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:00<00:00, 42.84it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_val = predict_Homo(model, val_loader)\n",
    "pred_test = predict_Homo(model, test_loader)\n",
    "pred_unlab = predict_Homo(model, unlabel_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False], device='cuda:0')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~(data.train_mask + data.val_mask + data.test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 6.6071e-03, 2.1020e-02, 2.2766e-03, 1.5532e-03, 2.4628e-02,\n",
       "        4.3376e-02, 3.7304e-03, 8.8654e-04, 8.9592e-01, 1.3337e-01, 4.0112e-01,\n",
       "        2.2467e-01, 1.0778e-01, 1.4155e-02, 9.7556e-02, 3.3932e-03, 2.8701e-03,\n",
       "        3.1890e-03, 3.6642e-03, 8.2425e-03], device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.vstack([pred_train, pred_val, pred_test, pred_unlab]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 2.0000, 2.0000,  ..., 2.0000, 2.0000, 2.0000])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:,1:].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds).sort_values(0).set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.019247</td>\n",
       "      <td>0.094892</td>\n",
       "      <td>0.009801</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>0.135843</td>\n",
       "      <td>0.116122</td>\n",
       "      <td>0.042467</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.570979</td>\n",
       "      <td>0.074326</td>\n",
       "      <td>0.313263</td>\n",
       "      <td>0.238476</td>\n",
       "      <td>0.239006</td>\n",
       "      <td>0.016720</td>\n",
       "      <td>0.098719</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.008460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.006607</td>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.024628</td>\n",
       "      <td>0.043376</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.895922</td>\n",
       "      <td>0.133369</td>\n",
       "      <td>0.401117</td>\n",
       "      <td>0.224667</td>\n",
       "      <td>0.107776</td>\n",
       "      <td>0.014155</td>\n",
       "      <td>0.097556</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>0.008243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.036314</td>\n",
       "      <td>0.079916</td>\n",
       "      <td>0.020602</td>\n",
       "      <td>0.013223</td>\n",
       "      <td>0.150390</td>\n",
       "      <td>0.320887</td>\n",
       "      <td>0.083175</td>\n",
       "      <td>0.012250</td>\n",
       "      <td>0.283245</td>\n",
       "      <td>0.163897</td>\n",
       "      <td>0.295154</td>\n",
       "      <td>0.228652</td>\n",
       "      <td>0.152374</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.003285</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>0.009327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.042081</td>\n",
       "      <td>0.198455</td>\n",
       "      <td>0.034115</td>\n",
       "      <td>0.011695</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.086096</td>\n",
       "      <td>0.553848</td>\n",
       "      <td>0.049560</td>\n",
       "      <td>0.014615</td>\n",
       "      <td>0.070768</td>\n",
       "      <td>0.037889</td>\n",
       "      <td>0.176445</td>\n",
       "      <td>0.077414</td>\n",
       "      <td>0.032098</td>\n",
       "      <td>0.568712</td>\n",
       "      <td>0.009925</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.006272</td>\n",
       "      <td>0.010132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.280098</td>\n",
       "      <td>0.327038</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>0.075238</td>\n",
       "      <td>0.008564</td>\n",
       "      <td>0.235872</td>\n",
       "      <td>0.043952</td>\n",
       "      <td>0.010466</td>\n",
       "      <td>0.010376</td>\n",
       "      <td>0.202305</td>\n",
       "      <td>0.081352</td>\n",
       "      <td>0.092840</td>\n",
       "      <td>0.221243</td>\n",
       "      <td>0.011949</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.005850</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.002659</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>0.009303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946.0</th>\n",
       "      <td>0.016753</td>\n",
       "      <td>0.457933</td>\n",
       "      <td>0.130361</td>\n",
       "      <td>0.021919</td>\n",
       "      <td>0.002812</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.273795</td>\n",
       "      <td>0.057261</td>\n",
       "      <td>0.018667</td>\n",
       "      <td>0.334646</td>\n",
       "      <td>0.197165</td>\n",
       "      <td>0.211783</td>\n",
       "      <td>0.165527</td>\n",
       "      <td>0.018890</td>\n",
       "      <td>0.041695</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>0.012095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947.0</th>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.025765</td>\n",
       "      <td>0.272693</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.597483</td>\n",
       "      <td>0.080397</td>\n",
       "      <td>0.005501</td>\n",
       "      <td>0.301580</td>\n",
       "      <td>0.156060</td>\n",
       "      <td>0.265921</td>\n",
       "      <td>0.186380</td>\n",
       "      <td>0.017666</td>\n",
       "      <td>0.045506</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>0.005876</td>\n",
       "      <td>0.010839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948.0</th>\n",
       "      <td>0.066828</td>\n",
       "      <td>0.569370</td>\n",
       "      <td>0.066741</td>\n",
       "      <td>0.023436</td>\n",
       "      <td>0.008628</td>\n",
       "      <td>0.032728</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>0.051949</td>\n",
       "      <td>0.012819</td>\n",
       "      <td>0.320929</td>\n",
       "      <td>0.209535</td>\n",
       "      <td>0.205743</td>\n",
       "      <td>0.173799</td>\n",
       "      <td>0.022401</td>\n",
       "      <td>0.033467</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>0.005922</td>\n",
       "      <td>0.012032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949.0</th>\n",
       "      <td>0.337854</td>\n",
       "      <td>0.261688</td>\n",
       "      <td>0.061938</td>\n",
       "      <td>0.027689</td>\n",
       "      <td>0.012168</td>\n",
       "      <td>0.203495</td>\n",
       "      <td>0.041538</td>\n",
       "      <td>0.040495</td>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.383321</td>\n",
       "      <td>0.166394</td>\n",
       "      <td>0.185239</td>\n",
       "      <td>0.183937</td>\n",
       "      <td>0.012905</td>\n",
       "      <td>0.039795</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.011815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2950.0</th>\n",
       "      <td>0.038308</td>\n",
       "      <td>0.734318</td>\n",
       "      <td>0.027329</td>\n",
       "      <td>0.017829</td>\n",
       "      <td>0.005538</td>\n",
       "      <td>0.123785</td>\n",
       "      <td>0.030731</td>\n",
       "      <td>0.008731</td>\n",
       "      <td>0.013431</td>\n",
       "      <td>0.327788</td>\n",
       "      <td>0.163990</td>\n",
       "      <td>0.228655</td>\n",
       "      <td>0.192979</td>\n",
       "      <td>0.016335</td>\n",
       "      <td>0.037667</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.005380</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>0.011131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2951 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              1         2         3         4         5         6         7   \\\n",
       "0                                                                              \n",
       "0.0     0.019247  0.094892  0.009801  0.005316  0.135843  0.116122  0.042467   \n",
       "1.0     0.006607  0.021020  0.002277  0.001553  0.024628  0.043376  0.003730   \n",
       "2.0     0.036314  0.079916  0.020602  0.013223  0.150390  0.320887  0.083175   \n",
       "3.0     0.042081  0.198455  0.034115  0.011695  0.009535  0.086096  0.553848   \n",
       "4.0     0.280098  0.327038  0.008397  0.075238  0.008564  0.235872  0.043952   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "2946.0  0.016753  0.457933  0.130361  0.021919  0.002812  0.020500  0.273795   \n",
       "2947.0  0.001878  0.025765  0.272693  0.012658  0.001383  0.002242  0.597483   \n",
       "2948.0  0.066828  0.569370  0.066741  0.023436  0.008628  0.032728  0.167500   \n",
       "2949.0  0.337854  0.261688  0.061938  0.027689  0.012168  0.203495  0.041538   \n",
       "2950.0  0.038308  0.734318  0.027329  0.017829  0.005538  0.123785  0.030731   \n",
       "\n",
       "              8         9         10        11        12        13        14  \\\n",
       "0                                                                              \n",
       "0.0     0.005333  0.570979  0.074326  0.313263  0.238476  0.239006  0.016720   \n",
       "1.0     0.000887  0.895922  0.133369  0.401117  0.224667  0.107776  0.014155   \n",
       "2.0     0.012250  0.283245  0.163897  0.295154  0.228652  0.152374  0.013412   \n",
       "3.0     0.049560  0.014615  0.070768  0.037889  0.176445  0.077414  0.032098   \n",
       "4.0     0.010466  0.010376  0.202305  0.081352  0.092840  0.221243  0.011949   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "2946.0  0.057261  0.018667  0.334646  0.197165  0.211783  0.165527  0.018890   \n",
       "2947.0  0.080397  0.005501  0.301580  0.156060  0.265921  0.186380  0.017666   \n",
       "2948.0  0.051949  0.012819  0.320929  0.209535  0.205743  0.173799  0.022401   \n",
       "2949.0  0.040495  0.013138  0.383321  0.166394  0.185239  0.183937  0.012905   \n",
       "2950.0  0.008731  0.013431  0.327788  0.163990  0.228655  0.192979  0.016335   \n",
       "\n",
       "              15        16        17        18        19        20  \n",
       "0                                                                   \n",
       "0.0     0.098719  0.003096  0.002180  0.002781  0.002974  0.008460  \n",
       "1.0     0.097556  0.003393  0.002870  0.003189  0.003664  0.008243  \n",
       "2.0     0.122542  0.004595  0.003285  0.003033  0.003729  0.009327  \n",
       "3.0     0.568712  0.009925  0.005847  0.004499  0.006272  0.010132  \n",
       "4.0     0.365854  0.005850  0.003704  0.002659  0.002941  0.009303  \n",
       "...          ...       ...       ...       ...       ...       ...  \n",
       "2946.0  0.041695  0.004684  0.003745  0.004047  0.005722  0.012095  \n",
       "2947.0  0.045506  0.003548  0.003296  0.003328  0.005876  0.010839  \n",
       "2948.0  0.033467  0.005918  0.005149  0.005105  0.005922  0.012032  \n",
       "2949.0  0.039795  0.004598  0.003670  0.003181  0.005144  0.011815  \n",
       "2950.0  0.037667  0.005469  0.005380  0.004293  0.006314  0.011131  \n",
       "\n",
       "[2951 rows x 20 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.to_csv(args.save_dir + 'preds.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.to_csv(args.save_dir + 'train_state.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.019247</td>\n",
       "      <td>0.094892</td>\n",
       "      <td>0.009801</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>0.135843</td>\n",
       "      <td>0.116122</td>\n",
       "      <td>0.042467</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.570979</td>\n",
       "      <td>0.074326</td>\n",
       "      <td>0.313263</td>\n",
       "      <td>0.238476</td>\n",
       "      <td>0.239006</td>\n",
       "      <td>0.016720</td>\n",
       "      <td>0.098719</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.008460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.006607</td>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.024628</td>\n",
       "      <td>0.043376</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.895922</td>\n",
       "      <td>0.133369</td>\n",
       "      <td>0.401117</td>\n",
       "      <td>0.224667</td>\n",
       "      <td>0.107776</td>\n",
       "      <td>0.014155</td>\n",
       "      <td>0.097556</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>0.008243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.036314</td>\n",
       "      <td>0.079916</td>\n",
       "      <td>0.020602</td>\n",
       "      <td>0.013223</td>\n",
       "      <td>0.150390</td>\n",
       "      <td>0.320887</td>\n",
       "      <td>0.083175</td>\n",
       "      <td>0.012250</td>\n",
       "      <td>0.283245</td>\n",
       "      <td>0.163897</td>\n",
       "      <td>0.295154</td>\n",
       "      <td>0.228652</td>\n",
       "      <td>0.152374</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.003285</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>0.009327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.042081</td>\n",
       "      <td>0.198455</td>\n",
       "      <td>0.034115</td>\n",
       "      <td>0.011695</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.086096</td>\n",
       "      <td>0.553848</td>\n",
       "      <td>0.049560</td>\n",
       "      <td>0.014615</td>\n",
       "      <td>0.070768</td>\n",
       "      <td>0.037889</td>\n",
       "      <td>0.176445</td>\n",
       "      <td>0.077414</td>\n",
       "      <td>0.032098</td>\n",
       "      <td>0.568712</td>\n",
       "      <td>0.009925</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.006272</td>\n",
       "      <td>0.010132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.280098</td>\n",
       "      <td>0.327038</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>0.075238</td>\n",
       "      <td>0.008564</td>\n",
       "      <td>0.235872</td>\n",
       "      <td>0.043952</td>\n",
       "      <td>0.010466</td>\n",
       "      <td>0.010376</td>\n",
       "      <td>0.202305</td>\n",
       "      <td>0.081352</td>\n",
       "      <td>0.092840</td>\n",
       "      <td>0.221243</td>\n",
       "      <td>0.011949</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.005850</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.002659</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>0.009303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946.0</th>\n",
       "      <td>0.016753</td>\n",
       "      <td>0.457933</td>\n",
       "      <td>0.130361</td>\n",
       "      <td>0.021919</td>\n",
       "      <td>0.002812</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.273795</td>\n",
       "      <td>0.057261</td>\n",
       "      <td>0.018667</td>\n",
       "      <td>0.334646</td>\n",
       "      <td>0.197165</td>\n",
       "      <td>0.211783</td>\n",
       "      <td>0.165527</td>\n",
       "      <td>0.018890</td>\n",
       "      <td>0.041695</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>0.012095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947.0</th>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.025765</td>\n",
       "      <td>0.272693</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.597483</td>\n",
       "      <td>0.080397</td>\n",
       "      <td>0.005501</td>\n",
       "      <td>0.301580</td>\n",
       "      <td>0.156060</td>\n",
       "      <td>0.265921</td>\n",
       "      <td>0.186380</td>\n",
       "      <td>0.017666</td>\n",
       "      <td>0.045506</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>0.005876</td>\n",
       "      <td>0.010839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948.0</th>\n",
       "      <td>0.066828</td>\n",
       "      <td>0.569370</td>\n",
       "      <td>0.066741</td>\n",
       "      <td>0.023436</td>\n",
       "      <td>0.008628</td>\n",
       "      <td>0.032728</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>0.051949</td>\n",
       "      <td>0.012819</td>\n",
       "      <td>0.320929</td>\n",
       "      <td>0.209535</td>\n",
       "      <td>0.205743</td>\n",
       "      <td>0.173799</td>\n",
       "      <td>0.022401</td>\n",
       "      <td>0.033467</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>0.005922</td>\n",
       "      <td>0.012032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949.0</th>\n",
       "      <td>0.337854</td>\n",
       "      <td>0.261688</td>\n",
       "      <td>0.061938</td>\n",
       "      <td>0.027689</td>\n",
       "      <td>0.012168</td>\n",
       "      <td>0.203495</td>\n",
       "      <td>0.041538</td>\n",
       "      <td>0.040495</td>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.383321</td>\n",
       "      <td>0.166394</td>\n",
       "      <td>0.185239</td>\n",
       "      <td>0.183937</td>\n",
       "      <td>0.012905</td>\n",
       "      <td>0.039795</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.011815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2950.0</th>\n",
       "      <td>0.038308</td>\n",
       "      <td>0.734318</td>\n",
       "      <td>0.027329</td>\n",
       "      <td>0.017829</td>\n",
       "      <td>0.005538</td>\n",
       "      <td>0.123785</td>\n",
       "      <td>0.030731</td>\n",
       "      <td>0.008731</td>\n",
       "      <td>0.013431</td>\n",
       "      <td>0.327788</td>\n",
       "      <td>0.163990</td>\n",
       "      <td>0.228655</td>\n",
       "      <td>0.192979</td>\n",
       "      <td>0.016335</td>\n",
       "      <td>0.037667</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.005380</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>0.011131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2951 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               1         2         3         4         5         6         7  \\\n",
       "0                                                                              \n",
       "0.0     0.019247  0.094892  0.009801  0.005316  0.135843  0.116122  0.042467   \n",
       "1.0     0.006607  0.021020  0.002277  0.001553  0.024628  0.043376  0.003730   \n",
       "2.0     0.036314  0.079916  0.020602  0.013223  0.150390  0.320887  0.083175   \n",
       "3.0     0.042081  0.198455  0.034115  0.011695  0.009535  0.086096  0.553848   \n",
       "4.0     0.280098  0.327038  0.008397  0.075238  0.008564  0.235872  0.043952   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "2946.0  0.016753  0.457933  0.130361  0.021919  0.002812  0.020500  0.273795   \n",
       "2947.0  0.001878  0.025765  0.272693  0.012658  0.001383  0.002242  0.597483   \n",
       "2948.0  0.066828  0.569370  0.066741  0.023436  0.008628  0.032728  0.167500   \n",
       "2949.0  0.337854  0.261688  0.061938  0.027689  0.012168  0.203495  0.041538   \n",
       "2950.0  0.038308  0.734318  0.027329  0.017829  0.005538  0.123785  0.030731   \n",
       "\n",
       "               8         9        10        11        12        13        14  \\\n",
       "0                                                                              \n",
       "0.0     0.005333  0.570979  0.074326  0.313263  0.238476  0.239006  0.016720   \n",
       "1.0     0.000887  0.895922  0.133369  0.401117  0.224667  0.107776  0.014155   \n",
       "2.0     0.012250  0.283245  0.163897  0.295154  0.228652  0.152374  0.013412   \n",
       "3.0     0.049560  0.014615  0.070768  0.037889  0.176445  0.077414  0.032098   \n",
       "4.0     0.010466  0.010376  0.202305  0.081352  0.092840  0.221243  0.011949   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "2946.0  0.057261  0.018667  0.334646  0.197165  0.211783  0.165527  0.018890   \n",
       "2947.0  0.080397  0.005501  0.301580  0.156060  0.265921  0.186380  0.017666   \n",
       "2948.0  0.051949  0.012819  0.320929  0.209535  0.205743  0.173799  0.022401   \n",
       "2949.0  0.040495  0.013138  0.383321  0.166394  0.185239  0.183937  0.012905   \n",
       "2950.0  0.008731  0.013431  0.327788  0.163990  0.228655  0.192979  0.016335   \n",
       "\n",
       "              15        16        17        18        19        20  \n",
       "0                                                                   \n",
       "0.0     0.098719  0.003096  0.002180  0.002781  0.002974  0.008460  \n",
       "1.0     0.097556  0.003393  0.002870  0.003189  0.003664  0.008243  \n",
       "2.0     0.122542  0.004595  0.003285  0.003033  0.003729  0.009327  \n",
       "3.0     0.568712  0.009925  0.005847  0.004499  0.006272  0.010132  \n",
       "4.0     0.365854  0.005850  0.003704  0.002659  0.002941  0.009303  \n",
       "...          ...       ...       ...       ...       ...       ...  \n",
       "2946.0  0.041695  0.004684  0.003745  0.004047  0.005722  0.012095  \n",
       "2947.0  0.045506  0.003548  0.003296  0.003328  0.005876  0.010839  \n",
       "2948.0  0.033467  0.005918  0.005149  0.005105  0.005922  0.012032  \n",
       "2949.0  0.039795  0.004598  0.003670  0.003181  0.005144  0.011815  \n",
       "2950.0  0.037667  0.005469  0.005380  0.004293  0.006314  0.011131  \n",
       "\n",
       "[2951 rows x 20 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pd.read_csv(args.save_dir + 'preds.csv', sep='\\t', index_col='0')\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.tensor(np.array(preds)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_confusion_matrix(y, y_pred, k=3):\n",
    "    dim = y.shape[-1]\n",
    "    y = y.topk(k=k, axis=1)[1]\n",
    "    y_pred = y_pred.topk(k=k, axis=1)[1]\n",
    "    conf = np.zeros((dim, dim))\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            conf = np.add(conf, confusion_matrix(y[:,i], y_pred[:,j], labels = range(dim)))\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ATT_conf = confusion_matrix(data.y[(data.att_lab) * data.test_mask][:,:9].argmax(axis=1).cpu(), \n",
    "                                 pred[(data.att_lab) * data.test_mask][:,:9].argmax(axis=1).cpu(), labels = range(9))\n",
    "test_VAL_conf = confusion_matrix(data.y[(data.val_lab) * data.test_mask][:,9:].argmax(axis=1).cpu(), \n",
    "                                 pred[(data.val_lab) * data.test_mask][:,9:].argmax(axis=1).cpu(), labels=range(11))\n",
    "test_VAL_conf_k = (top_k_confusion_matrix(data.y[(data.val_lab) * data.test_mask][:,9:].cpu(),  \n",
    "                                 pred[(data.val_lab) * data.test_mask][:,9:].cpu(),3)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 64,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0, 144,   0,   0,   0,   1,   0,   0,   0],\n",
       "       [  0,   0,  10,   0,   0,   0,   0,   1,   0],\n",
       "       [  2,   0,   0,  16,   0,   0,   0,   0,   0],\n",
       "       [  0,   1,   0,   0,  71,   0,   0,   0,   0],\n",
       "       [  0,   2,   0,   0,   0, 100,   0,   0,   0],\n",
       "       [  0,   2,   0,   0,   2,   0,  73,   0,   0],\n",
       "       [  0,   0,   2,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  12]], dtype=int64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ATT_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3, 19,  0,  1,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  5, 20,  3,  0,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  2, 28,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  2,  3,  1,  0, 36,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_VAL_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 90,  54,  47,  43,   0,  48,   0,   0,   0,   0,   0],\n",
       "       [ 50,  85,  73,  60,   0,  41,   0,   0,   0,   0,   0],\n",
       "       [ 54,  79, 116,  88,   0,  70,   1,   0,   0,   0,   0],\n",
       "       [ 55,  73,  89, 117,   0,  65,   0,   0,   0,   0,   0],\n",
       "       [  0,   3,   3,   3,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 45,  36,  70,  70,   0,  87,   1,   0,   0,   0,   0],\n",
       "       [  0,   0,   1,   0,   0,   2,   2,   0,   0,   1,   0],\n",
       "       [  0,   0,   0,   0,   0,   1,   1,   0,   0,   1,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   1,   1,   0,   0,   1,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_VAL_conf_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ATT_conf = confusion_matrix(data.y[(data.att_lab) * data.val_mask][:,:9].argmax(axis=1).cpu(), \n",
    "                                pred[(data.att_lab) * data.val_mask][:,:9].argmax(axis=1).cpu())\n",
    "val_VAL_conf = confusion_matrix(data.y[(data.val_lab) * data.val_mask][:,9:].argmax(axis=1).cpu(), \n",
    "                                 pred[(data.val_lab) * data.val_mask][:,9:].argmax(axis=1).cpu(), labels=range(11))\n",
    "val_VAL_conf_k = (top_k_confusion_matrix(data.y[(data.val_lab) * data.val_mask][:,9:].cpu(),  \n",
    "                                 pred[(data.val_lab) * data.val_mask][:,9:].cpu(),3)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,   1,   0,   0,   0,   1,   0,   0,   0],\n",
       "       [  1, 144,   0,   0,   0,   1,   0,   0,   0],\n",
       "       [  0,   0,   8,   0,   0,   0,   0,   0,   0],\n",
       "       [  1,   0,   0,  21,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,  75,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0, 103,   0,   0,   0],\n",
       "       [  0,   1,   0,   0,   0,   0,  61,   0,   0],\n",
       "       [  0,   0,   2,   0,   0,   0,   0,   3,   0],\n",
       "       [  0,   1,   0,   0,   1,   0,   0,   0,   8]], dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ATT_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65,  1,  2,  0,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 1,  9,  2,  5,  0,  2,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 24,  3,  0,  4,  0,  0,  0,  0,  0],\n",
       "       [ 1,  1,  0, 31,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 7,  4,  4,  1,  0, 34,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_VAL_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103,  51,  61,  57,   0,  52,   0,   0,   0,   0,   0],\n",
       "       [ 58,  83,  63,  71,   3,  43,   0,   0,   0,   0,   0],\n",
       "       [ 53,  48, 120,  87,   0,  85,   0,   0,   0,   0,   0],\n",
       "       [ 65,  71, 102, 118,   3,  67,   0,   0,   0,   0,   0],\n",
       "       [  0,   8,   5,   8,   3,   0,   0,   0,   0,   0,   0],\n",
       "       [ 48,  36,  90,  67,   0,  98,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_VAL_conf_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(val_ATT_conf),pd.DataFrame(test_ATT_conf)],axis=1).to_csv(args.save_dir+'confusion_matrix_ATT.csv')\n",
    "pd.concat([pd.DataFrame(val_VAL_conf),pd.DataFrame(test_VAL_conf)],axis=1).to_csv(args.save_dir+'confusion_matrix_VAL.csv')\n",
    "pd.concat([pd.DataFrame(val_VAL_conf_k),pd.DataFrame(test_VAL_conf_k)],axis=1).to_csv(args.save_dir+'confusion_matrix_VAL_k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_metrics(confusion_matrix, classes):\n",
    "    '''\n",
    "    Compute the per class precision, recall, and F1 for all the classes\n",
    "    \n",
    "    Args:\n",
    "    confusion_matrix (np.ndarry) with shape of (n_classes,n_classes): a confusion matrix of interest\n",
    "    classes (list of str) with shape (n_classes,): The names of classes\n",
    "    \n",
    "    Returns:\n",
    "    metrics_dict (dictionary): a dictionary that records the per class metrics\n",
    "    '''\n",
    "    num_class = confusion_matrix.shape[0]\n",
    "    metrics_dict = {}\n",
    "    for i in range(num_class):\n",
    "        key = classes[i]\n",
    "        temp_dict = {}\n",
    "        row = confusion_matrix[i,:]\n",
    "        col = confusion_matrix[:,i]\n",
    "        val = confusion_matrix[i,i]\n",
    "        precision = val/(row.sum()+0.000000001)\n",
    "        recall = val/(col.sum()+0.000000001)\n",
    "        F1 = 2*(precision*recall)/(precision+recall+0.000000001)\n",
    "        temp_dict['precision'] = precision\n",
    "        temp_dict['recall'] = recall\n",
    "        temp_dict['F1'] = F1\n",
    "        metrics_dict[key] = temp_dict\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_metrics_k(confusion_matrix, classes, k=3):\n",
    "    '''\n",
    "    Compute the per class precision, recall, and F1 for all the classes\n",
    "    \n",
    "    Args:\n",
    "    confusion_matrix (np.ndarry) with shape of (n_classes,n_classes): a confusion matrix of interest\n",
    "    classes (list of str) with shape (n_classes,): The names of classes\n",
    "    \n",
    "    Returns:\n",
    "    metrics_dict (dictionary): a dictionary that records the per class metrics\n",
    "    '''\n",
    "    num_class = confusion_matrix.shape[0]\n",
    "    metrics_dict = {}\n",
    "    for i in range(num_class):\n",
    "        key = classes[i]\n",
    "        temp_dict = {}\n",
    "        row = confusion_matrix[i,:]\n",
    "        col = confusion_matrix[:,i]\n",
    "        val = confusion_matrix[i,i]\n",
    "        precision = val*k/(row.sum()+0.000000001)\n",
    "        recall = val*k/(col.sum()+0.000000001)\n",
    "        F1 = 2*(precision*recall)/(precision+recall+0.000000001)\n",
    "        temp_dict['precision'] = precision\n",
    "        temp_dict['recall'] = recall\n",
    "        temp_dict['F1'] = F1\n",
    "        metrics_dict[key] = temp_dict\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Criterion i', 'Criterion ii', 'Criterion iii', 'Criterion iv', 'Criterion v', 'Criterion vi', \n",
    "              'Criterion vii', 'Criterion viii', 'Criterion ix', 'Criterion x', 'Others']\n",
    "categories = ['Building Elements',\n",
    " 'Urban Form Elements',\n",
    " 'Gastronomy',\n",
    " 'Interior Scenery',\n",
    " 'Natural Features and Land-scape Scenery',\n",
    " 'Monuments and Buildings',\n",
    " 'Peoples Activity and Association',\n",
    " 'Artifact Products',\n",
    " 'Urban Scenery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {}\n",
    "metrics_dict['test_ATT'] = per_class_metrics(test_ATT_conf, categories)\n",
    "metrics_dict['val_ATT'] = per_class_metrics(val_ATT_conf, categories)\n",
    "metrics_dict['test_VAL'] = per_class_metrics(test_VAL_conf, classes)\n",
    "metrics_dict['val_VAL'] = per_class_metrics(val_VAL_conf, classes)\n",
    "metrics_dict['test_VAL_k'] = per_class_metrics_k(test_VAL_conf_k, classes)\n",
    "metrics_dict['val_VAL_k'] = per_class_metrics_k(val_VAL_conf_k, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame.from_dict({(i,j): metrics_dict[i][j] \n",
    "                           for i in metrics_dict.keys() \n",
    "                           for j in metrics_dict[i].keys()},\n",
    "                       orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(args.save_dir+'per_class_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = GNNExplainer(model, epochs=200, return_type='log_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_features(model, loader):\n",
    "    model.eval()\n",
    "    seed_everything(args.seed)\n",
    "    explainer = GNNExplainer(model, epochs=200, return_type='log_prob')\n",
    "    all_preds = {}\n",
    "    \n",
    "    for batch in tqdm(loader):\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch.batch_size\n",
    "        edge_index = to_undirected(batch.edge_index)\n",
    "        for node_idx in tqdm(range(batch_size)):\n",
    "            node_feat_mask, _ = explainer.explain_node(node_idx, batch.x, edge_index)\n",
    "            all_preds[batch.n_id[node_idx]] = node_feat_mask\n",
    "    \n",
    "    out = torch.vstack([torch.hstack([i, all_preds[i]]) for i in all_preds.keys()])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_explain = explain_features(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_explain = explain_features(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_explain = explain_features(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "695"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explain_df = pd.DataFrame(train_explain.cpu()).sort_values(0).set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_explain_df = pd.DataFrame(val_explain.cpu()).sort_values(0).set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_explain_df = pd.DataFrame(test_explain.cpu()).sort_values(0).set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_explain_df = pd.concat([train_explain_df, val_explain_df, test_explain_df]).reset_index().rename(columns={0:'ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_explain_df['ID'] = all_explain_df['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1744</th>\n",
       "      <th>1745</th>\n",
       "      <th>1746</th>\n",
       "      <th>1747</th>\n",
       "      <th>1748</th>\n",
       "      <th>1749</th>\n",
       "      <th>1750</th>\n",
       "      <th>1751</th>\n",
       "      <th>1752</th>\n",
       "      <th>1753</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.196204</td>\n",
       "      <td>0.125538</td>\n",
       "      <td>0.829300</td>\n",
       "      <td>0.143392</td>\n",
       "      <td>0.785905</td>\n",
       "      <td>0.712615</td>\n",
       "      <td>0.207511</td>\n",
       "      <td>0.859502</td>\n",
       "      <td>0.163500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156079</td>\n",
       "      <td>0.748273</td>\n",
       "      <td>0.176544</td>\n",
       "      <td>0.884632</td>\n",
       "      <td>0.872025</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.121349</td>\n",
       "      <td>0.837967</td>\n",
       "      <td>0.172734</td>\n",
       "      <td>0.165242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>0.827174</td>\n",
       "      <td>0.194986</td>\n",
       "      <td>0.172719</td>\n",
       "      <td>0.177570</td>\n",
       "      <td>0.845929</td>\n",
       "      <td>0.147856</td>\n",
       "      <td>0.840769</td>\n",
       "      <td>0.864428</td>\n",
       "      <td>0.818672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182639</td>\n",
       "      <td>0.885735</td>\n",
       "      <td>0.124151</td>\n",
       "      <td>0.852755</td>\n",
       "      <td>0.695044</td>\n",
       "      <td>0.854522</td>\n",
       "      <td>0.852328</td>\n",
       "      <td>0.157640</td>\n",
       "      <td>0.222541</td>\n",
       "      <td>0.857266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>0.691643</td>\n",
       "      <td>0.848522</td>\n",
       "      <td>0.114636</td>\n",
       "      <td>0.353214</td>\n",
       "      <td>0.160089</td>\n",
       "      <td>0.834555</td>\n",
       "      <td>0.150483</td>\n",
       "      <td>0.319174</td>\n",
       "      <td>0.185085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595378</td>\n",
       "      <td>0.784241</td>\n",
       "      <td>0.736962</td>\n",
       "      <td>0.490071</td>\n",
       "      <td>0.190833</td>\n",
       "      <td>0.855286</td>\n",
       "      <td>0.818733</td>\n",
       "      <td>0.247251</td>\n",
       "      <td>0.848224</td>\n",
       "      <td>0.201359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>0.081797</td>\n",
       "      <td>0.831367</td>\n",
       "      <td>0.194305</td>\n",
       "      <td>0.812363</td>\n",
       "      <td>0.825632</td>\n",
       "      <td>0.812793</td>\n",
       "      <td>0.174359</td>\n",
       "      <td>0.155627</td>\n",
       "      <td>0.843013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705959</td>\n",
       "      <td>0.845373</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.529126</td>\n",
       "      <td>0.797624</td>\n",
       "      <td>0.228296</td>\n",
       "      <td>0.154702</td>\n",
       "      <td>0.144122</td>\n",
       "      <td>0.828859</td>\n",
       "      <td>0.158463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>0.111419</td>\n",
       "      <td>0.843864</td>\n",
       "      <td>0.198652</td>\n",
       "      <td>0.799678</td>\n",
       "      <td>0.837694</td>\n",
       "      <td>0.840953</td>\n",
       "      <td>0.154347</td>\n",
       "      <td>0.174330</td>\n",
       "      <td>0.853294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794225</td>\n",
       "      <td>0.189007</td>\n",
       "      <td>0.818096</td>\n",
       "      <td>0.402568</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.223977</td>\n",
       "      <td>0.119837</td>\n",
       "      <td>0.139210</td>\n",
       "      <td>0.132350</td>\n",
       "      <td>0.845993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>2931</td>\n",
       "      <td>0.159106</td>\n",
       "      <td>0.157753</td>\n",
       "      <td>0.809235</td>\n",
       "      <td>0.825924</td>\n",
       "      <td>0.159197</td>\n",
       "      <td>0.699754</td>\n",
       "      <td>0.208735</td>\n",
       "      <td>0.274189</td>\n",
       "      <td>0.154435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133370</td>\n",
       "      <td>0.306523</td>\n",
       "      <td>0.172947</td>\n",
       "      <td>0.130974</td>\n",
       "      <td>0.119893</td>\n",
       "      <td>0.154302</td>\n",
       "      <td>0.198281</td>\n",
       "      <td>0.157798</td>\n",
       "      <td>0.112518</td>\n",
       "      <td>0.194006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>2940</td>\n",
       "      <td>0.754659</td>\n",
       "      <td>0.843647</td>\n",
       "      <td>0.196224</td>\n",
       "      <td>0.774877</td>\n",
       "      <td>0.152437</td>\n",
       "      <td>0.838377</td>\n",
       "      <td>0.161009</td>\n",
       "      <td>0.186751</td>\n",
       "      <td>0.170173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.128474</td>\n",
       "      <td>0.163081</td>\n",
       "      <td>0.146479</td>\n",
       "      <td>0.138082</td>\n",
       "      <td>0.169267</td>\n",
       "      <td>0.134003</td>\n",
       "      <td>0.153524</td>\n",
       "      <td>0.173947</td>\n",
       "      <td>0.164563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>2944</td>\n",
       "      <td>0.129716</td>\n",
       "      <td>0.165112</td>\n",
       "      <td>0.354632</td>\n",
       "      <td>0.162761</td>\n",
       "      <td>0.140983</td>\n",
       "      <td>0.779570</td>\n",
       "      <td>0.120060</td>\n",
       "      <td>0.146458</td>\n",
       "      <td>0.752690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124579</td>\n",
       "      <td>0.208815</td>\n",
       "      <td>0.149776</td>\n",
       "      <td>0.856512</td>\n",
       "      <td>0.829717</td>\n",
       "      <td>0.829908</td>\n",
       "      <td>0.809690</td>\n",
       "      <td>0.115095</td>\n",
       "      <td>0.838183</td>\n",
       "      <td>0.152334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>2947</td>\n",
       "      <td>0.851133</td>\n",
       "      <td>0.199572</td>\n",
       "      <td>0.203811</td>\n",
       "      <td>0.165923</td>\n",
       "      <td>0.378331</td>\n",
       "      <td>0.161169</td>\n",
       "      <td>0.863782</td>\n",
       "      <td>0.885180</td>\n",
       "      <td>0.851160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819384</td>\n",
       "      <td>0.868243</td>\n",
       "      <td>0.195708</td>\n",
       "      <td>0.856170</td>\n",
       "      <td>0.232315</td>\n",
       "      <td>0.846350</td>\n",
       "      <td>0.180403</td>\n",
       "      <td>0.143601</td>\n",
       "      <td>0.826127</td>\n",
       "      <td>0.156721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>2948</td>\n",
       "      <td>0.147934</td>\n",
       "      <td>0.151329</td>\n",
       "      <td>0.833328</td>\n",
       "      <td>0.824868</td>\n",
       "      <td>0.168115</td>\n",
       "      <td>0.659532</td>\n",
       "      <td>0.382278</td>\n",
       "      <td>0.209036</td>\n",
       "      <td>0.166440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156321</td>\n",
       "      <td>0.263435</td>\n",
       "      <td>0.163073</td>\n",
       "      <td>0.713209</td>\n",
       "      <td>0.863007</td>\n",
       "      <td>0.215332</td>\n",
       "      <td>0.832675</td>\n",
       "      <td>0.179714</td>\n",
       "      <td>0.866486</td>\n",
       "      <td>0.159478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1751 rows × 1754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID         1         2         3         4         5         6  \\\n",
       "0        1  0.196204  0.125538  0.829300  0.143392  0.785905  0.712615   \n",
       "1       28  0.827174  0.194986  0.172719  0.177570  0.845929  0.147856   \n",
       "2       30  0.691643  0.848522  0.114636  0.353214  0.160089  0.834555   \n",
       "3       34  0.081797  0.831367  0.194305  0.812363  0.825632  0.812793   \n",
       "4       35  0.111419  0.843864  0.198652  0.799678  0.837694  0.840953   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "1746  2931  0.159106  0.157753  0.809235  0.825924  0.159197  0.699754   \n",
       "1747  2940  0.754659  0.843647  0.196224  0.774877  0.152437  0.838377   \n",
       "1748  2944  0.129716  0.165112  0.354632  0.162761  0.140983  0.779570   \n",
       "1749  2947  0.851133  0.199572  0.203811  0.165923  0.378331  0.161169   \n",
       "1750  2948  0.147934  0.151329  0.833328  0.824868  0.168115  0.659532   \n",
       "\n",
       "             7         8         9  ...      1744      1745      1746  \\\n",
       "0     0.207511  0.859502  0.163500  ...  0.156079  0.748273  0.176544   \n",
       "1     0.840769  0.864428  0.818672  ...  0.182639  0.885735  0.124151   \n",
       "2     0.150483  0.319174  0.185085  ...  0.595378  0.784241  0.736962   \n",
       "3     0.174359  0.155627  0.843013  ...  0.705959  0.845373  0.825102   \n",
       "4     0.154347  0.174330  0.853294  ...  0.794225  0.189007  0.818096   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1746  0.208735  0.274189  0.154435  ...  0.133370  0.306523  0.172947   \n",
       "1747  0.161009  0.186751  0.170173  ...  0.146667  0.128474  0.163081   \n",
       "1748  0.120060  0.146458  0.752690  ...  0.124579  0.208815  0.149776   \n",
       "1749  0.863782  0.885180  0.851160  ...  0.819384  0.868243  0.195708   \n",
       "1750  0.382278  0.209036  0.166440  ...  0.156321  0.263435  0.163073   \n",
       "\n",
       "          1747      1748      1749      1750      1751      1752      1753  \n",
       "0     0.884632  0.872025  0.150538  0.121349  0.837967  0.172734  0.165242  \n",
       "1     0.852755  0.695044  0.854522  0.852328  0.157640  0.222541  0.857266  \n",
       "2     0.490071  0.190833  0.855286  0.818733  0.247251  0.848224  0.201359  \n",
       "3     0.529126  0.797624  0.228296  0.154702  0.144122  0.828859  0.158463  \n",
       "4     0.402568  0.834414  0.223977  0.119837  0.139210  0.132350  0.845993  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1746  0.130974  0.119893  0.154302  0.198281  0.157798  0.112518  0.194006  \n",
       "1747  0.146479  0.138082  0.169267  0.134003  0.153524  0.173947  0.164563  \n",
       "1748  0.856512  0.829717  0.829908  0.809690  0.115095  0.838183  0.152334  \n",
       "1749  0.856170  0.232315  0.846350  0.180403  0.143601  0.826127  0.156721  \n",
       "1750  0.713209  0.863007  0.215332  0.832675  0.179714  0.866486  0.159478  \n",
       "\n",
       "[1751 rows x 1754 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_explain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_explain_df.to_csv(args.save_dir + 'all_explain.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explain_df.to_csv(args.save_dir + 'train_explain.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_explain_df = pd.read_csv(args.save_dir + 'all_explain.csv', sep='\\t', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explain_df = pd.read_csv(args.save_dir + 'train_explain.csv', sep='\\t', index_col = '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1744</th>\n",
       "      <th>1745</th>\n",
       "      <th>1746</th>\n",
       "      <th>1747</th>\n",
       "      <th>1748</th>\n",
       "      <th>1749</th>\n",
       "      <th>1750</th>\n",
       "      <th>1751</th>\n",
       "      <th>1752</th>\n",
       "      <th>1753</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.196204</td>\n",
       "      <td>0.125538</td>\n",
       "      <td>0.829300</td>\n",
       "      <td>0.143392</td>\n",
       "      <td>0.785905</td>\n",
       "      <td>0.712615</td>\n",
       "      <td>0.207511</td>\n",
       "      <td>0.859502</td>\n",
       "      <td>0.163500</td>\n",
       "      <td>0.845775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156079</td>\n",
       "      <td>0.748273</td>\n",
       "      <td>0.176544</td>\n",
       "      <td>0.884632</td>\n",
       "      <td>0.872025</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.121349</td>\n",
       "      <td>0.837967</td>\n",
       "      <td>0.172734</td>\n",
       "      <td>0.165242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28.0</th>\n",
       "      <td>0.827174</td>\n",
       "      <td>0.194986</td>\n",
       "      <td>0.172719</td>\n",
       "      <td>0.177570</td>\n",
       "      <td>0.845929</td>\n",
       "      <td>0.147856</td>\n",
       "      <td>0.840769</td>\n",
       "      <td>0.864428</td>\n",
       "      <td>0.818672</td>\n",
       "      <td>0.171176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182639</td>\n",
       "      <td>0.885735</td>\n",
       "      <td>0.124151</td>\n",
       "      <td>0.852755</td>\n",
       "      <td>0.695044</td>\n",
       "      <td>0.854522</td>\n",
       "      <td>0.852328</td>\n",
       "      <td>0.157639</td>\n",
       "      <td>0.222541</td>\n",
       "      <td>0.857266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30.0</th>\n",
       "      <td>0.691643</td>\n",
       "      <td>0.848522</td>\n",
       "      <td>0.114636</td>\n",
       "      <td>0.353214</td>\n",
       "      <td>0.160089</td>\n",
       "      <td>0.834555</td>\n",
       "      <td>0.150483</td>\n",
       "      <td>0.319174</td>\n",
       "      <td>0.185085</td>\n",
       "      <td>0.156366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595378</td>\n",
       "      <td>0.784241</td>\n",
       "      <td>0.736962</td>\n",
       "      <td>0.490071</td>\n",
       "      <td>0.190833</td>\n",
       "      <td>0.855286</td>\n",
       "      <td>0.818733</td>\n",
       "      <td>0.247251</td>\n",
       "      <td>0.848224</td>\n",
       "      <td>0.201359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34.0</th>\n",
       "      <td>0.081797</td>\n",
       "      <td>0.831367</td>\n",
       "      <td>0.194305</td>\n",
       "      <td>0.812363</td>\n",
       "      <td>0.825632</td>\n",
       "      <td>0.812793</td>\n",
       "      <td>0.174359</td>\n",
       "      <td>0.155627</td>\n",
       "      <td>0.843013</td>\n",
       "      <td>0.166483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705959</td>\n",
       "      <td>0.845373</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.529126</td>\n",
       "      <td>0.797624</td>\n",
       "      <td>0.228296</td>\n",
       "      <td>0.154702</td>\n",
       "      <td>0.144122</td>\n",
       "      <td>0.828859</td>\n",
       "      <td>0.158463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35.0</th>\n",
       "      <td>0.111419</td>\n",
       "      <td>0.843864</td>\n",
       "      <td>0.198652</td>\n",
       "      <td>0.799678</td>\n",
       "      <td>0.837694</td>\n",
       "      <td>0.840953</td>\n",
       "      <td>0.154347</td>\n",
       "      <td>0.174330</td>\n",
       "      <td>0.853294</td>\n",
       "      <td>0.165892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794225</td>\n",
       "      <td>0.189007</td>\n",
       "      <td>0.818096</td>\n",
       "      <td>0.402568</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.223977</td>\n",
       "      <td>0.119837</td>\n",
       "      <td>0.139210</td>\n",
       "      <td>0.132350</td>\n",
       "      <td>0.845993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874.0</th>\n",
       "      <td>0.450020</td>\n",
       "      <td>0.844422</td>\n",
       "      <td>0.133640</td>\n",
       "      <td>0.720120</td>\n",
       "      <td>0.173296</td>\n",
       "      <td>0.817665</td>\n",
       "      <td>0.135886</td>\n",
       "      <td>0.261146</td>\n",
       "      <td>0.162865</td>\n",
       "      <td>0.162220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205478</td>\n",
       "      <td>0.535880</td>\n",
       "      <td>0.134802</td>\n",
       "      <td>0.283869</td>\n",
       "      <td>0.143926</td>\n",
       "      <td>0.317814</td>\n",
       "      <td>0.848387</td>\n",
       "      <td>0.265529</td>\n",
       "      <td>0.859265</td>\n",
       "      <td>0.187919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925.0</th>\n",
       "      <td>0.167516</td>\n",
       "      <td>0.186201</td>\n",
       "      <td>0.841639</td>\n",
       "      <td>0.824562</td>\n",
       "      <td>0.157850</td>\n",
       "      <td>0.172615</td>\n",
       "      <td>0.140802</td>\n",
       "      <td>0.211073</td>\n",
       "      <td>0.157474</td>\n",
       "      <td>0.860990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209529</td>\n",
       "      <td>0.257052</td>\n",
       "      <td>0.176212</td>\n",
       "      <td>0.472837</td>\n",
       "      <td>0.834740</td>\n",
       "      <td>0.192689</td>\n",
       "      <td>0.817822</td>\n",
       "      <td>0.174657</td>\n",
       "      <td>0.846631</td>\n",
       "      <td>0.158055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927.0</th>\n",
       "      <td>0.812694</td>\n",
       "      <td>0.147226</td>\n",
       "      <td>0.172331</td>\n",
       "      <td>0.167961</td>\n",
       "      <td>0.380342</td>\n",
       "      <td>0.158814</td>\n",
       "      <td>0.832519</td>\n",
       "      <td>0.848071</td>\n",
       "      <td>0.859626</td>\n",
       "      <td>0.220389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820697</td>\n",
       "      <td>0.830039</td>\n",
       "      <td>0.176437</td>\n",
       "      <td>0.883862</td>\n",
       "      <td>0.273098</td>\n",
       "      <td>0.873988</td>\n",
       "      <td>0.185938</td>\n",
       "      <td>0.142743</td>\n",
       "      <td>0.867638</td>\n",
       "      <td>0.139545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934.0</th>\n",
       "      <td>0.152617</td>\n",
       "      <td>0.184280</td>\n",
       "      <td>0.823303</td>\n",
       "      <td>0.819465</td>\n",
       "      <td>0.178014</td>\n",
       "      <td>0.675692</td>\n",
       "      <td>0.396751</td>\n",
       "      <td>0.341538</td>\n",
       "      <td>0.176018</td>\n",
       "      <td>0.842607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194068</td>\n",
       "      <td>0.294293</td>\n",
       "      <td>0.202755</td>\n",
       "      <td>0.282282</td>\n",
       "      <td>0.856819</td>\n",
       "      <td>0.186665</td>\n",
       "      <td>0.843954</td>\n",
       "      <td>0.163481</td>\n",
       "      <td>0.869356</td>\n",
       "      <td>0.173763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2950.0</th>\n",
       "      <td>0.164907</td>\n",
       "      <td>0.181751</td>\n",
       "      <td>0.827862</td>\n",
       "      <td>0.838803</td>\n",
       "      <td>0.168541</td>\n",
       "      <td>0.110738</td>\n",
       "      <td>0.456622</td>\n",
       "      <td>0.255844</td>\n",
       "      <td>0.156731</td>\n",
       "      <td>0.848088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189603</td>\n",
       "      <td>0.232965</td>\n",
       "      <td>0.161803</td>\n",
       "      <td>0.299540</td>\n",
       "      <td>0.852363</td>\n",
       "      <td>0.199494</td>\n",
       "      <td>0.818966</td>\n",
       "      <td>0.166747</td>\n",
       "      <td>0.871036</td>\n",
       "      <td>0.176611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>361 rows × 1753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               1         2         3         4         5         6         7  \\\n",
       "0                                                                              \n",
       "1.0     0.196204  0.125538  0.829300  0.143392  0.785905  0.712615  0.207511   \n",
       "28.0    0.827174  0.194986  0.172719  0.177570  0.845929  0.147856  0.840769   \n",
       "30.0    0.691643  0.848522  0.114636  0.353214  0.160089  0.834555  0.150483   \n",
       "34.0    0.081797  0.831367  0.194305  0.812363  0.825632  0.812793  0.174359   \n",
       "35.0    0.111419  0.843864  0.198652  0.799678  0.837694  0.840953  0.154347   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "2874.0  0.450020  0.844422  0.133640  0.720120  0.173296  0.817665  0.135886   \n",
       "2925.0  0.167516  0.186201  0.841639  0.824562  0.157850  0.172615  0.140802   \n",
       "2927.0  0.812694  0.147226  0.172331  0.167961  0.380342  0.158814  0.832519   \n",
       "2934.0  0.152617  0.184280  0.823303  0.819465  0.178014  0.675692  0.396751   \n",
       "2950.0  0.164907  0.181751  0.827862  0.838803  0.168541  0.110738  0.456622   \n",
       "\n",
       "               8         9        10  ...      1744      1745      1746  \\\n",
       "0                                     ...                                 \n",
       "1.0     0.859502  0.163500  0.845775  ...  0.156079  0.748273  0.176544   \n",
       "28.0    0.864428  0.818672  0.171176  ...  0.182639  0.885735  0.124151   \n",
       "30.0    0.319174  0.185085  0.156366  ...  0.595378  0.784241  0.736962   \n",
       "34.0    0.155627  0.843013  0.166483  ...  0.705959  0.845373  0.825102   \n",
       "35.0    0.174330  0.853294  0.165892  ...  0.794225  0.189007  0.818096   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "2874.0  0.261146  0.162865  0.162220  ...  0.205478  0.535880  0.134802   \n",
       "2925.0  0.211073  0.157474  0.860990  ...  0.209529  0.257052  0.176212   \n",
       "2927.0  0.848071  0.859626  0.220389  ...  0.820697  0.830039  0.176437   \n",
       "2934.0  0.341538  0.176018  0.842607  ...  0.194068  0.294293  0.202755   \n",
       "2950.0  0.255844  0.156731  0.848088  ...  0.189603  0.232965  0.161803   \n",
       "\n",
       "            1747      1748      1749      1750      1751      1752      1753  \n",
       "0                                                                             \n",
       "1.0     0.884632  0.872025  0.150538  0.121349  0.837967  0.172734  0.165242  \n",
       "28.0    0.852755  0.695044  0.854522  0.852328  0.157639  0.222541  0.857266  \n",
       "30.0    0.490071  0.190833  0.855286  0.818733  0.247251  0.848224  0.201359  \n",
       "34.0    0.529126  0.797624  0.228296  0.154702  0.144122  0.828859  0.158463  \n",
       "35.0    0.402568  0.834414  0.223977  0.119837  0.139210  0.132350  0.845993  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "2874.0  0.283869  0.143926  0.317814  0.848387  0.265529  0.859265  0.187919  \n",
       "2925.0  0.472837  0.834740  0.192689  0.817822  0.174657  0.846631  0.158055  \n",
       "2927.0  0.883862  0.273098  0.873988  0.185938  0.142743  0.867638  0.139545  \n",
       "2934.0  0.282282  0.856819  0.186665  0.843954  0.163481  0.869356  0.173763  \n",
       "2950.0  0.299540  0.852363  0.199494  0.818966  0.166747  0.871036  0.176611  \n",
       "\n",
       "[361 rows x 1753 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_explain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1744</th>\n",
       "      <th>1745</th>\n",
       "      <th>1746</th>\n",
       "      <th>1747</th>\n",
       "      <th>1748</th>\n",
       "      <th>1749</th>\n",
       "      <th>1750</th>\n",
       "      <th>1751</th>\n",
       "      <th>1752</th>\n",
       "      <th>1753</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.196204</td>\n",
       "      <td>0.125538</td>\n",
       "      <td>0.829300</td>\n",
       "      <td>0.143392</td>\n",
       "      <td>0.785905</td>\n",
       "      <td>0.712615</td>\n",
       "      <td>0.207511</td>\n",
       "      <td>0.859502</td>\n",
       "      <td>0.163500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156079</td>\n",
       "      <td>0.748273</td>\n",
       "      <td>0.176544</td>\n",
       "      <td>0.884632</td>\n",
       "      <td>0.872025</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.121349</td>\n",
       "      <td>0.837967</td>\n",
       "      <td>0.172734</td>\n",
       "      <td>0.165242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>0.827174</td>\n",
       "      <td>0.194986</td>\n",
       "      <td>0.172719</td>\n",
       "      <td>0.177570</td>\n",
       "      <td>0.845929</td>\n",
       "      <td>0.147856</td>\n",
       "      <td>0.840769</td>\n",
       "      <td>0.864428</td>\n",
       "      <td>0.818672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182639</td>\n",
       "      <td>0.885735</td>\n",
       "      <td>0.124151</td>\n",
       "      <td>0.852755</td>\n",
       "      <td>0.695044</td>\n",
       "      <td>0.854522</td>\n",
       "      <td>0.852328</td>\n",
       "      <td>0.157639</td>\n",
       "      <td>0.222541</td>\n",
       "      <td>0.857266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>0.691643</td>\n",
       "      <td>0.848522</td>\n",
       "      <td>0.114636</td>\n",
       "      <td>0.353214</td>\n",
       "      <td>0.160089</td>\n",
       "      <td>0.834555</td>\n",
       "      <td>0.150483</td>\n",
       "      <td>0.319174</td>\n",
       "      <td>0.185085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595378</td>\n",
       "      <td>0.784241</td>\n",
       "      <td>0.736962</td>\n",
       "      <td>0.490071</td>\n",
       "      <td>0.190833</td>\n",
       "      <td>0.855286</td>\n",
       "      <td>0.818733</td>\n",
       "      <td>0.247251</td>\n",
       "      <td>0.848224</td>\n",
       "      <td>0.201359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>0.081797</td>\n",
       "      <td>0.831367</td>\n",
       "      <td>0.194305</td>\n",
       "      <td>0.812363</td>\n",
       "      <td>0.825632</td>\n",
       "      <td>0.812793</td>\n",
       "      <td>0.174359</td>\n",
       "      <td>0.155627</td>\n",
       "      <td>0.843013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705959</td>\n",
       "      <td>0.845373</td>\n",
       "      <td>0.825102</td>\n",
       "      <td>0.529126</td>\n",
       "      <td>0.797624</td>\n",
       "      <td>0.228296</td>\n",
       "      <td>0.154702</td>\n",
       "      <td>0.144122</td>\n",
       "      <td>0.828859</td>\n",
       "      <td>0.158463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>0.111419</td>\n",
       "      <td>0.843864</td>\n",
       "      <td>0.198652</td>\n",
       "      <td>0.799678</td>\n",
       "      <td>0.837694</td>\n",
       "      <td>0.840953</td>\n",
       "      <td>0.154347</td>\n",
       "      <td>0.174330</td>\n",
       "      <td>0.853294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794225</td>\n",
       "      <td>0.189007</td>\n",
       "      <td>0.818096</td>\n",
       "      <td>0.402568</td>\n",
       "      <td>0.834414</td>\n",
       "      <td>0.223977</td>\n",
       "      <td>0.119837</td>\n",
       "      <td>0.139210</td>\n",
       "      <td>0.132350</td>\n",
       "      <td>0.845993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>2931</td>\n",
       "      <td>0.159106</td>\n",
       "      <td>0.157753</td>\n",
       "      <td>0.809235</td>\n",
       "      <td>0.825924</td>\n",
       "      <td>0.159197</td>\n",
       "      <td>0.699754</td>\n",
       "      <td>0.208735</td>\n",
       "      <td>0.274189</td>\n",
       "      <td>0.154435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133370</td>\n",
       "      <td>0.306523</td>\n",
       "      <td>0.172947</td>\n",
       "      <td>0.130974</td>\n",
       "      <td>0.119893</td>\n",
       "      <td>0.154302</td>\n",
       "      <td>0.198281</td>\n",
       "      <td>0.157798</td>\n",
       "      <td>0.112518</td>\n",
       "      <td>0.194006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>2940</td>\n",
       "      <td>0.754659</td>\n",
       "      <td>0.843647</td>\n",
       "      <td>0.196224</td>\n",
       "      <td>0.774877</td>\n",
       "      <td>0.152437</td>\n",
       "      <td>0.838377</td>\n",
       "      <td>0.161009</td>\n",
       "      <td>0.186751</td>\n",
       "      <td>0.170174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.128474</td>\n",
       "      <td>0.163081</td>\n",
       "      <td>0.146479</td>\n",
       "      <td>0.138082</td>\n",
       "      <td>0.169267</td>\n",
       "      <td>0.134003</td>\n",
       "      <td>0.153524</td>\n",
       "      <td>0.173947</td>\n",
       "      <td>0.164563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>2944</td>\n",
       "      <td>0.129716</td>\n",
       "      <td>0.165112</td>\n",
       "      <td>0.354632</td>\n",
       "      <td>0.162761</td>\n",
       "      <td>0.140983</td>\n",
       "      <td>0.779570</td>\n",
       "      <td>0.120060</td>\n",
       "      <td>0.146458</td>\n",
       "      <td>0.752690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124579</td>\n",
       "      <td>0.208815</td>\n",
       "      <td>0.149776</td>\n",
       "      <td>0.856512</td>\n",
       "      <td>0.829717</td>\n",
       "      <td>0.829908</td>\n",
       "      <td>0.809690</td>\n",
       "      <td>0.115095</td>\n",
       "      <td>0.838183</td>\n",
       "      <td>0.152334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>2947</td>\n",
       "      <td>0.851133</td>\n",
       "      <td>0.199572</td>\n",
       "      <td>0.203811</td>\n",
       "      <td>0.165923</td>\n",
       "      <td>0.378331</td>\n",
       "      <td>0.161169</td>\n",
       "      <td>0.863781</td>\n",
       "      <td>0.885180</td>\n",
       "      <td>0.851160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819384</td>\n",
       "      <td>0.868243</td>\n",
       "      <td>0.195708</td>\n",
       "      <td>0.856170</td>\n",
       "      <td>0.232315</td>\n",
       "      <td>0.846350</td>\n",
       "      <td>0.180403</td>\n",
       "      <td>0.143601</td>\n",
       "      <td>0.826127</td>\n",
       "      <td>0.156721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>2948</td>\n",
       "      <td>0.147934</td>\n",
       "      <td>0.151329</td>\n",
       "      <td>0.833328</td>\n",
       "      <td>0.824868</td>\n",
       "      <td>0.168115</td>\n",
       "      <td>0.659532</td>\n",
       "      <td>0.382278</td>\n",
       "      <td>0.209036</td>\n",
       "      <td>0.166440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156321</td>\n",
       "      <td>0.263435</td>\n",
       "      <td>0.163073</td>\n",
       "      <td>0.713209</td>\n",
       "      <td>0.863007</td>\n",
       "      <td>0.215332</td>\n",
       "      <td>0.832675</td>\n",
       "      <td>0.179714</td>\n",
       "      <td>0.866486</td>\n",
       "      <td>0.159478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1751 rows × 1754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID         1         2         3         4         5         6  \\\n",
       "0        1  0.196204  0.125538  0.829300  0.143392  0.785905  0.712615   \n",
       "1       28  0.827174  0.194986  0.172719  0.177570  0.845929  0.147856   \n",
       "2       30  0.691643  0.848522  0.114636  0.353214  0.160089  0.834555   \n",
       "3       34  0.081797  0.831367  0.194305  0.812363  0.825632  0.812793   \n",
       "4       35  0.111419  0.843864  0.198652  0.799678  0.837694  0.840953   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "1746  2931  0.159106  0.157753  0.809235  0.825924  0.159197  0.699754   \n",
       "1747  2940  0.754659  0.843647  0.196224  0.774877  0.152437  0.838377   \n",
       "1748  2944  0.129716  0.165112  0.354632  0.162761  0.140983  0.779570   \n",
       "1749  2947  0.851133  0.199572  0.203811  0.165923  0.378331  0.161169   \n",
       "1750  2948  0.147934  0.151329  0.833328  0.824868  0.168115  0.659532   \n",
       "\n",
       "             7         8         9  ...      1744      1745      1746  \\\n",
       "0     0.207511  0.859502  0.163500  ...  0.156079  0.748273  0.176544   \n",
       "1     0.840769  0.864428  0.818672  ...  0.182639  0.885735  0.124151   \n",
       "2     0.150483  0.319174  0.185085  ...  0.595378  0.784241  0.736962   \n",
       "3     0.174359  0.155627  0.843013  ...  0.705959  0.845373  0.825102   \n",
       "4     0.154347  0.174330  0.853294  ...  0.794225  0.189007  0.818096   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1746  0.208735  0.274189  0.154435  ...  0.133370  0.306523  0.172947   \n",
       "1747  0.161009  0.186751  0.170174  ...  0.146667  0.128474  0.163081   \n",
       "1748  0.120060  0.146458  0.752690  ...  0.124579  0.208815  0.149776   \n",
       "1749  0.863781  0.885180  0.851160  ...  0.819384  0.868243  0.195708   \n",
       "1750  0.382278  0.209036  0.166440  ...  0.156321  0.263435  0.163073   \n",
       "\n",
       "          1747      1748      1749      1750      1751      1752      1753  \n",
       "0     0.884632  0.872025  0.150538  0.121349  0.837967  0.172734  0.165242  \n",
       "1     0.852755  0.695044  0.854522  0.852328  0.157639  0.222541  0.857266  \n",
       "2     0.490071  0.190833  0.855286  0.818733  0.247251  0.848224  0.201359  \n",
       "3     0.529126  0.797624  0.228296  0.154702  0.144122  0.828859  0.158463  \n",
       "4     0.402568  0.834414  0.223977  0.119837  0.139210  0.132350  0.845993  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1746  0.130974  0.119893  0.154302  0.198281  0.157798  0.112518  0.194006  \n",
       "1747  0.146479  0.138082  0.169267  0.134003  0.153524  0.173947  0.164563  \n",
       "1748  0.856512  0.829717  0.829908  0.809690  0.115095  0.838183  0.152334  \n",
       "1749  0.856170  0.232315  0.846350  0.180403  0.143601  0.826127  0.156721  \n",
       "1750  0.713209  0.863007  0.215332  0.832675  0.179714  0.866486  0.159478  \n",
       "\n",
       "[1751 rows x 1754 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_explain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID      1750\n",
       "1        760\n",
       "2        930\n",
       "3        930\n",
       "4       1327\n",
       "        ... \n",
       "1749     795\n",
       "1750     600\n",
       "1751     485\n",
       "1752     811\n",
       "1753     300\n",
       "Length: 1754, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_explain_df>0.2).sum()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_counts = pd.concat([(all_explain_df>0.2).sum().iloc[513:982],(all_explain_df>0.2).sum().iloc[1751:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = []\n",
    "for i in range(20):\n",
    "    thres.append((0.05*i, (((all_explain_df>(0.05*i)).sum().iloc[513:982])>0).sum()/len(feature_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3538135593220339"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "167/472"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.9936440677966102),\n",
       " (0.05, 0.9936440677966102),\n",
       " (0.1, 0.9936440677966102),\n",
       " (0.15000000000000002, 0.9936440677966102),\n",
       " (0.2, 0.336864406779661),\n",
       " (0.25, 0.336864406779661),\n",
       " (0.30000000000000004, 0.336864406779661),\n",
       " (0.35000000000000003, 0.336864406779661),\n",
       " (0.4, 0.336864406779661),\n",
       " (0.45, 0.336864406779661),\n",
       " (0.5, 0.3347457627118644),\n",
       " (0.55, 0.3347457627118644),\n",
       " (0.6000000000000001, 0.3347457627118644),\n",
       " (0.65, 0.3305084745762712),\n",
       " (0.7000000000000001, 0.3305084745762712),\n",
       " (0.75, 0.3220338983050847),\n",
       " (0.8, 0.3135593220338983),\n",
       " (0.8500000000000001, 0.19279661016949154),\n",
       " (0.9, 0.014830508474576272),\n",
       " (0.9500000000000001, 0.0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.993644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.993644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.993644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.993644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.336864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.336864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.336864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.336864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.336864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.336864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.334746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.334746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.334746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.330508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.330508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.322034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.313559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.192797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1\n",
       "0   0.00  0.993644\n",
       "1   0.05  0.993644\n",
       "2   0.10  0.993644\n",
       "3   0.15  0.993644\n",
       "4   0.20  0.336864\n",
       "5   0.25  0.336864\n",
       "6   0.30  0.336864\n",
       "7   0.35  0.336864\n",
       "8   0.40  0.336864\n",
       "9   0.45  0.336864\n",
       "10  0.50  0.334746\n",
       "11  0.55  0.334746\n",
       "12  0.60  0.334746\n",
       "13  0.65  0.330508\n",
       "14  0.70  0.330508\n",
       "15  0.75  0.322034\n",
       "16  0.80  0.313559\n",
       "17  0.85  0.192797\n",
       "18  0.90  0.014831\n",
       "19  0.95  0.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='0'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAY6klEQVR4nO3dfXBc9X3v8fd3H6Rd2ZK1tuUHdhG2wTQ4F2iCwlNjbpo2iYG2DDN0LqY3HiDgYW7IdO5fMHfubedO5rZNH2aaeyHX45CUpg/QtKEtSZ2QdkgLTfDFdkMAY0wdG+z1o2yDLVvW8/f+sSt5I0vaI+lI5+zu5zWj8Z5zflp9f7Lmo6Pf+Z3fMXdHRERqXyLqAkREJBwKdBGROqFAFxGpEwp0EZE6oUAXEakTqai+8NKlS33VqlVRfXkRkZq0a9euk+7eMdGxyAJ91apV7Ny5M6ovLyJSk8zsvcmOachFRKROKNBFROqEAl1EpE5ENoYuIhKVwcFBisUifX19UZcyqUwmQ6FQIJ1OB/6cqoFuZl8HfgU44e7/YYLjBnwZuAPoBe53938LXIGIyDwrFou0trayatUqShEWL+7OqVOnKBaLrF69OvDnBRlyeRrYMMXx24G15Y/NwP8N/NVFRCLQ19fHkiVLYhnmAGbGkiVLpv0XRNVAd/eXgNNTNLkL+IaXbAfazWzltKoQEZlncQ3zUTOpL4wx9DxwqGK7WN53dHxDM9tM6Syezs7OGX2xvcd6+IfXj8zoc+NiQXOKBz++mnRS16RFJDxhBPpEv0YmXGTd3bcCWwG6urpmtBD7vhPn+D8/2DeTT42F0eXnr80v4tarlkZbjIhE5sEHH+Q73/kOy5Yt48033wzlPcMI9CJwecV2AZizU+g7r1vJndfdOVdvP+cOne5l/e//gOL7F6IuRUQidP/99/Poo4+yadOm0N4zjL/5nwc2WcnNwBl3v2S4RUpWLMqQMCi+3xt1KSISodtuu43FixeH+p5Bpi0+A3wCWGpmReC3gTSAu28BtlGasriP0rTFB0KtsM6kkwlWtGUofqAzdJE4+J/f3s1bR86G+p7rLmvjt3/1w6G+ZxBVA93dN1Y57sDnQ6uoAeRzWQ5ryEVEQqY7RSNQyLXw6oGpZoKKyHyJ4kx6rmjeXATy7VmOne1jaHgk6lJEpI4o0COQz2UZHnGO9/RHXYqIRGTjxo3ccsst7N27l0KhwNe+9rVZv6eGXCJQyGUBKJ7uJd+ejbgaEYnCM888E/p76gw9AqMhflgzXUQkRAr0CFxWDnTdXCQiYVKgRyCTTtLR2qypiyIRcp/R6iPzZib1KdAjkm/PashFJCKZTIZTp07FNtRH10PPZDLT+jxdFI1IIZflzcNnoi5DpCEVCgWKxSLd3d1RlzKp0ScWTYcCPSL5XJbv7z7OyIiTSMR7XWaRepNOp6f1JKBaoSGXiBRyLQwMj9B9TnPRRSQcCvSIFDTTRURCpkCPSD6nuegiEi4FekTyY2foWhddRMKhQI/IguYUuZa05qKLSGgU6BHK57IaQxeR0CjQI1Rob9EYuoiERoEeodEnF8X1bjURqS0K9AgVclkuDA5z+vxA1KWISB1QoEdIy+iKSJgU6BEanYuuC6MiEgYFeoQKuRYATV0UkVAo0CO0KJumtTmlIRcRCYUCPWKluei6W1REZk+BHrGCbi4SkZAo0COmJxeJSFgU6BEr5Fro6RvizIXBqEsRkRqnQI/Y2DK6GnYRkVlSoEdMy+iKSFgU6BEr6EEXIhISBXrEFi9oIpNOaMhFRGYtUKCb2QYz22tm+8zs8QmOLzKzb5vZT8xst5k9EH6p9cnMKORaNHVRRGataqCbWRJ4ErgdWAdsNLN145p9HnjL3a8HPgH8kZk1hVxr3dLURREJQ5Az9BuBfe6+390HgGeBu8a1caDVzAxYCJwGhkKttI7pblERCUOQQM8Dhyq2i+V9lZ4ArgGOAG8Av+nuI+PfyMw2m9lOM9vZ3d09w5LrTyGX5f3eQXoH9DtQRGYuSKDbBPvGP2LnM8BrwGXAzwNPmFnbJZ/kvtXdu9y9q6OjY5ql1q+xddE1ji4isxAk0IvA5RXbBUpn4pUeAJ7zkn3AAeBD4ZRY/0aX0dWFURGZjSCBvgNYa2aryxc67wWeH9fmIPBLAGa2HPg5YH+Yhdaz0bnoRV0YFZFZSFVr4O5DZvYo8AKQBL7u7rvN7JHy8S3AF4GnzewNSkM0j7n7yTmsu650LGymKZnQhVERmZWqgQ7g7tuAbeP2bal4fQT4dLilNY5EwrisPaMxdBGZFd0pGhP5nOaii8jsKNBjIt+uB12IyOwo0GOikGuhu6efvsHhqEsRkRqlQI+J0bnoRzTsIiIzpECPCS2jKyKzpUCPCT25SERmS4EeEyvaMiQTpgujIjJjCvSYSCUTrGjLaMhFRGZMgR4jWkZXRGZDgR4jhVxWY+giMmMK9BgptGc5draPweFLlpIXEalKgR4j+VyWEYdjZ/qiLkVEapACPUa0LrqIzIYCPUbGnlykmS4iMgMK9BhZ2Z4B0EwXEZkRBXqMNKeSLG9r1kwXEZkRBXrMaBldEZkpBXrMFHItGkMXkRlRoMdMPpfl6JkLDI941KWISI1RoMdMvj3L4LBzokdz0UVkehToMVPQMroiMkMK9JgZDXRdGBWR6VKgx0y+vXS3qC6Mish0KdBjJtuUZMmCJp2hi8i0KdBjSOuii8hMKNBjqJDLashFRKZNgR5D+fbSgy7cNRddRIJToMdQvj1L/9AIJ88NRF2KiNQQBXoMja6LrmEXEZkOBXoM5cfmouvCqIgEFyjQzWyDme01s31m9vgkbT5hZq+Z2W4z+5dwy2wsed0tKiIzkKrWwMySwJPAp4AisMPMnnf3tyratANfATa4+0EzWzZH9TaEtkyatkxKc9FFZFqCnKHfCOxz9/3uPgA8C9w1rs19wHPufhDA3U+EW2bjyWsZXRGZpiCBngcOVWwXy/sqXQ3kzOyfzWyXmW0Kq8BGVchlNeQiItMSJNBtgn3jJ0ingBuAO4HPAP/DzK6+5I3MNpvZTjPb2d3dPe1iG0npyUW9mosuIoEFCfQicHnFdgE4MkGb77n7eXc/CbwEXD/+jdx9q7t3uXtXR0fHTGtuCIVclvMDw5y5MBh1KSJSI4IE+g5grZmtNrMm4F7g+XFt/h5Yb2YpM2sBbgL2hFtqY9EyuiIyXVUD3d2HgEeBFyiF9DfdfbeZPWJmj5Tb7AG+B7wOvAo85e5vzl3Z9W90GV0FuogEVXXaIoC7bwO2jdu3Zdz2HwB/EF5pjW3syUWa6SIiAelO0Zhqb0nT0pTU3aIiEpgCPabMbGzVRRGRIBToMaZ10UVkOhToMVZ6cpECXUSCUaDHWCHXwpkLg/T0aS66iFSnQI+xfLtmuohIcAr0GNMyuiIyHQr0GNNcdBGZDgV6jC1d0ExTKqELoyISiAI9xhIJo6C56CISkAI95kpTF3W3qIhUp0CPuXy7bi4SkWAU6DFXyGU5eW6AvsHhqEsRkZhToMdcXuuii0hACvSYG10XXcMuIlKNAj3mLj65SBdGRWRqCvSYW96WIZUwTV0UkaoU6DGXTBgr2zMachGRqhToNSDfrmV0RaQ6BXoNyLe3aMhFRKpSoNeAQi7L8Z4+BoZGoi5FRGJMgV4D8rks7nD0jM7SRWRyCvQaUNC66CISgAK9BhTKNxfpwqiITEWBXgNWLMpgBkVNXRSRKSjQa0BTKsGKtoyGXERkSgr0GlGai67b/0Vkcgr0GpHPaV10EZmaAr1GFHJZjp7pY2hYc9FFZGIK9BqRb29heMQ53tMfdSkiElMK9BqhuegiUk2gQDezDWa218z2mdnjU7T7mJkNm9k94ZUoUPnkIl0YFZGJVQ10M0sCTwK3A+uAjWa2bpJ2XwJeCLtIKc1yAZ2hi8jkgpyh3wjsc/f97j4APAvcNUG7LwDfAk6EWJ+UZdJJli5s1t2iIjKpIIGeBw5VbBfL+8aYWR64G9gy1RuZ2WYz22lmO7u7u6dba8PT1EURmUqQQLcJ9vm47T8GHnP34aneyN23unuXu3d1dHQELFFGFRToIjKFVIA2ReDyiu0CcGRcmy7gWTMDWArcYWZD7v53YRQpJYX2LP+4+zgjI04iMdHvWRFpZEECfQew1sxWA4eBe4H7Khu4++rR12b2NPAdhXn48rksA8MjnDzXz7K2TNTliEjMVB1ycfch4FFKs1f2AN90991m9oiZPTLXBcpFo3PRD+nCqIhMIMgZOu6+Ddg2bt+EF0Dd/f7ZlyUTyZfXRT/8wQVuuCIXcTUiEje6U7SG5HW3qIhMQYFeQxY2p2hvSetuURGZkAK9xuTbNXVRRCamQK8xhVxWd4uKyIQU6DUm397C4fcv4D7+3i4RaXQK9BqTz2W5MDjM+72DUZciIjETaNqixMfoXPRfe+JfaUrq9/FMpZJGUypBUzJBOpmgKZWgOVX6N50s7W8qb4+9Tl48HvWNutmmJAub0yxoTtKaSbGgOcWCptTY67R+NhqSAr3G3HLlEv5T1+X0Dk65bI5Mwd0ZGnYGhkcYGBphYHiEnr4hTpVfD47uL2+P/ltLo1zNqQQLm1MszJSCfux1c4qFzUmSs/iNZBi5BU2saMuwYlEzy9syLG/LsLilSUtSREyBXmPaMmm+dM91UZfRcNydoREfC/sow92BC4PDnOsb4lx/6eN8/9DY9vn+i/tHt3v6hjjR08f5k8P09A3N6hrMiDsfXBi85HuQThrLWjOsWJRhRTnkRwO/FP6lfZl0cnbfAJmUAl0kADMjnTTSyQQtTVFXE72h4RG6z/Vz7Ewfx8/2cexMH8fO9o+93nP0LD/Ye4LegUv/klyUTZcCf1GGFW3NFa9HfwnobH+mFOgiMm2pZIKVi7KsXJSdtI2709M/xPEzfRwrB/3xs6Ov+znR08fbR8/Sfa5/Wmf7+fYsH+3MKfAnoEAXkTlhZrRl0rRl0qxd3jppu5mc7d93Uye/c/e189GNmqJAF5FITfds/09feZc/336QDR9ewW1X60E5lTS3SURib/Rsf+3yVv77neu4smMBj3/rdXr6dD9GJQW6iNSUTDrJH/769Rw728fvbNsTdTmxokAXkZrzkc4cD9+2hmdePcRL7+iB86MU6CJSk/7rL1+toZdxFOgiUpM09HIpBbqI1CwNvfwsBbqI1DQNvVykQBeRmqahl4sU6CJS8zT0UqJAF5G6oKEXBbqI1AkNvSjQRaSONPrQiwJdROpKIw+9KNBFpK408tCLAl1E6k6jDr0o0EWkLlUOvZxtkKEXBbqI1KXKoZffbZChFwW6iNStRht6CRToZrbBzPaa2T4ze3yC479hZq+XP35kZteHX6qIyPQ10tBL1UA3syTwJHA7sA7YaGbrxjU7APxHd78O+CKwNexCRURmopGGXoKcod8I7HP3/e4+ADwL3FXZwN1/5O7vlze3A4VwyxQRmblGGXoJEuh54FDFdrG8bzKfA7470QEz22xmO81sZ3d3/X5TRSR+GmHoJUig2wT7fMKGZr9IKdAfm+i4u2919y537+ro6AhepYjILFUOvTz5g31RlzMnggR6Ebi8YrsAHBnfyMyuA54C7nL3U+GUJyISno905rjj2pX85faDdbksQJBA3wGsNbPVZtYE3As8X9nAzDqB54DPuvs74ZcpIhKOzbetoad/iL/acah64xpTNdDdfQh4FHgB2AN80913m9kjZvZIudlvAUuAr5jZa2a2c84qFhGZhesK7dy0ejF/8sN3GRoeibqcUAWah+7u29z9ane/0t3/V3nfFnffUn79kLvn3P3nyx9dc1m0iMhsPLx+DYc/uMC2N49FXUqodKeoiDScT35oGWs6FrD1pZ/iPuEcj5qkQBeRhpNIGA+vX8Obh8+yff/pqMsJjQJdRBrS3R/Js3RhE199eX/UpYRGgS4iDSmTTrLpllW8+PYJ/v14T9TlhEKBLiIN6z/ffAWZdIKnXj4QdSmhUKCLSMNavKCJe24o8Lc/PsyJnr6oy5k1BbqINLTPfXwNgyMj/Nkr70Vdyqwp0EWkoa1euoBPr1vOn21/j96BoajLmRUFuog0vM23reGD3kH+Zlcx6lJmRYEuIg3vhisW89HOdp56+QDDI7V7o5ECXUSE0nIAB0/38v3dtbscgAJdRAT49IdX0Lm4ha01fKORAl1EBEgmjIfWr+bHBz9g13u1uRyAAl1EpOyeGwq0t6TZ+lJtnqUr0EVEylqaUnz25iv4/lvHOXDyfNTlTJsCXUSkwqZbVpFOJPjav9beWboCXUSkQkdrM3d/JM9f7yxy6lx/1OVMiwJdRGSch9avpn9ohD/ffjDqUqZFgS4iMs7a5a188kPL+MYr79I3OBx1OYEp0EVEJvDw+jWcOj/A3/74cNSlBKZAFxGZwM1rFnNtfhFffXk/IzWyHIACXURkAmalG432d5/nxbdPRF1OIAp0EZFJ3HHtSvLt2ZpZDkCBLiIyiXQywQO/sIpXD5zmtUMfRF1OVQp0EZEp3HtjJ62ZFF+tgbN0BbqIyBQWNqe476ZOvvvGUQ6d7o26nCkp0EVEqnjg1tUkzPj6Dw9EXcqUFOgiIlWsWJTh166/jL/acYgzvYNRlzMpBbqISAAPrV9D78Awf/Hqe1GXMikFuohIAOsua2P92qU8/cN36R+K53IACnQRkYAeXr+GEz39PPyNXTzx4r/z4tvHOXrmAu7xuJM0FaSRmW0Avgwkgafc/ffGHbfy8TuAXuB+d/+3kGsVEYnU+rVLefAXVvNPe47z0jvdY/vbW9Jcs6KNa1a2cc3KVtZd1sZVyxbSnErOa31W7TeLmSWBd4BPAUVgB7DR3d+qaHMH8AVKgX4T8GV3v2mq9+3q6vKdO3fOrnoRkYj09A3y9rEe9hw9y56jZ3nraA97j52lb3AEgFTCuGrZwrGQL/3bxtKFzbP6uma2y927JjoW5Az9RmCfu+8vv9mzwF3AWxVt7gK+4aXfDtvNrN3MVrr70VlVLiISU62ZNB9btZiPrVo8tm94xHn31PlSwB8pBf0rPz31Mys2LmttZvNta3ho/ZrQawoS6HngUMV2kdJZeLU2eeBnAt3MNgObATo7O6dbq4hIrCUTxpUdC7myYyG/ct1lY/vfPz9QPos/y56jPXS0zu4sfTJBAt0m2Dd+nCZIG9x9K7AVSkMuAb62iEjNyy1o4tarlnLrVUvn9OsEmeVSBC6v2C4AR2bQRkRE5lCQQN8BrDWz1WbWBNwLPD+uzfPAJiu5GTij8XMRkflVdcjF3YfM7FHgBUrTFr/u7rvN7JHy8S3ANkozXPZRmrb4wNyVLCIiEwk0D93dt1EK7cp9WypeO/D5cEsTEZHp0J2iIiJ1QoEuIlInFOgiInVCgS4iUieqruUyZ1/YrBuY6cLCS4GTIZZTaxq9/6DvgfrfuP2/wt07JjoQWaDPhpntnGxxmkbQ6P0HfQ/U/8bu/2Q05CIiUicU6CIidaJWA31r1AVErNH7D/oeqP9yiZocQxcRkUvV6hm6iIiMo0AXEakTsQ50M9tgZnvNbJ+ZPT7BcTOz/10+/rqZfTSKOudKgP7/Rrnfr5vZj8zs+ijqnCvV+l/R7mNmNmxm98xnfXMtSP/N7BNm9pqZ7Tazf5nvGudSgJ//RWb2bTP7Sbn/WuXV3WP5QWmp3p8Ca4Am4CfAunFt7gC+S+mJSTcD/y/quue5/7cCufLr2xut/xXtXqS0Gug9Udc9z///7ZSe7dtZ3l4Wdd3z3P//Bnyp/LoDOA00RV17lB9xPkMfezi1uw8Aow+nrjT2cGp33w60m9nK+S50jlTtv7v/yN3fL29up/SkqHoR5P8f4AvAt4AT81ncPAjS//uA59z9IIC719P3IEj/HWg1MwMWUgr0ofktM17iHOiTPXh6um1q1XT79jlKf63Ui6r9N7M8cDewhfoT5P//aiBnZv9sZrvMbNO8VTf3gvT/CeAaSo+7fAP4TXcfmZ/y4inQAy4iEtrDqWtU4L6Z2S9SCvSPz2lF8ytI//8YeMzdh0snaXUlSP9TwA3ALwFZ4BUz2+7u78x1cfMgSP8/A7wGfBK4EvhHM3vZ3c/OcW2xFedAb/SHUwfqm5ldBzwF3O7up+aptvkQpP9dwLPlMF8K3GFmQ+7+d/NS4dwK+vN/0t3PA+fN7CXgeqAeAj1I/x8Afs9Lg+j7zOwA8CHg1fkpMX7iPOTS6A+nrtp/M+sEngM+WydnZZWq9t/dV7v7KndfBfwN8F/qJMwh2M//3wPrzSxlZi3ATcCeea5zrgTp/0FKf51gZsuBnwP2z2uVMRPbM3Rv8IdTB+z/bwFLgK+Uz1KHvE5WoAvY/7oVpP/uvsfMvge8DowAT7n7m9FVHZ6A//9fBJ42szcoDdE85u6NuqQuoFv/RUTqRpyHXEREZBoU6CIidUKBLiJSJxToIiJ1QoEuIlInFOgiFYKu8CgSR5q2KFJmZklKd1l+itKdijuAje7+VqSFiQSkM3SRi4Ku8CgSSwp0kYvqefVOaQAKdJGL6nn1TmkACnSRi+p59U5pAAp0kYuCrPAnEluxXW1RZL5NtsJfxGWJBKZpiyIidUJDLiIidUKBLiJSJxToIiJ1QoEuIlInFOgiInVCgS4iUicU6CIideL/AyPVcvw2s6eaAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(thres).set_index(0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513     107\n",
       "514      84\n",
       "515      22\n",
       "516       0\n",
       "517       7\n",
       "       ... \n",
       "980       0\n",
       "981       0\n",
       "1751    485\n",
       "1752    811\n",
       "1753    300\n",
       "Length: 472, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513     107\n",
       "514      84\n",
       "515      22\n",
       "517       7\n",
       "519     101\n",
       "       ... \n",
       "974     172\n",
       "976      11\n",
       "1751    485\n",
       "1752    811\n",
       "1753    300\n",
       "Length: 162, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_counts[feature_counts>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_counts = feature_counts.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1752    811\n",
       "1751    485\n",
       "970     428\n",
       "969     411\n",
       "595     350\n",
       "       ... \n",
       "707       0\n",
       "704       0\n",
       "700       0\n",
       "699       0\n",
       "749       0\n",
       "Length: 472, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_counts[feature_counts>0].hist(log=True, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1616    360\n",
       "1640    355\n",
       "1360    355\n",
       "1583    352\n",
       "1294    343\n",
       "1724    343\n",
       "1552    342\n",
       "1107    342\n",
       "1450    342\n",
       "1541    339\n",
       "1551    339\n",
       "1393    337\n",
       "1298    336\n",
       "1108    335\n",
       "1540    333\n",
       "1253    333\n",
       "1561    333\n",
       "1624    332\n",
       "1405    327\n",
       "1691    321\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_explain_df>0.5).sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=2926, x=[2926, 1753], y=[2926, 20], node_type=[2926], train_mask=[2926], val_mask=[2926], test_mask=[2926], edge_index=[2, 101360], edge_attr=[101360], n_id=[2926], batch_size=32)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explain node 10: 100%|███████████████████████████████| 200/200 [00:07<00:00, 26.84it/s]\n"
     ]
    }
   ],
   "source": [
    "node_idx = 10\n",
    "edge_index = to_undirected(batch.edge_index)\n",
    "node_feat_mask, _ = explainer.explain_node(node_idx, batch.x, edge_index)\n",
    "#ax, G = explainer.visualize_subgraph(node_idx, edge_index, edge_mask, y=data.y)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7, device='cuda:0')"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(node_feat_mask>0.9).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 28, 1152, 1254, 1474, 1552, 1567]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(len(node_feat_mask)) if node_feat_mask[i]>0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False, False, False, False, False], device='cuda:0')"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(node_feat_mask[515:982]>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEM_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#path = osp.join(os.getcwd(), '../../data/VEN')\n",
    "#transform = T.Compose([T.NormalizeFeatures(), T.ToSparseTensor()])\n",
    "dataset = VEN_tem('dataset/Venice_tem')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.train_mask,\n",
    ")\n",
    "seed_everything(args.seed)\n",
    "val_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.val_mask,\n",
    ")\n",
    "seed_everything(args.seed)\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.test_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 32.81it/s]\n"
     ]
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_loss_att, test_loss_val, test_att_acc, test_val_acc, test_val_jac, test_val_1 = test_Homo(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 58.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7277005653302095,\n",
       " 1.602298717089307,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 0.863342575749532,\n",
       " 90.30470914127424)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_Homo(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 35.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8052866606692958,\n",
       " 1.62163168691062,\n",
       " 97.96747967479675,\n",
       " 99.50738916256158,\n",
       " 0.7536945859786912,\n",
       " 79.3103448275862)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_Homo(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 40.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8224199704574069,\n",
       " 1.6355466842651367,\n",
       " 97.21669980119285,\n",
       " 99.47916666666667,\n",
       " 0.7473958457509676,\n",
       " 81.77083333333333)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_Homo(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 38.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 67.88it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 72.70it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 68.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 69.32it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 67.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 70.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 68.56it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 72.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 68.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 71.02it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 66.32it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 69.31it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 64.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 66.16it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 65.88it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 68.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 65.33it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 72.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 66.91it/s]\n"
     ]
    }
   ],
   "source": [
    "val_numbers = []\n",
    "test_numbers = []\n",
    "for seed in [0,1,2,42,100,233,1024,1337,2333,4399]:\n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    val_numbers.append(test_Homo(model, val_loader))\n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    test_numbers.append(test_Homo(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame(val_numbers, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])\n",
    "test_df = pd.DataFrame(test_numbers, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.805325</td>\n",
       "      <td>1.621630</td>\n",
       "      <td>9.796748e+01</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.752709</td>\n",
       "      <td>7.980296e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.497956e-14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>1.497956e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.805285</td>\n",
       "      <td>1.621613</td>\n",
       "      <td>9.796748e+01</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.751232</td>\n",
       "      <td>7.980296e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.805303</td>\n",
       "      <td>1.621629</td>\n",
       "      <td>9.796748e+01</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.751232</td>\n",
       "      <td>7.980296e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.805318</td>\n",
       "      <td>1.621633</td>\n",
       "      <td>9.796748e+01</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.753695</td>\n",
       "      <td>7.980296e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.805350</td>\n",
       "      <td>1.621634</td>\n",
       "      <td>9.796748e+01</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.753695</td>\n",
       "      <td>7.980296e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.805366</td>\n",
       "      <td>1.621639</td>\n",
       "      <td>9.796748e+01</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.753695</td>\n",
       "      <td>7.980296e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ATT_loss   VAL_loss       ATT_acc  VAL_k_acc  VAL_k_jac     VAL_1_acc\n",
       "count  10.000000  10.000000  1.000000e+01  10.000000  10.000000  1.000000e+01\n",
       "mean    0.805325   1.621630  9.796748e+01  99.507389   0.752709  7.980296e+01\n",
       "std     0.000028   0.000008  1.497956e-14   0.000000   0.001272  1.497956e-14\n",
       "min     0.805285   1.621613  9.796748e+01  99.507389   0.751232  7.980296e+01\n",
       "25%     0.805303   1.621629  9.796748e+01  99.507389   0.751232  7.980296e+01\n",
       "50%     0.805318   1.621633  9.796748e+01  99.507389   0.753695  7.980296e+01\n",
       "75%     0.805350   1.621634  9.796748e+01  99.507389   0.753695  7.980296e+01\n",
       "max     0.805366   1.621639  9.796748e+01  99.507389   0.753695  7.980296e+01"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.822416</td>\n",
       "      <td>1.635576</td>\n",
       "      <td>9.721670e+01</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>7.473958e-01</td>\n",
       "      <td>81.927083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1.497956e-14</td>\n",
       "      <td>1.497956e-14</td>\n",
       "      <td>1.170278e-16</td>\n",
       "      <td>0.251586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.822341</td>\n",
       "      <td>1.635556</td>\n",
       "      <td>9.721670e+01</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>7.473958e-01</td>\n",
       "      <td>81.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.822396</td>\n",
       "      <td>1.635569</td>\n",
       "      <td>9.721670e+01</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>7.473958e-01</td>\n",
       "      <td>81.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.822418</td>\n",
       "      <td>1.635578</td>\n",
       "      <td>9.721670e+01</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>7.473958e-01</td>\n",
       "      <td>81.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.822445</td>\n",
       "      <td>1.635580</td>\n",
       "      <td>9.721670e+01</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>7.473958e-01</td>\n",
       "      <td>82.161458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.822467</td>\n",
       "      <td>1.635598</td>\n",
       "      <td>9.721670e+01</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>7.473958e-01</td>\n",
       "      <td>82.291667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ATT_loss   VAL_loss       ATT_acc     VAL_k_acc     VAL_k_jac  \\\n",
       "count  10.000000  10.000000  1.000000e+01  1.000000e+01  1.000000e+01   \n",
       "mean    0.822416   1.635576  9.721670e+01  9.947917e+01  7.473958e-01   \n",
       "std     0.000041   0.000012  1.497956e-14  1.497956e-14  1.170278e-16   \n",
       "min     0.822341   1.635556  9.721670e+01  9.947917e+01  7.473958e-01   \n",
       "25%     0.822396   1.635569  9.721670e+01  9.947917e+01  7.473958e-01   \n",
       "50%     0.822418   1.635578  9.721670e+01  9.947917e+01  7.473958e-01   \n",
       "75%     0.822445   1.635580  9.721670e+01  9.947917e+01  7.473958e-01   \n",
       "max     0.822467   1.635598  9.721670e+01  9.947917e+01  7.473958e-01   \n",
       "\n",
       "       VAL_1_acc  \n",
       "count  10.000000  \n",
       "mean   81.927083  \n",
       "std     0.251586  \n",
       "min    81.770833  \n",
       "25%    81.770833  \n",
       "50%    81.770833  \n",
       "75%    82.161458  \n",
       "max    82.291667  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.to_csv(args.save_dir + 'TEM_val_metrics_transfer.csv', sep='\\t')\n",
    "test_df.to_csv(args.save_dir + 'TEM_test_metrics_transfer.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPA_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#path = osp.join(os.getcwd(), '../../data/VEN')\n",
    "#transform = T.Compose([T.NormalizeFeatures(), T.ToSparseTensor()])\n",
    "dataset = VEN_spa('dataset/Venice_spa')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.train_mask,\n",
    ")\n",
    "seed_everything(args.seed)\n",
    "val_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.val_mask,\n",
    ")\n",
    "seed_everything(args.seed)\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.test_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 28.23it/s]\n"
     ]
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_loss_att, test_loss_val, test_att_acc, test_val_acc, test_val_jac, test_val_1 = test_Homo(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 40.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7258908723860236,\n",
       " 1.5998343060881808,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 0.8614958554423747,\n",
       " 89.19667590027701)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_Homo(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 31.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8021168802084961,\n",
       " 1.6210297950970127,\n",
       " 97.96747967479675,\n",
       " 99.50738916256158,\n",
       " 0.7430213542994607,\n",
       " 80.78817733990148)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_Homo(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 30.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.815529429296376,\n",
       " 1.6366863368699949,\n",
       " 97.01789264413519,\n",
       " 99.47916666666667,\n",
       " 0.7482638930281004,\n",
       " 82.8125)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_Homo(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 30.41it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 39.24it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 41.36it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 39.35it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 40.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 39.93it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 41.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 40.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 40.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 38.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 40.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 39.83it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 41.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 39.70it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 28.62it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 37.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 39.64it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 39.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 40.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 39.66it/s]\n"
     ]
    }
   ],
   "source": [
    "val_numbers = []\n",
    "test_numbers = []\n",
    "for seed in [0,1,2,42,100,233,1024,1337,2333,4399]:\n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    val_numbers.append(test_Homo(model, val_loader))\n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    test_numbers.append(test_Homo(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame(val_numbers, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])\n",
    "test_df = pd.DataFrame(test_numbers, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.802074</td>\n",
       "      <td>1.620901</td>\n",
       "      <td>97.947154</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.744499</td>\n",
       "      <td>81.083744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.064274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.254383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.801935</td>\n",
       "      <td>1.620861</td>\n",
       "      <td>97.764228</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.740558</td>\n",
       "      <td>80.788177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.801982</td>\n",
       "      <td>1.620881</td>\n",
       "      <td>97.967480</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.743021</td>\n",
       "      <td>80.788177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.802064</td>\n",
       "      <td>1.620902</td>\n",
       "      <td>97.967480</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.745484</td>\n",
       "      <td>81.280788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.802153</td>\n",
       "      <td>1.620919</td>\n",
       "      <td>97.967480</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.745484</td>\n",
       "      <td>81.280788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.802264</td>\n",
       "      <td>1.620937</td>\n",
       "      <td>97.967480</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.747947</td>\n",
       "      <td>81.280788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ATT_loss   VAL_loss    ATT_acc  VAL_k_acc  VAL_k_jac  VAL_1_acc\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000\n",
       "mean    0.802074   1.620901  97.947154  99.507389   0.744499  81.083744\n",
       "std     0.000111   0.000025   0.064274   0.000000   0.002648   0.254383\n",
       "min     0.801935   1.620861  97.764228  99.507389   0.740558  80.788177\n",
       "25%     0.801982   1.620881  97.967480  99.507389   0.743021  80.788177\n",
       "50%     0.802064   1.620902  97.967480  99.507389   0.745484  81.280788\n",
       "75%     0.802153   1.620919  97.967480  99.507389   0.745484  81.280788\n",
       "max     0.802264   1.620937  97.967480  99.507389   0.747947  81.280788"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.815570</td>\n",
       "      <td>1.636649</td>\n",
       "      <td>96.998012</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>0.746354</td>\n",
       "      <td>82.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.062868</td>\n",
       "      <td>1.497956e-14</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.251586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.815445</td>\n",
       "      <td>1.636571</td>\n",
       "      <td>96.819085</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>0.743056</td>\n",
       "      <td>82.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.815489</td>\n",
       "      <td>1.636608</td>\n",
       "      <td>97.017893</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>82.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.815545</td>\n",
       "      <td>1.636644</td>\n",
       "      <td>97.017893</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>0.746094</td>\n",
       "      <td>82.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.815603</td>\n",
       "      <td>1.636689</td>\n",
       "      <td>97.017893</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>0.748047</td>\n",
       "      <td>83.203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.815845</td>\n",
       "      <td>1.636737</td>\n",
       "      <td>97.017893</td>\n",
       "      <td>9.947917e+01</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>83.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ATT_loss   VAL_loss    ATT_acc     VAL_k_acc  VAL_k_jac  VAL_1_acc\n",
       "count  10.000000  10.000000  10.000000  1.000000e+01  10.000000  10.000000\n",
       "mean    0.815570   1.636649  96.998012  9.947917e+01   0.746354  82.968750\n",
       "std     0.000118   0.000055   0.062868  1.497956e-14   0.002196   0.251586\n",
       "min     0.815445   1.636571  96.819085  9.947917e+01   0.743056  82.812500\n",
       "25%     0.815489   1.636608  97.017893  9.947917e+01   0.744792  82.812500\n",
       "50%     0.815545   1.636644  97.017893  9.947917e+01   0.746094  82.812500\n",
       "75%     0.815603   1.636689  97.017893  9.947917e+01   0.748047  83.203125\n",
       "max     0.815845   1.636737  97.017893  9.947917e+01   0.750000  83.333333"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.to_csv(args.save_dir + 'SPA_val_metrics_transfer.csv', sep='\\t')\n",
    "test_df.to_csv(args.save_dir + 'SPA_test_metrics_transfer.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOC_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#path = osp.join(os.getcwd(), '../../data/VEN')\n",
    "#transform = T.Compose([T.NormalizeFeatures(), T.ToSparseTensor()])\n",
    "dataset = VEN_soc('dataset/Venice_soc')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.train_mask,\n",
    ")\n",
    "seed_everything(args.seed)\n",
    "val_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.val_mask,\n",
    ")\n",
    "seed_everything(args.seed)\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.test_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 31.59it/s]\n"
     ]
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_loss_att, test_loss_val, test_att_acc, test_val_acc, test_val_jac, test_val_1 = test_Homo(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 46.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7273937661892159,\n",
       " 1.6026624008559125,\n",
       " 100.0,\n",
       " 99.7229916897507,\n",
       " 0.8559556839538743,\n",
       " 87.25761772853186)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_Homo(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 35.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8046205954823068,\n",
       " 1.6253719611708166,\n",
       " 97.96747967479675,\n",
       " 99.01477832512315,\n",
       " 0.7619047681686326,\n",
       " 80.78817733990148)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_Homo(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 37.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8232845396218196,\n",
       " 1.6415579629441102,\n",
       " 97.01789264413519,\n",
       " 99.47916666666667,\n",
       " 0.7465277885397276,\n",
       " 80.72916666666667)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "test_Homo(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 36.91it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 51.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 52.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 52.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 52.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 51.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 53.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 52.37it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 52.09it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 52.51it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 52.04it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 52.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 50.93it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 51.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 52.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 52.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 52.65it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 51.62it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 52.38it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 52.09it/s]\n"
     ]
    }
   ],
   "source": [
    "val_numbers = []\n",
    "test_numbers = []\n",
    "for seed in [0,1,2,42,100,233,1024,1337,2333,4399]:\n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    val_numbers.append(test_Homo(model, val_loader))\n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    test_numbers.append(test_Homo(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame(val_numbers, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])\n",
    "test_df = pd.DataFrame(test_numbers, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.804516</td>\n",
       "      <td>1.625295</td>\n",
       "      <td>9.796748e+01</td>\n",
       "      <td>9.901478e+01</td>\n",
       "      <td>0.762644</td>\n",
       "      <td>79.950739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>1.497956e-14</td>\n",
       "      <td>1.497956e-14</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.237954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.804401</td>\n",
       "      <td>1.625199</td>\n",
       "      <td>9.796748e+01</td>\n",
       "      <td>9.901478e+01</td>\n",
       "      <td>0.759442</td>\n",
       "      <td>79.802956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.804468</td>\n",
       "      <td>1.625247</td>\n",
       "      <td>9.796748e+01</td>\n",
       "      <td>9.901478e+01</td>\n",
       "      <td>0.759442</td>\n",
       "      <td>79.802956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.804474</td>\n",
       "      <td>1.625296</td>\n",
       "      <td>9.796748e+01</td>\n",
       "      <td>9.901478e+01</td>\n",
       "      <td>0.763136</td>\n",
       "      <td>79.802956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.804583</td>\n",
       "      <td>1.625336</td>\n",
       "      <td>9.796748e+01</td>\n",
       "      <td>9.901478e+01</td>\n",
       "      <td>0.764368</td>\n",
       "      <td>80.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.804721</td>\n",
       "      <td>1.625414</td>\n",
       "      <td>9.796748e+01</td>\n",
       "      <td>9.901478e+01</td>\n",
       "      <td>0.766831</td>\n",
       "      <td>80.295567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ATT_loss   VAL_loss       ATT_acc     VAL_k_acc  VAL_k_jac  VAL_1_acc\n",
       "count  10.000000  10.000000  1.000000e+01  1.000000e+01  10.000000  10.000000\n",
       "mean    0.804516   1.625295  9.796748e+01  9.901478e+01   0.762644  79.950739\n",
       "std     0.000101   0.000070  1.497956e-14  1.497956e-14   0.003083   0.237954\n",
       "min     0.804401   1.625199  9.796748e+01  9.901478e+01   0.759442  79.802956\n",
       "25%     0.804468   1.625247  9.796748e+01  9.901478e+01   0.759442  79.802956\n",
       "50%     0.804474   1.625296  9.796748e+01  9.901478e+01   0.763136  79.802956\n",
       "75%     0.804583   1.625336  9.796748e+01  9.901478e+01   0.764368  80.172414\n",
       "max     0.804721   1.625414  9.796748e+01  9.901478e+01   0.766831  80.295567"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.823223</td>\n",
       "      <td>1.641578</td>\n",
       "      <td>9.701789e+01</td>\n",
       "      <td>99.427083</td>\n",
       "      <td>0.745226</td>\n",
       "      <td>80.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.497956e-14</td>\n",
       "      <td>0.164702</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>0.329404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.823118</td>\n",
       "      <td>1.641513</td>\n",
       "      <td>9.701789e+01</td>\n",
       "      <td>98.958333</td>\n",
       "      <td>0.741319</td>\n",
       "      <td>80.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.823172</td>\n",
       "      <td>1.641551</td>\n",
       "      <td>9.701789e+01</td>\n",
       "      <td>99.479167</td>\n",
       "      <td>0.743924</td>\n",
       "      <td>80.338542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.823206</td>\n",
       "      <td>1.641589</td>\n",
       "      <td>9.701789e+01</td>\n",
       "      <td>99.479167</td>\n",
       "      <td>0.745226</td>\n",
       "      <td>80.729167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.823285</td>\n",
       "      <td>1.641604</td>\n",
       "      <td>9.701789e+01</td>\n",
       "      <td>99.479167</td>\n",
       "      <td>0.746528</td>\n",
       "      <td>80.729167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.823330</td>\n",
       "      <td>1.641624</td>\n",
       "      <td>9.701789e+01</td>\n",
       "      <td>99.479167</td>\n",
       "      <td>0.749132</td>\n",
       "      <td>81.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ATT_loss   VAL_loss       ATT_acc  VAL_k_acc  VAL_k_jac  VAL_1_acc\n",
       "count  10.000000  10.000000  1.000000e+01  10.000000  10.000000  10.000000\n",
       "mean    0.823223   1.641578  9.701789e+01  99.427083   0.745226  80.625000\n",
       "std     0.000073   0.000038  1.497956e-14   0.164702   0.002213   0.329404\n",
       "min     0.823118   1.641513  9.701789e+01  98.958333   0.741319  80.208333\n",
       "25%     0.823172   1.641551  9.701789e+01  99.479167   0.743924  80.338542\n",
       "50%     0.823206   1.641589  9.701789e+01  99.479167   0.745226  80.729167\n",
       "75%     0.823285   1.641604  9.701789e+01  99.479167   0.746528  80.729167\n",
       "max     0.823330   1.641624  9.701789e+01  99.479167   0.749132  81.250000"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.to_csv(args.save_dir + 'SOC_val_metrics_transfer.csv', sep='\\t')\n",
    "test_df.to_csv(args.save_dir + 'SOC_test_metrics_transfer.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking Visual and Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT_L(dataset.num_features, 256, dataset.num_classes,\n",
    "            heads=2, dropout=.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(args.save_dir+'GAT_best_model/model.pth',map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT_L(\n",
       "  (conv1): GATConv(1753, 256, heads=2)\n",
       "  (conv2): GATConv(512, 256, heads=1)\n",
       "  (lin1): Linear(1753, 256, bias=True)\n",
       "  (lin2): Linear(512, 20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_Mask(model, loader, mask = None):\n",
    "    model.eval()\n",
    "\n",
    "    total_examples_att = total_examples_val = 0\n",
    "    running_loss_1 = running_loss_2 = 0.\n",
    "    running_1_acc = 0.\n",
    "    running_1_val = 0.\n",
    "    running_k_acc = 0.\n",
    "    running_k_jac = 0.\n",
    "    \n",
    "    for batch in tqdm(loader):\n",
    "        loss_1 = 0\n",
    "        acc_1_t = 0\n",
    "        loss_2 = 0\n",
    "        acc_1_val = 0\n",
    "        acc_k_t = 0\n",
    "        jac_k_t = 0\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch.batch_size\n",
    "        edge_index = to_undirected(batch.edge_index)\n",
    "        \n",
    "        if mask == 'vis':\n",
    "            batch.x[:batch_size,:982] = torch.zeros(batch_size,982)\n",
    "        elif mask == 'tex':\n",
    "            batch.x[:batch_size,982:] = torch.zeros(batch_size,771)\n",
    "        \n",
    "        out = model(batch.x, edge_index)[:batch_size]\n",
    "        out_att = out[:,:9]\n",
    "        out_val = out[:,9:]\n",
    "        att_node = (batch.att_lab[:batch_size]).nonzero().squeeze()\n",
    "        val_node = (batch.val_lab[:batch_size]).nonzero().squeeze()\n",
    "\n",
    "        #print(type_node)\n",
    "\n",
    "        #pred_att = out_att.argmax(dim=-1)\n",
    "        #pred_val = out_val.argmax(dim=-1)\n",
    "\n",
    "        y = batch.y\n",
    "        y_att = y[:,:9]\n",
    "        y_val = y[:,9:]\n",
    "\n",
    "        if not att_node.shape[0]==0:\n",
    "            loss_1 = F.cross_entropy(out_att[att_node], y_att[:batch_size][att_node])\n",
    "            acc_1_t = compute_1_accuracy(y_att[:batch_size][att_node], out_att[att_node])\n",
    "\n",
    "        if not val_node.shape[0]==0:\n",
    "            loss_2 = F.cross_entropy(out_val[val_node], y_val[val_node])\n",
    "            acc_1_val = compute_1_accuracy(y_val[val_node], out_val[val_node])\n",
    "            acc_k_t = compute_k_accuracy(y_val[val_node], out_val[val_node], args.k)\n",
    "            jac_k_t = compute_jaccard_index(y_val[val_node], F.softmax(out_val[val_node],dim=-1), args.k)\n",
    "            #loss_3 = loss_1 + loss_2\n",
    "\n",
    "        total_examples_att += att_node.shape[0]\n",
    "        total_examples_val += val_node.shape[0]\n",
    "        #total_correct_att += int((pred_att == y_att[:batch_size]).sum())\n",
    "        #total_correct_val += int((pred_val == y_val[:batch_size]).sum())\n",
    "\n",
    "        running_loss_1 += float(loss_1) * att_node.shape[0]\n",
    "        running_loss_2 += float(loss_2) * val_node.shape[0]\n",
    "        running_1_acc += float(acc_1_t) * att_node.shape[0]\n",
    "        running_1_val += float(acc_1_val) * val_node.shape[0]\n",
    "        running_k_acc += float(acc_k_t) * val_node.shape[0]\n",
    "        running_k_jac += float(jac_k_t) * val_node.shape[0]\n",
    "    \n",
    "    return running_loss_1/total_examples_att, running_loss_2/total_examples_val, running_1_acc/ total_examples_att, running_k_acc/ total_examples_val, running_k_jac/ total_examples_val, running_1_val/total_examples_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:04<00:00,  2.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.999646943692025,\n",
       " 1.6306794280820935,\n",
       " 27.977839335180054,\n",
       " 99.7229916897507,\n",
       " 0.7479224376731302,\n",
       " 80.60941828254848)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Mask(model, train_loader, 'vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 20.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7632309584736494,\n",
       " 1.90505566095051,\n",
       " 100.0,\n",
       " 85.31855955678671,\n",
       " 0.013850415512465374,\n",
       " 31.57894736842105)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Mask(model, train_loader, 'tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 28.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.0648541254241293,\n",
       " 1.6346842003573339,\n",
       " 21.341463414634145,\n",
       " 99.50738916256158,\n",
       " 0.7561576401658834,\n",
       " 79.80295566502463)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Mask(model, val_loader, 'vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 28.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8126686634571572,\n",
       " 1.9455546393183065,\n",
       " 98.78048780487805,\n",
       " 69.45812807881774,\n",
       " 0.0024630541871921183,\n",
       " 23.645320197044335)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Mask(model, val_loader, 'tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 24.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.093477872683561,\n",
       " 1.646907826885581,\n",
       " 19.4831013916501,\n",
       " 98.95833333333333,\n",
       " 0.7847222263614336,\n",
       " 78.64583333333333)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Mask(model, test_loader, 'vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 26.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8236547274807575,\n",
       " 1.95404616681238,\n",
       " 98.01192842942346,\n",
       " 72.91666666666667,\n",
       " 0.005208333333333333,\n",
       " 23.958333333333332)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Mask(model, test_loader, 'tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 23.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.82it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 35.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 35.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.47it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.11it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 35.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.86it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.59it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 35.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.47it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 35.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 35.05it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 35.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 31.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.48it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 35.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 34.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 33.48it/s]\n"
     ]
    }
   ],
   "source": [
    "val_numbers_vis = []\n",
    "val_numbers_tex = []\n",
    "test_numbers_vis = []\n",
    "test_numbers_tex = []\n",
    "for seed in [0,1,2,42,100,233,1024,1337,2333,4399]:\n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    val_numbers_vis.append(test_Mask(model, val_loader, 'vis'))\n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    val_numbers_tex.append(test_Mask(model, val_loader, 'tex'))\n",
    "    \n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    test_numbers_vis.append(test_Mask(model, test_loader, 'vis'))\n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    test_numbers_tex.append(test_Mask(model, test_loader, 'tex'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_vis = pd.DataFrame(val_numbers_vis, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])\n",
    "val_df_tex = pd.DataFrame(val_numbers_tex, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])\n",
    "test_df_vis = pd.DataFrame(test_numbers_vis, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])\n",
    "test_df_tex = pd.DataFrame(test_numbers_tex, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.065060</td>\n",
       "      <td>1.634533</td>\n",
       "      <td>20.752033</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.761494</td>\n",
       "      <td>79.852217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.545803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>0.155777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.064089</td>\n",
       "      <td>1.634331</td>\n",
       "      <td>19.715447</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.755337</td>\n",
       "      <td>79.802956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.064714</td>\n",
       "      <td>1.634514</td>\n",
       "      <td>20.528455</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.759647</td>\n",
       "      <td>79.802956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.065248</td>\n",
       "      <td>1.634543</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.760263</td>\n",
       "      <td>79.802956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.065504</td>\n",
       "      <td>1.634590</td>\n",
       "      <td>21.138211</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.763342</td>\n",
       "      <td>79.802956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.065790</td>\n",
       "      <td>1.634623</td>\n",
       "      <td>21.544715</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>0.768473</td>\n",
       "      <td>80.295567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ATT_loss   VAL_loss    ATT_acc  VAL_k_acc  VAL_k_jac  VAL_1_acc\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000\n",
       "mean    2.065060   1.634533  20.752033  99.507389   0.761494  79.852217\n",
       "std     0.000605   0.000082   0.545803   0.000000   0.003797   0.155777\n",
       "min     2.064089   1.634331  19.715447  99.507389   0.755337  79.802956\n",
       "25%     2.064714   1.634514  20.528455  99.507389   0.759647  79.802956\n",
       "50%     2.065248   1.634543  20.833333  99.507389   0.760263  79.802956\n",
       "75%     2.065504   1.634590  21.138211  99.507389   0.763342  79.802956\n",
       "max     2.065790   1.634623  21.544715  99.507389   0.768473  80.295567"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df_vis.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.812524</td>\n",
       "      <td>1.945178</td>\n",
       "      <td>98.556911</td>\n",
       "      <td>70.344828</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>22.660099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>0.605553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.812248</td>\n",
       "      <td>1.944565</td>\n",
       "      <td>98.373984</td>\n",
       "      <td>69.458128</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>22.167488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.812429</td>\n",
       "      <td>1.944924</td>\n",
       "      <td>98.577236</td>\n",
       "      <td>69.950739</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>22.290640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.812535</td>\n",
       "      <td>1.945154</td>\n",
       "      <td>98.577236</td>\n",
       "      <td>70.197044</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>22.660099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.812618</td>\n",
       "      <td>1.945418</td>\n",
       "      <td>98.577236</td>\n",
       "      <td>70.812808</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>22.660099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.812745</td>\n",
       "      <td>1.946028</td>\n",
       "      <td>98.780488</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>23.645320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ATT_loss   VAL_loss    ATT_acc  VAL_k_acc  VAL_k_jac  VAL_1_acc\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000\n",
       "mean    0.812524   1.945178  98.556911  70.344828   0.002463  22.660099\n",
       "std     0.000153   0.000433   0.115375   0.605553   0.000000   0.464438\n",
       "min     0.812248   1.944565  98.373984  69.458128   0.002463  22.167488\n",
       "25%     0.812429   1.944924  98.577236  69.950739   0.002463  22.290640\n",
       "50%     0.812535   1.945154  98.577236  70.197044   0.002463  22.660099\n",
       "75%     0.812618   1.945418  98.577236  70.812808   0.002463  22.660099\n",
       "max     0.812745   1.946028  98.780488  71.428571   0.002463  23.645320"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df_tex.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.094852</td>\n",
       "      <td>1.646928</td>\n",
       "      <td>19.960239</td>\n",
       "      <td>9.895833e+01</td>\n",
       "      <td>0.785417</td>\n",
       "      <td>78.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.488776</td>\n",
       "      <td>1.497956e-14</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.428788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.094065</td>\n",
       "      <td>1.646764</td>\n",
       "      <td>19.085487</td>\n",
       "      <td>9.895833e+01</td>\n",
       "      <td>0.782118</td>\n",
       "      <td>77.604167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.094645</td>\n",
       "      <td>1.646878</td>\n",
       "      <td>19.681909</td>\n",
       "      <td>9.895833e+01</td>\n",
       "      <td>0.783854</td>\n",
       "      <td>78.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.094886</td>\n",
       "      <td>1.646906</td>\n",
       "      <td>19.980119</td>\n",
       "      <td>9.895833e+01</td>\n",
       "      <td>0.785590</td>\n",
       "      <td>78.385417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.095123</td>\n",
       "      <td>1.647002</td>\n",
       "      <td>20.278330</td>\n",
       "      <td>9.895833e+01</td>\n",
       "      <td>0.787326</td>\n",
       "      <td>78.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.095313</td>\n",
       "      <td>1.647062</td>\n",
       "      <td>20.675944</td>\n",
       "      <td>9.895833e+01</td>\n",
       "      <td>0.789063</td>\n",
       "      <td>78.645833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ATT_loss   VAL_loss    ATT_acc     VAL_k_acc  VAL_k_jac  VAL_1_acc\n",
       "count  10.000000  10.000000  10.000000  1.000000e+01  10.000000  10.000000\n",
       "mean    2.094852   1.646928  19.960239  9.895833e+01   0.785417  78.281250\n",
       "std     0.000377   0.000094   0.488776  1.497956e-14   0.002414   0.428788\n",
       "min     2.094065   1.646764  19.085487  9.895833e+01   0.782118  77.604167\n",
       "25%     2.094645   1.646878  19.681909  9.895833e+01   0.783854  78.125000\n",
       "50%     2.094886   1.646906  19.980119  9.895833e+01   0.785590  78.385417\n",
       "75%     2.095123   1.647002  20.278330  9.895833e+01   0.787326  78.645833\n",
       "max     2.095313   1.647062  20.675944  9.895833e+01   0.789063  78.645833"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_vis.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.823647</td>\n",
       "      <td>1.953603</td>\n",
       "      <td>97.992048</td>\n",
       "      <td>71.354167</td>\n",
       "      <td>5.208333e-03</td>\n",
       "      <td>22.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.174075</td>\n",
       "      <td>0.549007</td>\n",
       "      <td>9.142795e-19</td>\n",
       "      <td>0.857576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.823451</td>\n",
       "      <td>1.953091</td>\n",
       "      <td>97.813121</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>5.208333e-03</td>\n",
       "      <td>21.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.823531</td>\n",
       "      <td>1.953449</td>\n",
       "      <td>97.813121</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>5.208333e-03</td>\n",
       "      <td>22.005208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.823640</td>\n",
       "      <td>1.953584</td>\n",
       "      <td>98.011928</td>\n",
       "      <td>71.354167</td>\n",
       "      <td>5.208333e-03</td>\n",
       "      <td>22.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.823745</td>\n",
       "      <td>1.953738</td>\n",
       "      <td>98.161034</td>\n",
       "      <td>71.744792</td>\n",
       "      <td>5.208333e-03</td>\n",
       "      <td>22.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.823916</td>\n",
       "      <td>1.954276</td>\n",
       "      <td>98.210736</td>\n",
       "      <td>72.395833</td>\n",
       "      <td>5.208333e-03</td>\n",
       "      <td>23.958333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ATT_loss   VAL_loss    ATT_acc  VAL_k_acc     VAL_k_jac  VAL_1_acc\n",
       "count  10.000000  10.000000  10.000000  10.000000  1.000000e+01  10.000000\n",
       "mean    0.823647   1.953603  97.992048  71.354167  5.208333e-03  22.708333\n",
       "std     0.000147   0.000314   0.174075   0.549007  9.142795e-19   0.857576\n",
       "min     0.823451   1.953091  97.813121  70.833333  5.208333e-03  21.354167\n",
       "25%     0.823531   1.953449  97.813121  70.833333  5.208333e-03  22.005208\n",
       "50%     0.823640   1.953584  98.011928  71.354167  5.208333e-03  22.916667\n",
       "75%     0.823745   1.953738  98.161034  71.744792  5.208333e-03  22.916667\n",
       "max     0.823916   1.954276  98.210736  72.395833  5.208333e-03  23.958333"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_tex.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_vis.to_csv(args.save_dir + 'vis_masked_val_metrics.csv', sep='\\t')\n",
    "val_df_tex.to_csv(args.save_dir + 'tex_masked_val_metrics.csv', sep='\\t')\n",
    "test_df_vis.to_csv(args.save_dir + 'vis_masked_test_metrics.csv', sep='\\t')\n",
    "test_df_tex.to_csv(args.save_dir + 'tex_masked_test_metrics.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using kNN Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_knn():\n",
    "    set_seed_everywhere(args.seed, args.cuda)\n",
    "    #transform = T.Compose([T.ToSparseTensor()])\n",
    "    dataset = VEN_Homo('dataset/Venice_homo')\n",
    "    data = dataset[0]\n",
    "    data.n_id = torch.arange(data.num_nodes)\n",
    "    edge_index = knn_graph(data.x[:,:982].to(device), k=3, loop=False, cosine=True)\n",
    "    data.edge_index = edge_index\n",
    "    data = data.to(device)\n",
    "    \n",
    "    train_loader = NeighborLoader(\n",
    "        data,\n",
    "        # Sample 25 neighbors for each node and edge type for 2 iterations\n",
    "        num_neighbors=[3*args.sample_nodes] * 2,\n",
    "        # Use a batch size of 32 for sampling training nodes\n",
    "        batch_size=args.batch_size,\n",
    "        input_nodes=data.train_mask,\n",
    "    )\n",
    "    val_loader = NeighborLoader(\n",
    "        data,\n",
    "        # Sample 25 neighbors for each node and edge type for 2 iterations\n",
    "        num_neighbors=[3*args.sample_nodes] * 2,\n",
    "        # Use a batch size of 32 for sampling validating nodes\n",
    "        batch_size=args.batch_size,\n",
    "        input_nodes=data.val_mask,\n",
    "    )\n",
    "    test_loader = NeighborLoader(\n",
    "        data,\n",
    "        # Sample 25 neighbors for each node and edge type for 2 iterations\n",
    "        num_neighbors=[3*args.sample_nodes] * 2,\n",
    "        # Use a batch size of 32 for sampling testing nodes\n",
    "        batch_size=args.batch_size,\n",
    "        input_nodes=data.test_mask,\n",
    "    )\n",
    " \n",
    "    model = GAT_L(in_channels=data.x.shape[-1], hidden_channels = 256, \n",
    "            out_channels = data.y.shape[-1], dropout = 0.1, heads=2).to(device)\n",
    "    return data, model, train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_knn(verbose=False):\n",
    "    \n",
    "    _, model, train_loader, val_loader, test_loader = initialization_knn()\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Use {} GPUs !\".format(torch.cuda.device_count()))\n",
    "        model = DataParallel(model)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=args.l2)\n",
    "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "    #                                           mode='min', factor=0.5,\n",
    "    #                                           patience=1)\n",
    "\n",
    "    train_state = make_train_state(args)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(args.num_epochs):\n",
    "            train_state['epoch_index'] = epoch\n",
    "            \n",
    "            loss = train_Homo(model, optimizer, train_loader)\n",
    "            train_loss_att, train_loss_val, train_att_acc, train_val_acc, train_val_jac, train_val_1 = test_Homo(model, train_loader)\n",
    "            val_loss_att, val_loss_val, val_att_acc, val_val_acc, val_val_jac, val_val_1 = test_Homo(model, val_loader)\n",
    "            if verbose:\n",
    "                print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train_ATT: {train_att_acc:.4f}, Train_VAL: {train_val_acc:.4f}, Val_vis_tex_ATT: {val_att_acc:.4f}, Val_vis_tex_VAL: {val_val_acc:.4f}')\n",
    "            \n",
    "            train_state['train_loss'].append(loss)\n",
    "            train_state['train_ATT_loss'].append(train_loss_att)\n",
    "            train_state['train_VAL_loss'].append(train_loss_val)\n",
    "            train_state['train_ATT_acc'].append(train_att_acc)\n",
    "            train_state['train_VAL_acc'].append(train_val_acc)\n",
    "            train_state['train_VAL_jac'].append(train_val_jac)\n",
    "            train_state['train_VAL_acc_1'].append(train_val_1)\n",
    "            \n",
    "            train_state['val_ATT_loss'].append(val_loss_att)\n",
    "            train_state['val_VAL_loss'].append(val_loss_val)\n",
    "            train_state['val_loss'].append(val_loss_att + 3*val_loss_val)\n",
    "            train_state['val_ATT_acc'].append(val_att_acc)\n",
    "            train_state['val_VAL_acc'].append(val_val_acc)\n",
    "            train_state['val_VAL_jac'].append(val_val_jac)\n",
    "            train_state['val_VAL_acc_1'].append(val_val_1)\n",
    "            \n",
    "            train_state = update_train_state(args=args, model=model,\n",
    "                                                train_state=train_state)\n",
    "            if train_state['stop_early']:\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting loop\")\n",
    "        pass\n",
    "    \n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_state = training_loop_knn(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = osp.join(os.getcwd(), '../../data/VEN')\n",
    "transform = T.Compose([T.NormalizeFeatures(), T.ToSparseTensor()])\n",
    "dataset = VEN_Homo('dataset/Venice_homo')\n",
    "data_0 = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = knn_graph(data_0.x[:,:982].to(device), k=3, loop=False, cosine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=2951, x=[2951, 1753], y=[2951, 20], node_type=[2951], att_lab=[2951], val_lab=[2951], train_mask=[2951], val_mask=[2951], test_mask=[2951], edge_index=[2, 8853], edge_attr=[1071977])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_0.edge_index = edge_index\n",
    "data_0 = data_0.to(device)\n",
    "data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "train_loader = NeighborLoader(\n",
    "    data_0,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.train_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "val_loader = NeighborLoader(\n",
    "    data_0,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.val_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "test_loader = NeighborLoader(\n",
    "    data_0,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data.test_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT_L(in_channels=data_0.x.shape[-1], hidden_channels = 256, \n",
    "            out_channels = data.y.shape[-1], dropout = 0.1, heads=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(args.save_dir+'GAT_knn_feature/model.pth',map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 185.33it/s]\n"
     ]
    }
   ],
   "source": [
    "test_loss_att, test_loss_val, test_att_acc, test_val_acc, test_val_jac, test_val_1 = test_Homo(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 162.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7305126030029022,\n",
       " 1.6036959784182816,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 0.8374884650317586,\n",
       " 90.02770083102493)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Homo(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 69.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8216052069896604,\n",
       " 1.622527096071854,\n",
       " 97.15447154471545,\n",
       " 99.50738916256158,\n",
       " 0.7479474603248935,\n",
       " 80.29556650246306)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Homo(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 136.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8268375741558568,\n",
       " 1.639881905168295,\n",
       " 95.82504970178927,\n",
       " 100.0,\n",
       " 0.7630208432674408,\n",
       " 79.16666666666667)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Homo(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 102.86it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 137.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 300.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 306.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 309.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 300.53it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 315.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 289.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 301.00it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 318.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 312.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 312.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 286.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 289.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 285.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 308.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 304.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 325.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 317.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 283.60it/s]\n"
     ]
    }
   ],
   "source": [
    "val_numbers = []\n",
    "test_numbers = []\n",
    "for seed in [0,1,2,42,100,233,1024,1337,2333,4399]:\n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    val_numbers.append(test_Homo(model, val_loader))\n",
    "    set_seed_everywhere(seed, args.cuda)\n",
    "    test_numbers.append(test_Homo(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame(val_numbers, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])\n",
    "test_df = pd.DataFrame(test_numbers, columns=['ATT_loss', 'VAL_loss', 'ATT_acc', 'VAL_k_acc', 'VAL_k_jac', 'VAL_1_acc'],\n",
    "            index = [0,1,2,42,100,233,1024,1337,2333,4399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.216052e-01</td>\n",
       "      <td>1.622527e+00</td>\n",
       "      <td>9.715447e+01</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>7.479475e-01</td>\n",
       "      <td>8.029557e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.134308e-08</td>\n",
       "      <td>1.723721e-08</td>\n",
       "      <td>1.497956e-14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.170278e-16</td>\n",
       "      <td>1.497956e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.216052e-01</td>\n",
       "      <td>1.622527e+00</td>\n",
       "      <td>9.715447e+01</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>7.479475e-01</td>\n",
       "      <td>8.029557e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.216052e-01</td>\n",
       "      <td>1.622527e+00</td>\n",
       "      <td>9.715447e+01</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>7.479475e-01</td>\n",
       "      <td>8.029557e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.216052e-01</td>\n",
       "      <td>1.622527e+00</td>\n",
       "      <td>9.715447e+01</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>7.479475e-01</td>\n",
       "      <td>8.029557e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.216052e-01</td>\n",
       "      <td>1.622527e+00</td>\n",
       "      <td>9.715447e+01</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>7.479475e-01</td>\n",
       "      <td>8.029557e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.216052e-01</td>\n",
       "      <td>1.622527e+00</td>\n",
       "      <td>9.715447e+01</td>\n",
       "      <td>99.507389</td>\n",
       "      <td>7.479475e-01</td>\n",
       "      <td>8.029557e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ATT_loss      VAL_loss       ATT_acc  VAL_k_acc     VAL_k_jac  \\\n",
       "count  1.000000e+01  1.000000e+01  1.000000e+01  10.000000  1.000000e+01   \n",
       "mean   8.216052e-01  1.622527e+00  9.715447e+01  99.507389  7.479475e-01   \n",
       "std    1.134308e-08  1.723721e-08  1.497956e-14   0.000000  1.170278e-16   \n",
       "min    8.216052e-01  1.622527e+00  9.715447e+01  99.507389  7.479475e-01   \n",
       "25%    8.216052e-01  1.622527e+00  9.715447e+01  99.507389  7.479475e-01   \n",
       "50%    8.216052e-01  1.622527e+00  9.715447e+01  99.507389  7.479475e-01   \n",
       "75%    8.216052e-01  1.622527e+00  9.715447e+01  99.507389  7.479475e-01   \n",
       "max    8.216052e-01  1.622527e+00  9.715447e+01  99.507389  7.479475e-01   \n",
       "\n",
       "          VAL_1_acc  \n",
       "count  1.000000e+01  \n",
       "mean   8.029557e+01  \n",
       "std    1.497956e-14  \n",
       "min    8.029557e+01  \n",
       "25%    8.029557e+01  \n",
       "50%    8.029557e+01  \n",
       "75%    8.029557e+01  \n",
       "max    8.029557e+01  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT_loss</th>\n",
       "      <th>VAL_loss</th>\n",
       "      <th>ATT_acc</th>\n",
       "      <th>VAL_k_acc</th>\n",
       "      <th>VAL_k_jac</th>\n",
       "      <th>VAL_1_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.268376e-01</td>\n",
       "      <td>1.639882e+00</td>\n",
       "      <td>9.582505e+01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.763021</td>\n",
       "      <td>7.916667e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.507309e-09</td>\n",
       "      <td>2.851020e-08</td>\n",
       "      <td>1.497956e-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.497956e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.268376e-01</td>\n",
       "      <td>1.639882e+00</td>\n",
       "      <td>9.582505e+01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.763021</td>\n",
       "      <td>7.916667e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.268376e-01</td>\n",
       "      <td>1.639882e+00</td>\n",
       "      <td>9.582505e+01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.763021</td>\n",
       "      <td>7.916667e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.268376e-01</td>\n",
       "      <td>1.639882e+00</td>\n",
       "      <td>9.582505e+01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.763021</td>\n",
       "      <td>7.916667e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.268376e-01</td>\n",
       "      <td>1.639882e+00</td>\n",
       "      <td>9.582505e+01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.763021</td>\n",
       "      <td>7.916667e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.268376e-01</td>\n",
       "      <td>1.639882e+00</td>\n",
       "      <td>9.582505e+01</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.763021</td>\n",
       "      <td>7.916667e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ATT_loss      VAL_loss       ATT_acc  VAL_k_acc  VAL_k_jac  \\\n",
       "count  1.000000e+01  1.000000e+01  1.000000e+01       10.0  10.000000   \n",
       "mean   8.268376e-01  1.639882e+00  9.582505e+01      100.0   0.763021   \n",
       "std    9.507309e-09  2.851020e-08  1.497956e-14        0.0   0.000000   \n",
       "min    8.268376e-01  1.639882e+00  9.582505e+01      100.0   0.763021   \n",
       "25%    8.268376e-01  1.639882e+00  9.582505e+01      100.0   0.763021   \n",
       "50%    8.268376e-01  1.639882e+00  9.582505e+01      100.0   0.763021   \n",
       "75%    8.268376e-01  1.639882e+00  9.582505e+01      100.0   0.763021   \n",
       "max    8.268376e-01  1.639882e+00  9.582505e+01      100.0   0.763021   \n",
       "\n",
       "          VAL_1_acc  \n",
       "count  1.000000e+01  \n",
       "mean   7.916667e+01  \n",
       "std    1.497956e-14  \n",
       "min    7.916667e+01  \n",
       "25%    7.916667e+01  \n",
       "50%    7.916667e+01  \n",
       "75%    7.916667e+01  \n",
       "max    7.916667e+01  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.to_csv(args.save_dir + 'knn_val_metrics.csv', sep='\\t')\n",
    "test_df.to_csv(args.save_dir + 'knn_test_metrics.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state['test_ATT_loss']=test_loss_att\n",
    "train_state['test_VAL_loss']=test_loss_val\n",
    "train_state['test_loss']=test_loss_att + 3*test_loss_val\n",
    "train_state['test_ATT_acc']=test_att_acc\n",
    "train_state['test_VAL_acc_1']=test_val_1\n",
    "train_state['test_VAL_acc']=test_val_acc\n",
    "train_state['test_VAL_jac']=test_val_jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': True,\n",
       " 'early_stopping_step': 100,\n",
       " 'early_stopping_best_ATT_acc_val': 97.35772357723577,\n",
       " 'early_stopping_best_VAL_acc_val': 100.0,\n",
       " 'early_stopping_best_ATT_acc_val_2': 0,\n",
       " 'early_stopping_lowest_loss': 5.689186611493243,\n",
       " 'learning_rate': 0.0001,\n",
       " 'epoch_index': 216,\n",
       " 'train_loss': [3.9013789097468057,\n",
       "  3.3044732213020325,\n",
       "  3.063999434312185,\n",
       "  2.876014073689779,\n",
       "  2.753539582093557,\n",
       "  2.6616336504618325,\n",
       "  2.5993056495984397,\n",
       "  2.5452094872792563,\n",
       "  2.519130210081736,\n",
       "  2.4983943502108255,\n",
       "  2.4804656704266868,\n",
       "  2.4660059611002603,\n",
       "  2.4520874619483948,\n",
       "  2.4471658865610757,\n",
       "  2.4305782516797385,\n",
       "  2.4196654756863913,\n",
       "  2.424438774585724,\n",
       "  2.414077083269755,\n",
       "  2.4156991640726724,\n",
       "  2.4109241366386414,\n",
       "  2.4062357743581138,\n",
       "  2.3997324109077454,\n",
       "  2.3996223409970603,\n",
       "  2.3959054946899414,\n",
       "  2.390456259250641,\n",
       "  2.3920201460520425,\n",
       "  2.3935344417889914,\n",
       "  2.3898675044377646,\n",
       "  2.3823262651761374,\n",
       "  2.3811779022216797,\n",
       "  2.380528966585795,\n",
       "  2.37201456228892,\n",
       "  2.3697983423868814,\n",
       "  2.3692850867907205,\n",
       "  2.3701539436976113,\n",
       "  2.369903286298116,\n",
       "  2.3663511872291565,\n",
       "  2.3662860989570618,\n",
       "  2.361597001552582,\n",
       "  2.3580870628356934,\n",
       "  2.3600632747014365,\n",
       "  2.3611660401026406,\n",
       "  2.35589329401652,\n",
       "  2.356559077898661,\n",
       "  2.355980336666107,\n",
       "  2.3527438839276633,\n",
       "  2.3525700171788535,\n",
       "  2.3546069860458374,\n",
       "  2.3523265719413757,\n",
       "  2.3500893115997314,\n",
       "  2.34830242395401,\n",
       "  2.348533848921458,\n",
       "  2.346158961455027,\n",
       "  2.3450563152631125,\n",
       "  2.3438262740770974,\n",
       "  2.3412606517473855,\n",
       "  2.344762106736501,\n",
       "  2.3438493808110556,\n",
       "  2.3444604873657227,\n",
       "  2.343117654323578,\n",
       "  2.337418814500173,\n",
       "  2.3416635394096375,\n",
       "  2.3409508069356284,\n",
       "  2.3419528206189475,\n",
       "  2.3409248987833657,\n",
       "  2.340092937151591,\n",
       "  2.3403691053390503,\n",
       "  2.3390488028526306,\n",
       "  2.337515711784363,\n",
       "  2.3377736806869507,\n",
       "  2.335831264654795,\n",
       "  2.333675762017568,\n",
       "  2.3315987984339395,\n",
       "  2.333753844102224,\n",
       "  2.3336262305577598,\n",
       "  2.331785579522451,\n",
       "  2.3333584467569985,\n",
       "  2.3331743081410727,\n",
       "  2.3330628077189126,\n",
       "  2.33286186059316,\n",
       "  2.331745187441508,\n",
       "  2.3286930322647095,\n",
       "  2.329561948776245,\n",
       "  2.331435044606527,\n",
       "  2.33100418249766,\n",
       "  2.3295982480049133,\n",
       "  2.329732616742452,\n",
       "  2.3280714948972068,\n",
       "  2.325533409913381,\n",
       "  2.3271875381469727,\n",
       "  2.3277432719866433,\n",
       "  2.326240122318268,\n",
       "  2.324749310811361,\n",
       "  2.3248435457547507,\n",
       "  2.324320395787557,\n",
       "  2.325072685877482,\n",
       "  2.3260463078816733,\n",
       "  2.324484407901764,\n",
       "  2.324164628982544,\n",
       "  2.3244160811106362,\n",
       "  2.3230260809262595,\n",
       "  2.3228655656178794,\n",
       "  2.3230655988057456,\n",
       "  2.322174608707428,\n",
       "  2.32227889696757,\n",
       "  2.3256950775782266,\n",
       "  2.324086626370748,\n",
       "  2.323958178361257,\n",
       "  2.3239632646242776,\n",
       "  2.320767800013224,\n",
       "  2.322141488393148,\n",
       "  2.321998397509257,\n",
       "  2.323688546816508,\n",
       "  2.3242374857266745,\n",
       "  2.3225953181584678,\n",
       "  2.3221940398216248,\n",
       "  2.3228743275006614,\n",
       "  2.323632597923279,\n",
       "  2.3221842845280967,\n",
       "  2.3216541012128196,\n",
       "  2.3227697809537253,\n",
       "  2.323441207408905,\n",
       "  2.324722627798716,\n",
       "  2.3233086665471396,\n",
       "  2.3249409000078836,\n",
       "  2.3226672212282815,\n",
       "  2.321860651175181,\n",
       "  2.3215746879577637,\n",
       "  2.3230021397272744,\n",
       "  2.319094181060791,\n",
       "  2.3169965942700705,\n",
       "  2.316856841246287,\n",
       "  2.317968805631002,\n",
       "  2.3157885670661926,\n",
       "  2.3181766271591187,\n",
       "  2.3193138043085733,\n",
       "  2.3170297344525657,\n",
       "  2.315159022808075,\n",
       "  2.314607620239258,\n",
       "  2.317909836769104,\n",
       "  2.3165202339490256,\n",
       "  2.316524545351664,\n",
       "  2.3169257640838623,\n",
       "  2.318154990673065,\n",
       "  2.315414309501648,\n",
       "  2.3159154057502747,\n",
       "  2.3153666853904724,\n",
       "  2.3185481627782187,\n",
       "  2.319142997264862,\n",
       "  2.3168526689211526,\n",
       "  2.3201963702837625,\n",
       "  2.3191670775413513,\n",
       "  2.3136077324549356,\n",
       "  2.3160784443219504,\n",
       "  2.3152659932772317,\n",
       "  2.314660827318827,\n",
       "  2.3131484190622964,\n",
       "  2.3141521414120994,\n",
       "  2.314752439657847,\n",
       "  2.3122166792551675,\n",
       "  2.312665263811747,\n",
       "  2.3151030937830606,\n",
       "  2.313265840212504,\n",
       "  2.3135782281557717,\n",
       "  2.3131203651428223,\n",
       "  2.311948597431183,\n",
       "  2.3126655220985413,\n",
       "  2.3144569595654807,\n",
       "  2.3151902556419373,\n",
       "  2.3144580721855164,\n",
       "  2.3114023009936013,\n",
       "  2.3112278978029885,\n",
       "  2.3130096991856894,\n",
       "  2.310043295224508,\n",
       "  2.3118812640508017,\n",
       "  2.3095643520355225,\n",
       "  2.3117295503616333,\n",
       "  2.310691257317861,\n",
       "  2.3099008798599243,\n",
       "  2.310289124647776,\n",
       "  2.31043932835261,\n",
       "  2.310156504313151,\n",
       "  2.309151808420817,\n",
       "  2.310096244017283,\n",
       "  2.3112393021583557,\n",
       "  2.3097328543663025,\n",
       "  2.311374564965566,\n",
       "  2.3109683791796365,\n",
       "  2.3100127379099527,\n",
       "  2.3108014265696206,\n",
       "  2.3118702371915183,\n",
       "  2.3090566992759705,\n",
       "  2.3119232455889382,\n",
       "  2.312085827191671,\n",
       "  2.311039626598358,\n",
       "  2.310037354628245,\n",
       "  2.308786610762278,\n",
       "  2.30816912651062,\n",
       "  2.3090386390686035,\n",
       "  2.3090526262919107,\n",
       "  2.3086772759755454,\n",
       "  2.3083658615748086,\n",
       "  2.307975192864736,\n",
       "  2.308564086755117,\n",
       "  2.3095143834749856,\n",
       "  2.308999319871267,\n",
       "  2.307913919289907,\n",
       "  2.3091397285461426,\n",
       "  2.309756557146708,\n",
       "  2.3092787067095437,\n",
       "  2.3094342152277627,\n",
       "  2.309023916721344,\n",
       "  2.3110435605049133,\n",
       "  2.3117740750312805,\n",
       "  2.310886323451996,\n",
       "  2.3133356173833213,\n",
       "  2.3126059571901956],\n",
       " 'train_ATT_loss': [1.6703123425512763,\n",
       "  1.4455454210825573,\n",
       "  1.2524783432979003,\n",
       "  1.1105111464238893,\n",
       "  1.010668070692765,\n",
       "  0.9442297226834495,\n",
       "  0.9018898132435172,\n",
       "  0.8703969221696299,\n",
       "  0.8497977375653972,\n",
       "  0.8342918253341209,\n",
       "  0.823805843859168,\n",
       "  0.8127313381747195,\n",
       "  0.8054599453207528,\n",
       "  0.799949260134446,\n",
       "  0.7939267627420188,\n",
       "  0.7911593849639152,\n",
       "  0.7863204490775216,\n",
       "  0.7817103502823045,\n",
       "  0.7794440370517425,\n",
       "  0.7759600803462422,\n",
       "  0.7741309765963673,\n",
       "  0.7707456190170013,\n",
       "  0.7700663598620661,\n",
       "  0.7675529540080444,\n",
       "  0.7652685790841269,\n",
       "  0.7640182354443621,\n",
       "  0.7652119820798203,\n",
       "  0.7607100957648576,\n",
       "  0.7591091746108354,\n",
       "  0.7605631234242975,\n",
       "  0.7562401045392425,\n",
       "  0.7542633969037487,\n",
       "  0.7532073526831545,\n",
       "  0.7524743762042714,\n",
       "  0.7520083803219147,\n",
       "  0.7521097286916506,\n",
       "  0.7493764158761402,\n",
       "  0.748187159897548,\n",
       "  0.7484317241943443,\n",
       "  0.747479058864044,\n",
       "  0.7464405542595565,\n",
       "  0.7452290976146582,\n",
       "  0.745644304891042,\n",
       "  0.7445864045058591,\n",
       "  0.7439944460121218,\n",
       "  0.7436050031323842,\n",
       "  0.7429002308779477,\n",
       "  0.7422949957385288,\n",
       "  0.7416596077485759,\n",
       "  0.7417738165221386,\n",
       "  0.7401565899479092,\n",
       "  0.7395397343794065,\n",
       "  0.7399412953292234,\n",
       "  0.7388021465483795,\n",
       "  0.737940185122873,\n",
       "  0.7382534986717879,\n",
       "  0.739077279442235,\n",
       "  0.7389212655228591,\n",
       "  0.737337813152831,\n",
       "  0.73736643543534,\n",
       "  0.736423618245323,\n",
       "  0.7366249972433264,\n",
       "  0.7352659776600444,\n",
       "  0.7357221670097922,\n",
       "  0.7359204047934831,\n",
       "  0.7359873650807093,\n",
       "  0.7346621347266221,\n",
       "  0.7353455155179771,\n",
       "  0.734642601574557,\n",
       "  0.7342834259664583,\n",
       "  0.7342617394521296,\n",
       "  0.73266542172498,\n",
       "  0.7335737057009563,\n",
       "  0.7340481345673348,\n",
       "  0.7331413372401716,\n",
       "  0.7326976610683008,\n",
       "  0.7347635499658347,\n",
       "  0.7343193271153521,\n",
       "  0.7350267886785259,\n",
       "  0.7330581078568984,\n",
       "  0.7314294251378554,\n",
       "  0.7315636480283869,\n",
       "  0.7326116847529636,\n",
       "  0.7318126516328954,\n",
       "  0.7334271454745053,\n",
       "  0.7319552858780626,\n",
       "  0.7303787038927263,\n",
       "  0.7303699804805323,\n",
       "  0.7305126135699307,\n",
       "  0.7319323717722271,\n",
       "  0.7301201305230899,\n",
       "  0.7296492326953075,\n",
       "  0.729597564714437,\n",
       "  0.7294994404441432,\n",
       "  0.729609235336906,\n",
       "  0.728862943246424,\n",
       "  0.7302205153119201,\n",
       "  0.7296018412238673,\n",
       "  0.7289550669305542,\n",
       "  0.7287479224627699,\n",
       "  0.7280282957732183,\n",
       "  0.7284637615291035,\n",
       "  0.7289007387002749,\n",
       "  0.7319177552603618,\n",
       "  0.7278194521602831,\n",
       "  0.7287821527002921,\n",
       "  0.7294419673671353,\n",
       "  0.7271556912036483,\n",
       "  0.7279469897542303,\n",
       "  0.7285974075259264,\n",
       "  0.7283969224324848,\n",
       "  0.7288301819909643,\n",
       "  0.7292968366614999,\n",
       "  0.7278620181321437,\n",
       "  0.727131896071817,\n",
       "  0.7267731708833055,\n",
       "  0.7288623539694788,\n",
       "  0.7281381608376543,\n",
       "  0.7285112215211187,\n",
       "  0.7278185179689254,\n",
       "  0.7277427811041433,\n",
       "  0.7276751892718581,\n",
       "  0.7277164561596604,\n",
       "  0.7285784430120791,\n",
       "  0.7288285305625514,\n",
       "  0.7258214133267917,\n",
       "  0.7274536711687527,\n",
       "  0.7271198656750518,\n",
       "  0.726828133135291,\n",
       "  0.7257240391504071,\n",
       "  0.7259544089560364,\n",
       "  0.7255596039698065,\n",
       "  0.7258593378965214,\n",
       "  0.7260250079334608,\n",
       "  0.7268637167780023,\n",
       "  0.7256135724257895,\n",
       "  0.7267175531453373,\n",
       "  0.7258177842129631,\n",
       "  0.7267478512925124,\n",
       "  0.7272582546165445,\n",
       "  0.7275014319908586,\n",
       "  0.7259756708079098,\n",
       "  0.7257267550418252,\n",
       "  0.7258525513876178,\n",
       "  0.7255465059729495,\n",
       "  0.7280070000738318,\n",
       "  0.730450572894881,\n",
       "  0.7332489668827638,\n",
       "  0.7271344425605605,\n",
       "  0.7250790132049708,\n",
       "  0.7290524860168097,\n",
       "  0.7292194239320517,\n",
       "  0.7281624795327226,\n",
       "  0.7264639477650545,\n",
       "  0.7252124883131307,\n",
       "  0.7257029997014603,\n",
       "  0.7256559600790452,\n",
       "  0.7260614427172907,\n",
       "  0.7248989284203654,\n",
       "  0.723982420819618,\n",
       "  0.725025879874454,\n",
       "  0.7263637761330011,\n",
       "  0.726732432512035,\n",
       "  0.7254641046154202,\n",
       "  0.7248424138058586,\n",
       "  0.7240662734924591,\n",
       "  0.7257841300436004,\n",
       "  0.7265795882058605,\n",
       "  0.7252607685707283,\n",
       "  0.7245349558766859,\n",
       "  0.7240272613443496,\n",
       "  0.7252636094502795,\n",
       "  0.724119121678318,\n",
       "  0.7237205619296869,\n",
       "  0.7232537198595063,\n",
       "  0.723356262633675,\n",
       "  0.7237839323992215,\n",
       "  0.7239905331603708,\n",
       "  0.7240013138409136,\n",
       "  0.7233227526051846,\n",
       "  0.7234579250422871,\n",
       "  0.7233142345896058,\n",
       "  0.7239533580241111,\n",
       "  0.7237220882048567,\n",
       "  0.7230414310650812,\n",
       "  0.7231686386044996,\n",
       "  0.7238235724599738,\n",
       "  0.7237691172602434,\n",
       "  0.7238029099567445,\n",
       "  0.7240052622771329,\n",
       "  0.7248836443695005,\n",
       "  0.7274082790121147,\n",
       "  0.7240586117364033,\n",
       "  0.7237669440848015,\n",
       "  0.7253245909458382,\n",
       "  0.723839928073566,\n",
       "  0.7224228709028038,\n",
       "  0.7233301151822479,\n",
       "  0.7244074450305295,\n",
       "  0.7236786801729176,\n",
       "  0.7225652053085391,\n",
       "  0.7226674933182565,\n",
       "  0.7225451324124745,\n",
       "  0.7228740390978361,\n",
       "  0.7228996400357613,\n",
       "  0.7221050662016935,\n",
       "  0.7227293893571045,\n",
       "  0.7224950818473943,\n",
       "  0.723465484430255,\n",
       "  0.7232207264596405,\n",
       "  0.7228506103446939,\n",
       "  0.7246219803091561,\n",
       "  0.7251030276026422,\n",
       "  0.7237290883658666,\n",
       "  0.7283622490402074,\n",
       "  0.7291163055850528,\n",
       "  0.728411727169544],\n",
       " 'train_VAL_loss': [1.7723907189355992,\n",
       "  1.7200188210796452,\n",
       "  1.6988592643156606,\n",
       "  1.6833924431549876,\n",
       "  1.6735492961228389,\n",
       "  1.6638847823948741,\n",
       "  1.6563964111322842,\n",
       "  1.650989502420716,\n",
       "  1.6471911576315967,\n",
       "  1.6426148880221507,\n",
       "  1.6394205872702137,\n",
       "  1.637348282039991,\n",
       "  1.6350182337774135,\n",
       "  1.633738258538814,\n",
       "  1.6320007408754977,\n",
       "  1.6297911590486351,\n",
       "  1.6296463686343374,\n",
       "  1.6274252502541793,\n",
       "  1.6262557810363347,\n",
       "  1.625292140030795,\n",
       "  1.6236076714919876,\n",
       "  1.623103440633441,\n",
       "  1.6220485222967047,\n",
       "  1.6215543175668268,\n",
       "  1.6213895267396752,\n",
       "  1.6209656095901024,\n",
       "  1.619691441924288,\n",
       "  1.6194329000906271,\n",
       "  1.6193053653035467,\n",
       "  1.6177562656825268,\n",
       "  1.6170348642275274,\n",
       "  1.6166905891201833,\n",
       "  1.616057092132991,\n",
       "  1.6164057271302241,\n",
       "  1.6159929862643212,\n",
       "  1.6150433330324547,\n",
       "  1.6144379796744053,\n",
       "  1.6143874408795893,\n",
       "  1.6136280747331742,\n",
       "  1.6135175541827553,\n",
       "  1.613320354939828,\n",
       "  1.6128002841056548,\n",
       "  1.6127248077841676,\n",
       "  1.612829939810523,\n",
       "  1.6127366560648022,\n",
       "  1.6116764277962767,\n",
       "  1.611121354671066,\n",
       "  1.6111118076910933,\n",
       "  1.6107382064380804,\n",
       "  1.6105525021407743,\n",
       "  1.6101669627873851,\n",
       "  1.6101447971243608,\n",
       "  1.6098184717659145,\n",
       "  1.6090804401196932,\n",
       "  1.6089976259876155,\n",
       "  1.608647743090368,\n",
       "  1.6087524293202111,\n",
       "  1.608555488639261,\n",
       "  1.608138537803185,\n",
       "  1.6076620764349305,\n",
       "  1.607341255507641,\n",
       "  1.6071142394126616,\n",
       "  1.6068972794963383,\n",
       "  1.6067566544725624,\n",
       "  1.6069049296947067,\n",
       "  1.6061532606378486,\n",
       "  1.6059028560765232,\n",
       "  1.6058142594683533,\n",
       "  1.6055802011093605,\n",
       "  1.6057228328778803,\n",
       "  1.6055234967836713,\n",
       "  1.605358214919917,\n",
       "  1.6060993734158968,\n",
       "  1.606236065853996,\n",
       "  1.6057486785085577,\n",
       "  1.6060003038918873,\n",
       "  1.6046872036608963,\n",
       "  1.6048192905256953,\n",
       "  1.60537814301467,\n",
       "  1.6051728874032187,\n",
       "  1.6052321327690273,\n",
       "  1.6049884039279165,\n",
       "  1.605175287769772,\n",
       "  1.6044925999443287,\n",
       "  1.6036223430712797,\n",
       "  1.6033404444723578,\n",
       "  1.6030084086257004,\n",
       "  1.603583694825212,\n",
       "  1.6036959784182816,\n",
       "  1.6052277058445517,\n",
       "  1.6052640045778903,\n",
       "  1.6040522675765188,\n",
       "  1.6032707443527898,\n",
       "  1.6025097178620316,\n",
       "  1.6025494462565373,\n",
       "  1.6020712350544177,\n",
       "  1.6022225443346019,\n",
       "  1.602944013814847,\n",
       "  1.6030020007136125,\n",
       "  1.6021055016161,\n",
       "  1.6021296155749927,\n",
       "  1.6025610610718872,\n",
       "  1.6026035989750786,\n",
       "  1.6036508961727745,\n",
       "  1.6025451771770487,\n",
       "  1.602760933773009,\n",
       "  1.6025749563840617,\n",
       "  1.6041736866958913,\n",
       "  1.6029814438806675,\n",
       "  1.6040589958016562,\n",
       "  1.604567563434717,\n",
       "  1.6052609137220726,\n",
       "  1.6057159038792026,\n",
       "  1.6070184816614081,\n",
       "  1.6058058933538082,\n",
       "  1.6054218594717518,\n",
       "  1.6027852002933745,\n",
       "  1.6023629098057417,\n",
       "  1.6031059845662843,\n",
       "  1.6025294455134638,\n",
       "  1.6023332161256152,\n",
       "  1.602220383047067,\n",
       "  1.6064541148346878,\n",
       "  1.608289894634997,\n",
       "  1.6120952711211016,\n",
       "  1.610083097235978,\n",
       "  1.6060215967844067,\n",
       "  1.6022737904598838,\n",
       "  1.6011940663541122,\n",
       "  1.6012312992788087,\n",
       "  1.6009024834038479,\n",
       "  1.6015204584169256,\n",
       "  1.6012277517292308,\n",
       "  1.6009896627093285,\n",
       "  1.6004799201547935,\n",
       "  1.6000599134661815,\n",
       "  1.6000463206351958,\n",
       "  1.600157240090938,\n",
       "  1.6007073836973829,\n",
       "  1.5999843053870584,\n",
       "  1.6006211148074458,\n",
       "  1.6002665054765104,\n",
       "  1.5989344806882484,\n",
       "  1.5989922658228148,\n",
       "  1.5992364067780345,\n",
       "  1.598882011099205,\n",
       "  1.598554298488057,\n",
       "  1.5988102378607456,\n",
       "  1.5987997494245831,\n",
       "  1.5986774129550543,\n",
       "  1.5986702848339345,\n",
       "  1.598465506389861,\n",
       "  1.5986192226409912,\n",
       "  1.5985802900758146,\n",
       "  1.59865237768337,\n",
       "  1.5983698027946283,\n",
       "  1.5984430914110095,\n",
       "  1.5978348991547264,\n",
       "  1.597740055121213,\n",
       "  1.597622065332788,\n",
       "  1.5977179096015868,\n",
       "  1.597360941841992,\n",
       "  1.5977022968170715,\n",
       "  1.5982191978729332,\n",
       "  1.5987128402387667,\n",
       "  1.598362915403625,\n",
       "  1.599275866703974,\n",
       "  1.5997547753299703,\n",
       "  1.598981398294507,\n",
       "  1.5980392007616417,\n",
       "  1.5978181438102617,\n",
       "  1.5977582683853826,\n",
       "  1.5976959655159397,\n",
       "  1.597984464544999,\n",
       "  1.5976942444111832,\n",
       "  1.597721015317288,\n",
       "  1.5976446966055027,\n",
       "  1.5975368822050227,\n",
       "  1.5980153562619746,\n",
       "  1.5975640255988799,\n",
       "  1.5976776178523775,\n",
       "  1.5974414556640668,\n",
       "  1.597215182233055,\n",
       "  1.5971063283011524,\n",
       "  1.5975483253061606,\n",
       "  1.5976132620074412,\n",
       "  1.5974395853000334,\n",
       "  1.597160145516541,\n",
       "  1.597905331701453,\n",
       "  1.5978860237591814,\n",
       "  1.5969742179907591,\n",
       "  1.596611034506906,\n",
       "  1.596291140506142,\n",
       "  1.5969874670631008,\n",
       "  1.5973259999811484,\n",
       "  1.5976987875729716,\n",
       "  1.5977676710593733,\n",
       "  1.5972557824074067,\n",
       "  1.596857357553498,\n",
       "  1.5966497953578707,\n",
       "  1.5966061323963705,\n",
       "  1.5968112651661162,\n",
       "  1.5970187956606583,\n",
       "  1.5971922256939959,\n",
       "  1.5974231469664217,\n",
       "  1.596801671955394,\n",
       "  1.5977854177231934,\n",
       "  1.5977899483366356,\n",
       "  1.5975356234077602,\n",
       "  1.597668000205402,\n",
       "  1.597915463500406,\n",
       "  1.5981029866474816,\n",
       "  1.5965985545161028,\n",
       "  1.5966870269616886,\n",
       "  1.596373481763697,\n",
       "  1.5973002022323186,\n",
       "  1.5974151163550294],\n",
       " 'train_ATT_acc': [58.17174515235457,\n",
       "  69.25207756232687,\n",
       "  77.28531855955679,\n",
       "  81.7174515235457,\n",
       "  86.70360110803324,\n",
       "  91.41274238227147,\n",
       "  93.07479224376732,\n",
       "  93.90581717451524,\n",
       "  93.90581717451524,\n",
       "  94.73684210526316,\n",
       "  96.39889196675901,\n",
       "  96.67590027700831,\n",
       "  96.95290858725762,\n",
       "  97.50692520775624,\n",
       "  98.06094182825485,\n",
       "  98.06094182825485,\n",
       "  98.61495844875347,\n",
       "  98.89196675900277,\n",
       "  98.89196675900277,\n",
       "  99.16897506925208,\n",
       "  99.44598337950139,\n",
       "  99.16897506925208,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0],\n",
       " 'train_VAL_acc': [88.9196675900277,\n",
       "  91.41274238227147,\n",
       "  92.24376731301939,\n",
       "  94.45983379501385,\n",
       "  96.67590027700831,\n",
       "  98.06094182825485,\n",
       "  98.61495844875347,\n",
       "  99.44598337950139,\n",
       "  99.16897506925208,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.16897506925208,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.16897506925208,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.44598337950139,\n",
       "  99.44598337950139,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.7229916897507,\n",
       "  99.7229916897507],\n",
       " 'train_VAL_jac': [0.11772853185595568,\n",
       "  0.4464450648616886,\n",
       "  0.5166205078611084,\n",
       "  0.5554016765795255,\n",
       "  0.5927977931796679,\n",
       "  0.6311172778586601,\n",
       "  0.656971387255555,\n",
       "  0.6722068390357527,\n",
       "  0.6772853291265852,\n",
       "  0.7091412848052556,\n",
       "  0.7197599384593171,\n",
       "  0.7211449747270494,\n",
       "  0.7349953902395148,\n",
       "  0.734072027444179,\n",
       "  0.7359187477513364,\n",
       "  0.7451523598541513,\n",
       "  0.7530009304057198,\n",
       "  0.7622345425085347,\n",
       "  0.7557710135082129,\n",
       "  0.7710064705719247,\n",
       "  0.788088647942794,\n",
       "  0.7857802488797259,\n",
       "  0.788550331982219,\n",
       "  0.7825484817378079,\n",
       "  0.7765466367769109,\n",
       "  0.7723915121231714,\n",
       "  0.7927054566359586,\n",
       "  0.7788550411234932,\n",
       "  0.7756232792650893,\n",
       "  0.7890120107381298,\n",
       "  0.7996306643921913,\n",
       "  0.8019390634552593,\n",
       "  0.8084025924555813,\n",
       "  0.7950138609825409,\n",
       "  0.7950138609825409,\n",
       "  0.7977839388015198,\n",
       "  0.8019390634552593,\n",
       "  0.8014773846993486,\n",
       "  0.8033241102900202,\n",
       "  0.8102493180462528,\n",
       "  0.8088642764950063,\n",
       "  0.8074792296602455,\n",
       "  0.7996306643921913,\n",
       "  0.7996306643921913,\n",
       "  0.7936288141477802,\n",
       "  0.8047091465577525,\n",
       "  0.806094188108999,\n",
       "  0.8047091465577525,\n",
       "  0.8144044374164782,\n",
       "  0.8047091465577525,\n",
       "  0.8125577171093209,\n",
       "  0.8079409136996705,\n",
       "  0.8130194011487459,\n",
       "  0.818559567353732,\n",
       "  0.818097883314307,\n",
       "  0.8194829301490678,\n",
       "  0.8190212461096428,\n",
       "  0.8167128417630605,\n",
       "  0.8314866200708616,\n",
       "  0.8319482988267725,\n",
       "  0.837026791559362,\n",
       "  0.8407202321736766,\n",
       "  0.8444136727879913,\n",
       "  0.8282548529289436,\n",
       "  0.8254847698264505,\n",
       "  0.8384118278270943,\n",
       "  0.8416435949690124,\n",
       "  0.8365651075199371,\n",
       "  0.8411819162131017,\n",
       "  0.8310249413149509,\n",
       "  0.8365651075199371,\n",
       "  0.8319482988267725,\n",
       "  0.8199446089049786,\n",
       "  0.8254847751099648,\n",
       "  0.8240997335587182,\n",
       "  0.8324099828661974,\n",
       "  0.8351800659686904,\n",
       "  0.8277931741730328,\n",
       "  0.8259464538658755,\n",
       "  0.8287165369683686,\n",
       "  0.8264081326217863,\n",
       "  0.8236380495192932,\n",
       "  0.8324099828661974,\n",
       "  0.8356417447246013,\n",
       "  0.8439519940320804,\n",
       "  0.8624192129541962,\n",
       "  0.857340730788635,\n",
       "  0.8499538389929774,\n",
       "  0.8374884650317586,\n",
       "  0.8236380495192932,\n",
       "  0.8287165369683686,\n",
       "  0.8374884703152727,\n",
       "  0.8393351906224301,\n",
       "  0.851800564583649,\n",
       "  0.8448753568274162,\n",
       "  0.8513388805442239,\n",
       "  0.8513388805442239,\n",
       "  0.8504155177488881,\n",
       "  0.8351800606851763,\n",
       "  0.8587257670563674,\n",
       "  0.8554940051979636,\n",
       "  0.8596491298517032,\n",
       "  0.8476454346463951,\n",
       "  0.8545706424026278,\n",
       "  0.8582640883004566,\n",
       "  0.8531856008513813,\n",
       "  0.8554940051979636,\n",
       "  0.8434903152761697,\n",
       "  0.8564173679932994,\n",
       "  0.8559556892373885,\n",
       "  0.8305632572755259,\n",
       "  0.8356417447246013,\n",
       "  0.824561412314629,\n",
       "  0.8176362045583963,\n",
       "  0.8208679664168002,\n",
       "  0.8351800659686904,\n",
       "  0.857340730788635,\n",
       "  0.8513388805442239,\n",
       "  0.8462603930951486,\n",
       "  0.8624192129541962,\n",
       "  0.8730378666082578,\n",
       "  0.8504155230324024,\n",
       "  0.8116343490304709,\n",
       "  0.7987072963133413,\n",
       "  0.7700831077765891,\n",
       "  0.7853185701238151,\n",
       "  0.8167128417630605,\n",
       "  0.8333333456615332,\n",
       "  0.843028636520259,\n",
       "  0.8328716616221082,\n",
       "  0.8314866200708616,\n",
       "  0.835641739441087,\n",
       "  0.8370267862758478,\n",
       "  0.8393351853389159,\n",
       "  0.8499538389929774,\n",
       "  0.8481071186858201,\n",
       "  0.8578024095445459,\n",
       "  0.854570647686142,\n",
       "  0.8471837611739985,\n",
       "  0.8522622433395597,\n",
       "  0.8494921602370666,\n",
       "  0.8587257670563674,\n",
       "  0.8674977004032716,\n",
       "  0.8818097893881336,\n",
       "  0.8795013903250655,\n",
       "  0.8818097946716478,\n",
       "  0.8924284483257093,\n",
       "  0.8859649246089016,\n",
       "  0.8855032405694766,\n",
       "  0.8841182043017443,\n",
       "  0.8776546753014224,\n",
       "  0.863804259788957,\n",
       "  0.8711911515846147,\n",
       "  0.8822714787110728,\n",
       "  0.8878116449160589,\n",
       "  0.881809799955162,\n",
       "  0.8771929965455116,\n",
       "  0.8901200492626412,\n",
       "  0.892428443042195,\n",
       "  0.899353656081942,\n",
       "  0.902123739184435,\n",
       "  0.9012003763890993,\n",
       "  0.8910434014909485,\n",
       "  0.8928901217981059,\n",
       "  0.8910434014909485,\n",
       "  0.8905817227350378,\n",
       "  0.8739612294035936,\n",
       "  0.8753462656713259,\n",
       "  0.8868882768372089,\n",
       "  0.8905817227350378,\n",
       "  0.8813481106322227,\n",
       "  0.8670360216473608,\n",
       "  0.8702677887892789,\n",
       "  0.8753462709548401,\n",
       "  0.873037871891772,\n",
       "  0.872114503812922,\n",
       "  0.8744229081595043,\n",
       "  0.8698061047498539,\n",
       "  0.8624192129541962,\n",
       "  0.8656509800961143,\n",
       "  0.8702677835057647,\n",
       "  0.8734995506476828,\n",
       "  0.8799630796480047,\n",
       "  0.8822714787110728,\n",
       "  0.8850415618135659,\n",
       "  0.8873499661601482,\n",
       "  0.8910434067744628,\n",
       "  0.8836565202623193,\n",
       "  0.874884586915415,\n",
       "  0.8716528250570112,\n",
       "  0.9062788638381747,\n",
       "  0.9044321435310173,\n",
       "  0.9025854179403459,\n",
       "  0.8716528250570112,\n",
       "  0.8753462709548401,\n",
       "  0.8582640935839708,\n",
       "  0.863342575749532,\n",
       "  0.8624192129541962,\n",
       "  0.8799630743644905,\n",
       "  0.8961218836565097,\n",
       "  0.8827331521834693,\n",
       "  0.8961218836565097,\n",
       "  0.8781163487738189,\n",
       "  0.8965835676959347,\n",
       "  0.8891966811837912,\n",
       "  0.9002770083102493,\n",
       "  0.892428443042195,\n",
       "  0.8813481106322227,\n",
       "  0.8970452464518455,\n",
       "  0.8988919667590027,\n",
       "  0.880886431876312,\n",
       "  0.8855032352859624,\n",
       "  0.8979686092471812,\n",
       "  0.9072022213499962,\n",
       "  0.8864265980812982,\n",
       "  0.8859649193253873,\n",
       "  0.8707294675451897],\n",
       " 'train_VAL_acc_1': [50.41551246537396,\n",
       "  59.2797783933518,\n",
       "  61.772853185595565,\n",
       "  65.65096952908587,\n",
       "  68.69806094182826,\n",
       "  70.91412742382272,\n",
       "  73.9612188365651,\n",
       "  74.79224376731302,\n",
       "  72.85318559556787,\n",
       "  75.90027700831025,\n",
       "  75.90027700831025,\n",
       "  77.00831024930748,\n",
       "  77.00831024930748,\n",
       "  77.28531855955679,\n",
       "  78.39335180055402,\n",
       "  79.50138504155125,\n",
       "  78.11634349030471,\n",
       "  79.22437673130194,\n",
       "  79.77839335180056,\n",
       "  81.7174515235457,\n",
       "  83.10249307479225,\n",
       "  81.7174515235457,\n",
       "  81.7174515235457,\n",
       "  81.4404432132964,\n",
       "  80.60941828254848,\n",
       "  80.88642659279779,\n",
       "  81.7174515235457,\n",
       "  82.54847645429363,\n",
       "  80.60941828254848,\n",
       "  83.93351800554017,\n",
       "  84.7645429362881,\n",
       "  84.48753462603878,\n",
       "  83.93351800554017,\n",
       "  84.21052631578948,\n",
       "  83.65650969529086,\n",
       "  85.0415512465374,\n",
       "  85.31855955678671,\n",
       "  83.93351800554017,\n",
       "  84.7645429362881,\n",
       "  84.21052631578948,\n",
       "  84.21052631578948,\n",
       "  84.48753462603878,\n",
       "  83.65650969529086,\n",
       "  83.93351800554017,\n",
       "  81.99445983379502,\n",
       "  83.93351800554017,\n",
       "  84.48753462603878,\n",
       "  83.93351800554017,\n",
       "  85.0415512465374,\n",
       "  85.31855955678671,\n",
       "  86.70360110803324,\n",
       "  85.59556786703601,\n",
       "  86.42659279778394,\n",
       "  86.70360110803324,\n",
       "  85.31855955678671,\n",
       "  86.42659279778394,\n",
       "  85.87257617728532,\n",
       "  85.31855955678671,\n",
       "  85.87257617728532,\n",
       "  87.53462603878117,\n",
       "  88.08864265927978,\n",
       "  88.08864265927978,\n",
       "  87.25761772853186,\n",
       "  87.53462603878117,\n",
       "  86.14958448753463,\n",
       "  87.81163434903047,\n",
       "  88.6426592797784,\n",
       "  88.36565096952909,\n",
       "  88.9196675900277,\n",
       "  87.81163434903047,\n",
       "  88.36565096952909,\n",
       "  86.42659279778394,\n",
       "  84.7645429362881,\n",
       "  85.87257617728532,\n",
       "  85.31855955678671,\n",
       "  84.7645429362881,\n",
       "  86.98060941828255,\n",
       "  85.87257617728532,\n",
       "  85.87257617728532,\n",
       "  86.14958448753463,\n",
       "  87.25761772853186,\n",
       "  86.70360110803324,\n",
       "  85.87257617728532,\n",
       "  88.9196675900277,\n",
       "  89.19667590027701,\n",
       "  90.02770083102493,\n",
       "  88.9196675900277,\n",
       "  90.58171745152355,\n",
       "  90.02770083102493,\n",
       "  86.14958448753463,\n",
       "  85.31855955678671,\n",
       "  87.53462603878117,\n",
       "  88.9196675900277,\n",
       "  88.08864265927978,\n",
       "  88.36565096952909,\n",
       "  91.13573407202216,\n",
       "  90.02770083102493,\n",
       "  88.6426592797784,\n",
       "  88.9196675900277,\n",
       "  89.19667590027701,\n",
       "  90.30470914127424,\n",
       "  88.36565096952909,\n",
       "  88.08864265927978,\n",
       "  87.53462603878117,\n",
       "  88.6426592797784,\n",
       "  89.19667590027701,\n",
       "  90.02770083102493,\n",
       "  87.25761772853186,\n",
       "  88.36565096952909,\n",
       "  87.25761772853186,\n",
       "  86.70360110803324,\n",
       "  85.87257617728532,\n",
       "  86.98060941828255,\n",
       "  85.59556786703601,\n",
       "  86.98060941828255,\n",
       "  85.87257617728532,\n",
       "  87.81163434903047,\n",
       "  87.81163434903047,\n",
       "  86.14958448753463,\n",
       "  86.98060941828255,\n",
       "  85.31855955678671,\n",
       "  86.42659279778394,\n",
       "  83.93351800554017,\n",
       "  82.27146814404432,\n",
       "  78.39335180055402,\n",
       "  80.05540166204986,\n",
       "  82.82548476454294,\n",
       "  87.25761772853186,\n",
       "  88.9196675900277,\n",
       "  88.08864265927978,\n",
       "  87.81163434903047,\n",
       "  86.98060941828255,\n",
       "  86.98060941828255,\n",
       "  87.53462603878117,\n",
       "  87.81163434903047,\n",
       "  89.19667590027701,\n",
       "  88.9196675900277,\n",
       "  87.81163434903047,\n",
       "  86.70360110803324,\n",
       "  87.81163434903047,\n",
       "  85.87257617728532,\n",
       "  86.42659279778394,\n",
       "  90.30470914127424,\n",
       "  89.19667590027701,\n",
       "  90.58171745152355,\n",
       "  90.02770083102493,\n",
       "  90.85872576177286,\n",
       "  91.68975069252078,\n",
       "  92.5207756232687,\n",
       "  91.41274238227147,\n",
       "  91.41274238227147,\n",
       "  90.58171745152355,\n",
       "  90.02770083102493,\n",
       "  92.24376731301939,\n",
       "  91.68975069252078,\n",
       "  92.797783933518,\n",
       "  93.07479224376732,\n",
       "  93.62880886426593,\n",
       "  92.797783933518,\n",
       "  92.797783933518,\n",
       "  91.68975069252078,\n",
       "  92.5207756232687,\n",
       "  91.96675900277009,\n",
       "  89.47368421052632,\n",
       "  88.08864265927978,\n",
       "  88.6426592797784,\n",
       "  86.98060941828255,\n",
       "  87.81163434903047,\n",
       "  87.25761772853186,\n",
       "  90.30470914127424,\n",
       "  87.53462603878117,\n",
       "  87.81163434903047,\n",
       "  88.6426592797784,\n",
       "  86.70360110803324,\n",
       "  88.9196675900277,\n",
       "  90.02770083102493,\n",
       "  90.02770083102493,\n",
       "  90.58171745152355,\n",
       "  89.47368421052632,\n",
       "  91.41274238227147,\n",
       "  91.68975069252078,\n",
       "  90.85872576177286,\n",
       "  90.85872576177286,\n",
       "  93.62880886426593,\n",
       "  91.13573407202216,\n",
       "  91.13573407202216,\n",
       "  91.96675900277009,\n",
       "  92.24376731301939,\n",
       "  90.58171745152355,\n",
       "  91.13573407202216,\n",
       "  90.85872576177286,\n",
       "  91.96675900277009,\n",
       "  92.797783933518,\n",
       "  91.41274238227147,\n",
       "  91.96675900277009,\n",
       "  90.02770083102493,\n",
       "  91.41274238227147,\n",
       "  90.85872576177286,\n",
       "  89.19667590027701,\n",
       "  91.13573407202216,\n",
       "  91.68975069252078,\n",
       "  90.58171745152355,\n",
       "  88.6426592797784,\n",
       "  87.81163434903047,\n",
       "  90.30470914127424,\n",
       "  90.02770083102493,\n",
       "  88.9196675900277,\n",
       "  88.9196675900277,\n",
       "  88.6426592797784,\n",
       "  90.30470914127424,\n",
       "  88.6426592797784,\n",
       "  88.36565096952909,\n",
       "  89.75069252077563,\n",
       "  91.68975069252078,\n",
       "  91.13573407202216,\n",
       "  91.13573407202216,\n",
       "  90.02770083102493],\n",
       " 'val_loss': [7.020905508541356,\n",
       "  6.626189017956838,\n",
       "  6.387608883170248,\n",
       "  6.21646633657288,\n",
       "  6.103776616313904,\n",
       "  6.017634580147783,\n",
       "  5.964591511965009,\n",
       "  5.9194606219218535,\n",
       "  5.889683846667611,\n",
       "  5.863476667790558,\n",
       "  5.840357306610478,\n",
       "  5.828818487622835,\n",
       "  5.817941876508991,\n",
       "  5.8063476162713945,\n",
       "  5.800406438199867,\n",
       "  5.786719185546458,\n",
       "  5.784577833513574,\n",
       "  5.7724690504282306,\n",
       "  5.767993552994086,\n",
       "  5.766974748841795,\n",
       "  5.758154812044111,\n",
       "  5.753090739521124,\n",
       "  5.746768333638185,\n",
       "  5.7507980346549585,\n",
       "  5.746734632410769,\n",
       "  5.745857237836755,\n",
       "  5.7348125442928755,\n",
       "  5.737702089258789,\n",
       "  5.736113561949993,\n",
       "  5.731580577673468,\n",
       "  5.7273828288110336,\n",
       "  5.7273649024014,\n",
       "  5.725365574664899,\n",
       "  5.728064948329699,\n",
       "  5.729004976939223,\n",
       "  5.721715025470986,\n",
       "  5.717495148792381,\n",
       "  5.714050165579905,\n",
       "  5.711663841209972,\n",
       "  5.717402283945384,\n",
       "  5.71629274187276,\n",
       "  5.711845104237777,\n",
       "  5.7096593178356025,\n",
       "  5.710712083278738,\n",
       "  5.712963490263854,\n",
       "  5.709447622170885,\n",
       "  5.704763036048347,\n",
       "  5.707474431177346,\n",
       "  5.704939059921812,\n",
       "  5.704954898069979,\n",
       "  5.704193339262045,\n",
       "  5.706493380978614,\n",
       "  5.703629502715306,\n",
       "  5.700984098996244,\n",
       "  5.700821477383753,\n",
       "  5.701840009302351,\n",
       "  5.700784731323157,\n",
       "  5.7051978358933475,\n",
       "  5.701066338429677,\n",
       "  5.698772965978481,\n",
       "  5.699434155009478,\n",
       "  5.699366812754286,\n",
       "  5.696877142642796,\n",
       "  5.6960920107612205,\n",
       "  5.69712106121907,\n",
       "  5.698573632845007,\n",
       "  5.691751856000428,\n",
       "  5.692696541505403,\n",
       "  5.693982528833462,\n",
       "  5.690263701864881,\n",
       "  5.691702966756855,\n",
       "  5.693760764438788,\n",
       "  5.697416128095893,\n",
       "  5.699989230024347,\n",
       "  5.698380625326373,\n",
       "  5.700066075750664,\n",
       "  5.688585155420288,\n",
       "  5.694042752092165,\n",
       "  5.695207504079422,\n",
       "  5.69019584881896,\n",
       "  5.69444203711627,\n",
       "  5.695020788176703,\n",
       "  5.69409523614166,\n",
       "  5.692625732571358,\n",
       "  5.6889619969950225,\n",
       "  5.690107395481952,\n",
       "  5.688577127069231,\n",
       "  5.689240784406652,\n",
       "  5.689186611493243,\n",
       "  5.69176803878502,\n",
       "  5.696735138860352,\n",
       "  5.693096950852526,\n",
       "  5.692588677917853,\n",
       "  5.690962340132059,\n",
       "  5.690560537838187,\n",
       "  5.690436772437289,\n",
       "  5.690153232838328,\n",
       "  5.694255860998551,\n",
       "  5.6983755929752515,\n",
       "  5.692089788346851,\n",
       "  5.694874683665,\n",
       "  5.696957193964034,\n",
       "  5.697304552681546,\n",
       "  5.703849579802483,\n",
       "  5.699829001040982,\n",
       "  5.706205434828365,\n",
       "  5.699097440652307,\n",
       "  5.70192850568497,\n",
       "  5.700773866537351,\n",
       "  5.704950054120166,\n",
       "  5.709149887460439,\n",
       "  5.708597937633065,\n",
       "  5.713534691360244,\n",
       "  5.71692843477645,\n",
       "  5.712978294935419,\n",
       "  5.711320408810875,\n",
       "  5.699783241497801,\n",
       "  5.698251623881584,\n",
       "  5.700505649226725,\n",
       "  5.701782695207571,\n",
       "  5.6973069840128705,\n",
       "  5.6899547300431035,\n",
       "  5.702240418428662,\n",
       "  5.708470637505335,\n",
       "  5.715796869577727,\n",
       "  5.7073361157339715,\n",
       "  5.6971928939325345,\n",
       "  5.6920438600455885,\n",
       "  5.6872911351864985,\n",
       "  5.687859481531569,\n",
       "  5.687065686572372,\n",
       "  5.690691872133792,\n",
       "  5.69272146664652,\n",
       "  5.69301331891277,\n",
       "  5.692400927460138,\n",
       "  5.690311288787308,\n",
       "  5.686106333021737,\n",
       "  5.686340895264588,\n",
       "  5.692421282140774,\n",
       "  5.688823019902255,\n",
       "  5.696637493856435,\n",
       "  5.692339758728755,\n",
       "  5.684906452330845,\n",
       "  5.689411378160673,\n",
       "  5.6932241556358605,\n",
       "  5.695778386163608,\n",
       "  5.698152163543361,\n",
       "  5.703859251226625,\n",
       "  5.694648570193155,\n",
       "  5.685901293340313,\n",
       "  5.683698348001865,\n",
       "  5.6832006495056415,\n",
       "  5.6850070494517935,\n",
       "  5.683161235070137,\n",
       "  5.685547814223729,\n",
       "  5.686995150458063,\n",
       "  5.6904702943715515,\n",
       "  5.687909968329533,\n",
       "  5.686395147723665,\n",
       "  5.684136377957774,\n",
       "  5.687055372331028,\n",
       "  5.689210627957145,\n",
       "  5.690847266942109,\n",
       "  5.696571055513712,\n",
       "  5.700586104177923,\n",
       "  5.697898197853932,\n",
       "  5.70220366214392,\n",
       "  5.707864975875788,\n",
       "  5.698142904949239,\n",
       "  5.689187762783846,\n",
       "  5.686568890772367,\n",
       "  5.690884637318927,\n",
       "  5.68478105546376,\n",
       "  5.684827828244723,\n",
       "  5.685896817953286,\n",
       "  5.688605006562943,\n",
       "  5.690576146792901,\n",
       "  5.684198509901314,\n",
       "  5.684717269696024,\n",
       "  5.683215725127212,\n",
       "  5.68329861131962,\n",
       "  5.684025390212186,\n",
       "  5.684467436657364,\n",
       "  5.684256395050916,\n",
       "  5.688909900025403,\n",
       "  5.69090132769368,\n",
       "  5.686135851911136,\n",
       "  5.690740302963602,\n",
       "  5.687061439256841,\n",
       "  5.689642097085573,\n",
       "  5.695939675308882,\n",
       "  5.69720554908131,\n",
       "  5.686824011222119,\n",
       "  5.685015796373736,\n",
       "  5.687718851882182,\n",
       "  5.68531541118414,\n",
       "  5.688200025558782,\n",
       "  5.691581749095256,\n",
       "  5.687998897283272,\n",
       "  5.6839229006401855,\n",
       "  5.686551006445729,\n",
       "  5.691338298122064,\n",
       "  5.6900081199673025,\n",
       "  5.695307911733798,\n",
       "  5.695944280184845,\n",
       "  5.694831390029162,\n",
       "  5.6999641828028,\n",
       "  5.698428973514983,\n",
       "  5.697299443957273,\n",
       "  5.69457523463889,\n",
       "  5.696687482858699,\n",
       "  5.697476850892914,\n",
       "  5.691328717701889,\n",
       "  5.693498808772269,\n",
       "  5.702149357411122,\n",
       "  5.701634327243559,\n",
       "  5.6920814803565785],\n",
       " 'val_ATT_loss': [1.7302052972277975,\n",
       "  1.5062038614982511,\n",
       "  1.3251400144119574,\n",
       "  1.1916224969838694,\n",
       "  1.1014582692849926,\n",
       "  1.0365629611703437,\n",
       "  0.9994486805384721,\n",
       "  0.9680191342665897,\n",
       "  0.9466754972207837,\n",
       "  0.9326609622656814,\n",
       "  0.9173655950926184,\n",
       "  0.9074803927565008,\n",
       "  0.9018846186922818,\n",
       "  0.8950553886531815,\n",
       "  0.8905992488550946,\n",
       "  0.8860206582197329,\n",
       "  0.8832128093979223,\n",
       "  0.8775335781700243,\n",
       "  0.8726309250767638,\n",
       "  0.8712512779284299,\n",
       "  0.8679828196763992,\n",
       "  0.8656361076405378,\n",
       "  0.8613749895153976,\n",
       "  0.8636155047310077,\n",
       "  0.8571863712334051,\n",
       "  0.8591916986354967,\n",
       "  0.8534834206831164,\n",
       "  0.8542534585406141,\n",
       "  0.8500023019023057,\n",
       "  0.8499137617465926,\n",
       "  0.8495479902116264,\n",
       "  0.8462638308604559,\n",
       "  0.8445062771802996,\n",
       "  0.8435863634435142,\n",
       "  0.8462472127947381,\n",
       "  0.8433952751925321,\n",
       "  0.8416838426657809,\n",
       "  0.8395055738406453,\n",
       "  0.8357648986384152,\n",
       "  0.838005598119604,\n",
       "  0.8366700264254237,\n",
       "  0.8367127149812574,\n",
       "  0.8350474039713541,\n",
       "  0.8339200769740391,\n",
       "  0.8340908085427633,\n",
       "  0.8340794152118326,\n",
       "  0.8312222493615576,\n",
       "  0.8341160946018328,\n",
       "  0.8322723263162908,\n",
       "  0.8314426793315546,\n",
       "  0.8302987370549179,\n",
       "  0.8319316988553458,\n",
       "  0.8291626078326527,\n",
       "  0.8296025953884048,\n",
       "  0.8295297912223553,\n",
       "  0.8304245645195488,\n",
       "  0.8281733278578859,\n",
       "  0.8317340607565593,\n",
       "  0.8285085597658545,\n",
       "  0.8255861304639801,\n",
       "  0.8264070611174513,\n",
       "  0.827059676613265,\n",
       "  0.8264128523386591,\n",
       "  0.8268791220294751,\n",
       "  0.8251225632138368,\n",
       "  0.8285083407308997,\n",
       "  0.8233038170066306,\n",
       "  0.8225475468528949,\n",
       "  0.8252029000985913,\n",
       "  0.822687013483629,\n",
       "  0.824745705699533,\n",
       "  0.8241524009442911,\n",
       "  0.8233910304744069,\n",
       "  0.8262249682007766,\n",
       "  0.8256543035187373,\n",
       "  0.8246897067723236,\n",
       "  0.8191647667710374,\n",
       "  0.824900582069304,\n",
       "  0.8217636609707422,\n",
       "  0.8187766392541126,\n",
       "  0.8224892520565328,\n",
       "  0.8219296894664687,\n",
       "  0.8218270902468906,\n",
       "  0.8232369948693408,\n",
       "  0.8228506368350207,\n",
       "  0.8230071678394224,\n",
       "  0.8208760088536797,\n",
       "  0.8213947987168785,\n",
       "  0.8216052228600029,\n",
       "  0.818833389054469,\n",
       "  0.8219615648674771,\n",
       "  0.8205452430539015,\n",
       "  0.8221080001534485,\n",
       "  0.822831332077825,\n",
       "  0.8217223920230943,\n",
       "  0.8219602271551039,\n",
       "  0.8188107212384542,\n",
       "  0.8173921441401892,\n",
       "  0.8214606490077042,\n",
       "  0.8188622854347152,\n",
       "  0.8181761362688328,\n",
       "  0.8191549752543612,\n",
       "  0.8185599669208371,\n",
       "  0.8196094234784445,\n",
       "  0.8210649148720067,\n",
       "  0.8254999331827086,\n",
       "  0.8207732138594961,\n",
       "  0.8187162352771293,\n",
       "  0.820402429840429,\n",
       "  0.8203980251540982,\n",
       "  0.8234809038600301,\n",
       "  0.8195614789317294,\n",
       "  0.8232942493708153,\n",
       "  0.8219374896791892,\n",
       "  0.8216366004653093,\n",
       "  0.821402918880548,\n",
       "  0.8199538083338156,\n",
       "  0.8205437255584127,\n",
       "  0.818944967738012,\n",
       "  0.8230458046120357,\n",
       "  0.824462977972457,\n",
       "  0.8200105451713733,\n",
       "  0.8207603494326273,\n",
       "  0.8257042761982941,\n",
       "  0.8237697451822157,\n",
       "  0.8199493274456118,\n",
       "  0.8231208589745731,\n",
       "  0.8257081879348289,\n",
       "  0.8204562920865005,\n",
       "  0.817705850048763,\n",
       "  0.8173699403196816,\n",
       "  0.8171123788366473,\n",
       "  0.821495219338231,\n",
       "  0.8213410462305798,\n",
       "  0.8192978508588744,\n",
       "  0.8209994939284596,\n",
       "  0.8174812453306788,\n",
       "  0.8166561227261535,\n",
       "  0.8205618679038877,\n",
       "  0.8193409818459333,\n",
       "  0.8222366720438004,\n",
       "  0.8181315943962191,\n",
       "  0.8164419606933749,\n",
       "  0.8180094386746244,\n",
       "  0.8204248883133012,\n",
       "  0.8233521617040401,\n",
       "  0.8267216208746763,\n",
       "  0.8317834404183597,\n",
       "  0.8233971303798319,\n",
       "  0.8160392571755541,\n",
       "  0.8173874543934334,\n",
       "  0.8172824365821311,\n",
       "  0.8183526851055098,\n",
       "  0.8181056892726479,\n",
       "  0.8190235013399667,\n",
       "  0.8207107371673351,\n",
       "  0.8252928397519802,\n",
       "  0.8231870534458781,\n",
       "  0.8206084150124372,\n",
       "  0.8182471399384785,\n",
       "  0.8171234348925148,\n",
       "  0.8206710168501226,\n",
       "  0.8194359075732347,\n",
       "  0.821585954325955,\n",
       "  0.8228028326984343,\n",
       "  0.822195071515029,\n",
       "  0.8216503003263861,\n",
       "  0.8258786414696918,\n",
       "  0.8191382268337699,\n",
       "  0.8165391994927956,\n",
       "  0.815902560101292,\n",
       "  0.8209519987183858,\n",
       "  0.8150131229947253,\n",
       "  0.8161582009094518,\n",
       "  0.8178426576096837,\n",
       "  0.8198856072939509,\n",
       "  0.8220199209645511,\n",
       "  0.8176435661025163,\n",
       "  0.8173003458395237,\n",
       "  0.8173849390047353,\n",
       "  0.8166329332483493,\n",
       "  0.8171011841878658,\n",
       "  0.8170017661844812,\n",
       "  0.8163028479349322,\n",
       "  0.8181348082011308,\n",
       "  0.8183366166866892,\n",
       "  0.8155772835016251,\n",
       "  0.8205867807070414,\n",
       "  0.8142246474337772,\n",
       "  0.8173495990231754,\n",
       "  0.8226715085225377,\n",
       "  0.8273349668436903,\n",
       "  0.8185379776528211,\n",
       "  0.818619575926928,\n",
       "  0.8202535936502906,\n",
       "  0.8176396581215587,\n",
       "  0.8179275435887701,\n",
       "  0.8215035700458821,\n",
       "  0.8203870732852114,\n",
       "  0.8154917318161911,\n",
       "  0.818292614163422,\n",
       "  0.8197202721262365,\n",
       "  0.8173500751334477,\n",
       "  0.8180524091410443,\n",
       "  0.8182416564323068,\n",
       "  0.8192839022816681,\n",
       "  0.8210958577995378,\n",
       "  0.8187086573703503,\n",
       "  0.8186437983096131,\n",
       "  0.8160367182842115,\n",
       "  0.8176494378869127,\n",
       "  0.8166053394476572,\n",
       "  0.8170895119750403,\n",
       "  0.8186169827614374,\n",
       "  0.8294448526651879,\n",
       "  0.8288594120886268,\n",
       "  0.8194424391277437],\n",
       " 'val_VAL_loss': [1.7635667371045192,\n",
       "  1.706661718819529,\n",
       "  1.6874896229194303,\n",
       "  1.6749479465296704,\n",
       "  1.667439449009637,\n",
       "  1.660357206325813,\n",
       "  1.655047610475512,\n",
       "  1.6504804958850878,\n",
       "  1.6476694498156093,\n",
       "  1.6436052351749588,\n",
       "  1.64099723717262,\n",
       "  1.6404460316221114,\n",
       "  1.6386857526055698,\n",
       "  1.6370974092060708,\n",
       "  1.6366023964482574,\n",
       "  1.6335661757755748,\n",
       "  1.633788341371884,\n",
       "  1.631645157419402,\n",
       "  1.6317875426391075,\n",
       "  1.6319078236377884,\n",
       "  1.6300573307892372,\n",
       "  1.6291515439601953,\n",
       "  1.6284644480409294,\n",
       "  1.6290608433079836,\n",
       "  1.6298494203924545,\n",
       "  1.6288885130670858,\n",
       "  1.6271097078699197,\n",
       "  1.6278162102393916,\n",
       "  1.628703753349229,\n",
       "  1.6272222719756253,\n",
       "  1.6259449461998023,\n",
       "  1.6270336905136484,\n",
       "  1.6269530991615333,\n",
       "  1.628159528295395,\n",
       "  1.6275859213814947,\n",
       "  1.6261065834261514,\n",
       "  1.6252704353755332,\n",
       "  1.62484819724642,\n",
       "  1.625299647523852,\n",
       "  1.6264655619419266,\n",
       "  1.6265409051491122,\n",
       "  1.6250441297521732,\n",
       "  1.6248706379547495,\n",
       "  1.6255973354348996,\n",
       "  1.6262908939070302,\n",
       "  1.6251227356530176,\n",
       "  1.6245135955622632,\n",
       "  1.6244527788585044,\n",
       "  1.6242222445351737,\n",
       "  1.6245040729128082,\n",
       "  1.6246315340690425,\n",
       "  1.6248538940410895,\n",
       "  1.6248222982942178,\n",
       "  1.6237938345359464,\n",
       "  1.6237638953871327,\n",
       "  1.623805148260934,\n",
       "  1.6242038011550903,\n",
       "  1.624487925045596,\n",
       "  1.6241859262212743,\n",
       "  1.6243956118381668,\n",
       "  1.6243423646306756,\n",
       "  1.6241023787136735,\n",
       "  1.6234880967680456,\n",
       "  1.6230709629105817,\n",
       "  1.6239994993350777,\n",
       "  1.623355097371369,\n",
       "  1.6228160129979325,\n",
       "  1.6233829982175028,\n",
       "  1.6229265429116235,\n",
       "  1.6225255627937505,\n",
       "  1.6223190870191075,\n",
       "  1.623202787831499,\n",
       "  1.6246750325404953,\n",
       "  1.6245880872745233,\n",
       "  1.624242107269212,\n",
       "  1.6251254563261135,\n",
       "  1.62314012954975,\n",
       "  1.6230473900076203,\n",
       "  1.6244812810362266,\n",
       "  1.6238064031882826,\n",
       "  1.6239842616865787,\n",
       "  1.624363699570078,\n",
       "  1.6240893819649231,\n",
       "  1.6231295792340057,\n",
       "  1.6220371200533337,\n",
       "  1.6223667425475097,\n",
       "  1.6225670394051839,\n",
       "  1.622615328563258,\n",
       "  1.6225271295444132,\n",
       "  1.6243115499101837,\n",
       "  1.6249245246642916,\n",
       "  1.6241839025995415,\n",
       "  1.6234935592548014,\n",
       "  1.6227103360180783,\n",
       "  1.6229460486050309,\n",
       "  1.6228255150940618,\n",
       "  1.623780837199958,\n",
       "  1.625621238952787,\n",
       "  1.6256383146558488,\n",
       "  1.6244091676373786,\n",
       "  1.6255661824653889,\n",
       "  1.6259340729032243,\n",
       "  1.6262481952535695,\n",
       "  1.628080052108013,\n",
       "  1.6262546953896584,\n",
       "  1.6269018338818855,\n",
       "  1.6261080755976034,\n",
       "  1.6277374234692803,\n",
       "  1.626790478898974,\n",
       "  1.628184009655356,\n",
       "  1.6285563278668032,\n",
       "  1.629678819567112,\n",
       "  1.6300801473298097,\n",
       "  1.6316636483657536,\n",
       "  1.6304472314900365,\n",
       "  1.6299724966434423,\n",
       "  1.6266098110546618,\n",
       "  1.6259026327743906,\n",
       "  1.6271868938295713,\n",
       "  1.6262456301985115,\n",
       "  1.6242813353468044,\n",
       "  1.6233147282905767,\n",
       "  1.6271600229986782,\n",
       "  1.627588787102347,\n",
       "  1.630675708131837,\n",
       "  1.6291289294294535,\n",
       "  1.6246906783193202,\n",
       "  1.6221118907035865,\n",
       "  1.6222782810333327,\n",
       "  1.6233845438276018,\n",
       "  1.6232319154175632,\n",
       "  1.624526497765715,\n",
       "  1.6237420824360964,\n",
       "  1.62389075756073,\n",
       "  1.624367692200421,\n",
       "  1.623103931619616,\n",
       "  1.6228750292303527,\n",
       "  1.6232282575128114,\n",
       "  1.623953138078962,\n",
       "  1.6231606793521074,\n",
       "  1.624800273937545,\n",
       "  1.6247360547775118,\n",
       "  1.62282149721249,\n",
       "  1.6238006464953494,\n",
       "  1.624266422440853,\n",
       "  1.624142074819856,\n",
       "  1.6238101808895617,\n",
       "  1.624025270269422,\n",
       "  1.6237504799377742,\n",
       "  1.6232873453882528,\n",
       "  1.6221036312028105,\n",
       "  1.6219727376411701,\n",
       "  1.6222181214487612,\n",
       "  1.6216851819324962,\n",
       "  1.6221747709612542,\n",
       "  1.6220948044302428,\n",
       "  1.621725818206524,\n",
       "  1.6215743049612186,\n",
       "  1.6219289109037427,\n",
       "  1.621963079339765,\n",
       "  1.623310645812838,\n",
       "  1.6228465370356744,\n",
       "  1.6238037864562913,\n",
       "  1.6249950337292525,\n",
       "  1.6259277571598296,\n",
       "  1.6252343754463008,\n",
       "  1.6268511206058447,\n",
       "  1.6273287781353654,\n",
       "  1.6263348927051562,\n",
       "  1.6242161877636838,\n",
       "  1.6235554435570252,\n",
       "  1.623310879533514,\n",
       "  1.6232559774896782,\n",
       "  1.6228898757784238,\n",
       "  1.622684720114534,\n",
       "  1.622906466422997,\n",
       "  1.6228520752761164,\n",
       "  1.622184981266266,\n",
       "  1.6224723079521668,\n",
       "  1.621943595374159,\n",
       "  1.6222218926904237,\n",
       "  1.6223080686747735,\n",
       "  1.6224885568242942,\n",
       "  1.6226511823719945,\n",
       "  1.6235916972747577,\n",
       "  1.6241882370023304,\n",
       "  1.6235195228031702,\n",
       "  1.6233845074188533,\n",
       "  1.6242789306076877,\n",
       "  1.6240974993541324,\n",
       "  1.6244227222621148,\n",
       "  1.6232901940792066,\n",
       "  1.622762011189766,\n",
       "  1.622132073482269,\n",
       "  1.6224884194106304,\n",
       "  1.6225585843541939,\n",
       "  1.6234241606566706,\n",
       "  1.623359393016458,\n",
       "  1.6225372746660205,\n",
       "  1.622810389607998,\n",
       "  1.6227527974274358,\n",
       "  1.6238726753319426,\n",
       "  1.6242193482779517,\n",
       "  1.6257518341975847,\n",
       "  1.6259008745841792,\n",
       "  1.625182495915831,\n",
       "  1.626289441667754,\n",
       "  1.6265734387148778,\n",
       "  1.62621854854922,\n",
       "  1.6261795054515595,\n",
       "  1.6263460149905953,\n",
       "  1.6269571704817523,\n",
       "  1.6247464019089497,\n",
       "  1.624960608670277,\n",
       "  1.6242348349153115,\n",
       "  1.624258305051644,\n",
       "  1.624213013742945],\n",
       " 'val_ATT_acc': [50.203252032520325,\n",
       "  64.63414634146342,\n",
       "  75.60975609756098,\n",
       "  79.26829268292683,\n",
       "  81.70731707317073,\n",
       "  85.36585365853658,\n",
       "  87.60162601626017,\n",
       "  87.8048780487805,\n",
       "  89.63414634146342,\n",
       "  89.4308943089431,\n",
       "  90.2439024390244,\n",
       "  91.46341463414635,\n",
       "  90.65040650406505,\n",
       "  91.869918699187,\n",
       "  91.46341463414635,\n",
       "  92.6829268292683,\n",
       "  92.27642276422765,\n",
       "  92.47967479674797,\n",
       "  92.47967479674797,\n",
       "  92.47967479674797,\n",
       "  93.08943089430895,\n",
       "  93.29268292682927,\n",
       "  93.29268292682927,\n",
       "  92.47967479674797,\n",
       "  93.69918699186992,\n",
       "  93.90243902439025,\n",
       "  93.90243902439025,\n",
       "  92.88617886178862,\n",
       "  93.4959349593496,\n",
       "  93.90243902439025,\n",
       "  94.3089430894309,\n",
       "  94.3089430894309,\n",
       "  94.51219512195122,\n",
       "  94.3089430894309,\n",
       "  94.3089430894309,\n",
       "  94.51219512195122,\n",
       "  94.51219512195122,\n",
       "  94.51219512195122,\n",
       "  95.1219512195122,\n",
       "  94.71544715447155,\n",
       "  94.91869918699187,\n",
       "  94.91869918699187,\n",
       "  94.51219512195122,\n",
       "  95.73170731707317,\n",
       "  94.91869918699187,\n",
       "  94.71544715447155,\n",
       "  95.1219512195122,\n",
       "  95.32520325203252,\n",
       "  95.1219512195122,\n",
       "  94.3089430894309,\n",
       "  96.13821138211382,\n",
       "  95.9349593495935,\n",
       "  96.7479674796748,\n",
       "  96.34146341463415,\n",
       "  95.9349593495935,\n",
       "  95.1219512195122,\n",
       "  95.9349593495935,\n",
       "  95.73170731707317,\n",
       "  96.34146341463415,\n",
       "  96.7479674796748,\n",
       "  96.95121951219512,\n",
       "  96.13821138211382,\n",
       "  96.34146341463415,\n",
       "  96.13821138211382,\n",
       "  95.73170731707317,\n",
       "  95.32520325203252,\n",
       "  96.7479674796748,\n",
       "  96.13821138211382,\n",
       "  96.13821138211382,\n",
       "  96.34146341463415,\n",
       "  95.73170731707317,\n",
       "  96.54471544715447,\n",
       "  95.52845528455285,\n",
       "  95.52845528455285,\n",
       "  95.9349593495935,\n",
       "  95.9349593495935,\n",
       "  96.7479674796748,\n",
       "  96.54471544715447,\n",
       "  96.34146341463415,\n",
       "  96.13821138211382,\n",
       "  95.9349593495935,\n",
       "  96.54471544715447,\n",
       "  96.13821138211382,\n",
       "  96.13821138211382,\n",
       "  95.52845528455285,\n",
       "  95.32520325203252,\n",
       "  96.34146341463415,\n",
       "  95.9349593495935,\n",
       "  97.15447154471545,\n",
       "  97.15447154471545,\n",
       "  95.52845528455285,\n",
       "  96.13821138211382,\n",
       "  96.7479674796748,\n",
       "  96.13821138211382,\n",
       "  96.7479674796748,\n",
       "  96.13821138211382,\n",
       "  96.34146341463415,\n",
       "  96.13821138211382,\n",
       "  96.13821138211382,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  96.13821138211382,\n",
       "  97.15447154471545,\n",
       "  96.95121951219512,\n",
       "  96.54471544715447,\n",
       "  96.34146341463415,\n",
       "  96.54471544715447,\n",
       "  96.34146341463415,\n",
       "  95.9349593495935,\n",
       "  95.32520325203252,\n",
       "  95.1219512195122,\n",
       "  96.13821138211382,\n",
       "  96.13821138211382,\n",
       "  96.7479674796748,\n",
       "  96.13821138211382,\n",
       "  95.9349593495935,\n",
       "  97.35772357723577,\n",
       "  95.52845528455285,\n",
       "  96.7479674796748,\n",
       "  95.9349593495935,\n",
       "  96.13821138211382,\n",
       "  96.13821138211382,\n",
       "  95.52845528455285,\n",
       "  94.71544715447155,\n",
       "  94.51219512195122,\n",
       "  96.34146341463415,\n",
       "  96.34146341463415,\n",
       "  95.9349593495935,\n",
       "  95.73170731707317,\n",
       "  96.54471544715447,\n",
       "  96.13821138211382,\n",
       "  95.9349593495935,\n",
       "  95.73170731707317,\n",
       "  95.73170731707317,\n",
       "  96.34146341463415,\n",
       "  95.73170731707317,\n",
       "  96.7479674796748,\n",
       "  95.73170731707317,\n",
       "  96.54471544715447,\n",
       "  96.34146341463415,\n",
       "  96.13821138211382,\n",
       "  96.7479674796748,\n",
       "  96.34146341463415,\n",
       "  95.9349593495935,\n",
       "  95.73170731707317,\n",
       "  96.13821138211382,\n",
       "  95.32520325203252,\n",
       "  95.1219512195122,\n",
       "  95.52845528455285,\n",
       "  96.95121951219512,\n",
       "  96.95121951219512,\n",
       "  96.7479674796748,\n",
       "  96.54471544715447,\n",
       "  96.54471544715447,\n",
       "  95.73170731707317,\n",
       "  95.73170731707317,\n",
       "  95.32520325203252,\n",
       "  95.73170731707317,\n",
       "  95.9349593495935,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  96.7479674796748,\n",
       "  96.54471544715447,\n",
       "  96.54471544715447,\n",
       "  95.9349593495935,\n",
       "  95.52845528455285,\n",
       "  96.34146341463415,\n",
       "  95.73170731707317,\n",
       "  96.34146341463415,\n",
       "  96.34146341463415,\n",
       "  96.54471544715447,\n",
       "  95.73170731707317,\n",
       "  97.15447154471545,\n",
       "  97.35772357723577,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  96.7479674796748,\n",
       "  96.34146341463415,\n",
       "  96.34146341463415,\n",
       "  96.7479674796748,\n",
       "  96.7479674796748,\n",
       "  97.35772357723577,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  96.54471544715447,\n",
       "  96.54471544715447,\n",
       "  96.13821138211382,\n",
       "  95.9349593495935,\n",
       "  96.7479674796748,\n",
       "  96.54471544715447,\n",
       "  95.9349593495935,\n",
       "  95.73170731707317,\n",
       "  96.34146341463415,\n",
       "  96.13821138211382,\n",
       "  96.7479674796748,\n",
       "  96.34146341463415,\n",
       "  96.7479674796748,\n",
       "  96.34146341463415,\n",
       "  96.13821138211382,\n",
       "  96.13821138211382,\n",
       "  96.34146341463415,\n",
       "  96.13821138211382,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  96.34146341463415,\n",
       "  96.54471544715447,\n",
       "  95.9349593495935,\n",
       "  96.13821138211382,\n",
       "  96.34146341463415,\n",
       "  96.54471544715447,\n",
       "  96.7479674796748,\n",
       "  96.13821138211382,\n",
       "  96.54471544715447,\n",
       "  96.13821138211382,\n",
       "  94.71544715447155,\n",
       "  94.71544715447155,\n",
       "  95.73170731707317],\n",
       " 'val_VAL_acc': [86.69950738916256,\n",
       "  92.11822660098522,\n",
       "  92.61083743842364,\n",
       "  95.07389162561576,\n",
       "  96.55172413793103,\n",
       "  97.04433497536945,\n",
       "  96.55172413793103,\n",
       "  97.53694581280789,\n",
       "  97.53694581280789,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  98.52216748768473,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  98.52216748768473,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  98.52216748768473,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  98.0295566502463,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  98.0295566502463,\n",
       "  98.0295566502463,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  97.53694581280789,\n",
       "  97.53694581280789,\n",
       "  97.04433497536945,\n",
       "  97.53694581280789,\n",
       "  98.0295566502463,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  97.53694581280789,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  98.0295566502463,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  99.50738916256158,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  98.0295566502463,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  98.52216748768473,\n",
       "  98.0295566502463,\n",
       "  98.0295566502463,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  98.52216748768473,\n",
       "  99.01477832512315,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158,\n",
       "  99.50738916256158,\n",
       "  99.01477832512315,\n",
       "  99.50738916256158],\n",
       " 'val_VAL_jac': [0.13793103448275862,\n",
       "  0.5197044334975369,\n",
       "  0.6059113323982126,\n",
       "  0.6477832535804786,\n",
       "  0.6674876870780155,\n",
       "  0.6798029603629276,\n",
       "  0.6904761990890127,\n",
       "  0.706896561119944,\n",
       "  0.705254526561117,\n",
       "  0.7298850684330381,\n",
       "  0.747126447743383,\n",
       "  0.7512315341404506,\n",
       "  0.7536945906765943,\n",
       "  0.7487684799532585,\n",
       "  0.7512315341404506,\n",
       "  0.7610837532381706,\n",
       "  0.7512315364894021,\n",
       "  0.770935969986939,\n",
       "  0.7586206990509785,\n",
       "  0.7561576448637863,\n",
       "  0.768472915799747,\n",
       "  0.7660098616125548,\n",
       "  0.770935969986939,\n",
       "  0.7758620783613233,\n",
       "  0.770935969986939,\n",
       "  0.7660098616125548,\n",
       "  0.7758620783613233,\n",
       "  0.7758620783613233,\n",
       "  0.7635468074253627,\n",
       "  0.768472915799747,\n",
       "  0.7635468074253627,\n",
       "  0.7594417139814404,\n",
       "  0.7545156056070562,\n",
       "  0.7520525490709127,\n",
       "  0.7619047681686326,\n",
       "  0.769293930730209,\n",
       "  0.7660098616125548,\n",
       "  0.7471264430454799,\n",
       "  0.7545156056070562,\n",
       "  0.7569786574452969,\n",
       "  0.7553366205375183,\n",
       "  0.7594417139814404,\n",
       "  0.7717569872663526,\n",
       "  0.7545156056070562,\n",
       "  0.7553366205375183,\n",
       "  0.750410512163134,\n",
       "  0.7520525490709127,\n",
       "  0.7619047658196811,\n",
       "  0.7528735663503262,\n",
       "  0.7569786574452969,\n",
       "  0.7545156032581047,\n",
       "  0.7569786574452969,\n",
       "  0.7471264406965283,\n",
       "  0.7471264406965283,\n",
       "  0.7430213496015576,\n",
       "  0.7380952412271734,\n",
       "  0.747947457975942,\n",
       "  0.7520525490709127,\n",
       "  0.739737278134952,\n",
       "  0.7307060786655971,\n",
       "  0.7233169161040207,\n",
       "  0.7348111697605678,\n",
       "  0.739737278134952,\n",
       "  0.747947457975942,\n",
       "  0.7454844037887498,\n",
       "  0.7282430244784049,\n",
       "  0.7463054257660664,\n",
       "  0.7224958988246072,\n",
       "  0.7274220071989914,\n",
       "  0.7463054257660664,\n",
       "  0.7422003323221441,\n",
       "  0.7356321870399813,\n",
       "  0.7183908077296365,\n",
       "  0.7331691328527892,\n",
       "  0.7331691328527892,\n",
       "  0.7208538619168287,\n",
       "  0.7307060786655971,\n",
       "  0.7356321870399813,\n",
       "  0.7422003323221441,\n",
       "  0.7430213496015576,\n",
       "  0.7380952412271734,\n",
       "  0.7372742239477599,\n",
       "  0.7348111697605678,\n",
       "  0.7495894948837205,\n",
       "  0.7422003346710957,\n",
       "  0.7397372804839035,\n",
       "  0.727422009547943,\n",
       "  0.738095243576125,\n",
       "  0.7479474603248935,\n",
       "  0.7577996770736619,\n",
       "  0.7602627312608541,\n",
       "  0.7471264430454799,\n",
       "  0.7348111721095193,\n",
       "  0.7323481179223272,\n",
       "  0.727422009547943,\n",
       "  0.7249589530117994,\n",
       "  0.717569790450223,\n",
       "  0.7151067362630309,\n",
       "  0.7077175737014545,\n",
       "  0.7200328446374151,\n",
       "  0.7233169161040207,\n",
       "  0.7134646993552523,\n",
       "  0.7159277535424444,\n",
       "  0.717569790450223,\n",
       "  0.7274220071989914,\n",
       "  0.706075536793676,\n",
       "  0.7093596129581846,\n",
       "  0.7216748838941452,\n",
       "  0.7167487755197609,\n",
       "  0.7339901524811543,\n",
       "  0.7216748815451937,\n",
       "  0.7142857189836174,\n",
       "  0.7167487755197609,\n",
       "  0.7093596106092331,\n",
       "  0.717569790450223,\n",
       "  0.7151067386119824,\n",
       "  0.7118226671453767,\n",
       "  0.7183908077296365,\n",
       "  0.7142857189836174,\n",
       "  0.706896556422041,\n",
       "  0.7068965540730895,\n",
       "  0.7200328469863666,\n",
       "  0.7290640441067701,\n",
       "  0.7216748815451937,\n",
       "  0.7200328446374151,\n",
       "  0.7241379333834342,\n",
       "  0.7307060786655971,\n",
       "  0.7520525490709127,\n",
       "  0.7504105145120855,\n",
       "  0.7438423692299228,\n",
       "  0.7389162608555385,\n",
       "  0.7389162608555385,\n",
       "  0.7479474603248935,\n",
       "  0.7356321893889328,\n",
       "  0.7372742286456629,\n",
       "  0.7372742286456629,\n",
       "  0.7307060810145486,\n",
       "  0.7241379380813373,\n",
       "  0.7364532090172979,\n",
       "  0.727422009547943,\n",
       "  0.7192118273580016,\n",
       "  0.7159277535424444,\n",
       "  0.7175697927991745,\n",
       "  0.717569790450223,\n",
       "  0.7364532090172979,\n",
       "  0.7208538619168287,\n",
       "  0.7397372828328551,\n",
       "  0.7356321917378844,\n",
       "  0.7339901524811543,\n",
       "  0.7454844061377013,\n",
       "  0.738095243576125,\n",
       "  0.7504105145120855,\n",
       "  0.7553366228864697,\n",
       "  0.7454844061377013,\n",
       "  0.740558297763317,\n",
       "  0.7315270959450106,\n",
       "  0.7438423668809713,\n",
       "  0.7430213519505092,\n",
       "  0.7356321870399813,\n",
       "  0.7282430268273565,\n",
       "  0.7290640417578185,\n",
       "  0.7290640417578185,\n",
       "  0.7307060786655971,\n",
       "  0.7266009875706264,\n",
       "  0.7241379333834342,\n",
       "  0.7323481155733756,\n",
       "  0.7036124826064838,\n",
       "  0.7110016475170117,\n",
       "  0.7151067386119824,\n",
       "  0.7290640464557215,\n",
       "  0.7241379357323858,\n",
       "  0.717569790450223,\n",
       "  0.7307060857124517,\n",
       "  0.7397372804839035,\n",
       "  0.7298850637351351,\n",
       "  0.7249589553607508,\n",
       "  0.727422009547943,\n",
       "  0.7422003346710957,\n",
       "  0.7438423692299228,\n",
       "  0.7528735686992777,\n",
       "  0.7504105145120855,\n",
       "  0.749589497232672,\n",
       "  0.740558297763317,\n",
       "  0.7479474603248935,\n",
       "  0.738095243576125,\n",
       "  0.7438423692299228,\n",
       "  0.7430213519505092,\n",
       "  0.7315270982939621,\n",
       "  0.7233169208019238,\n",
       "  0.7274220118968945,\n",
       "  0.7323481202712787,\n",
       "  0.7454844061377013,\n",
       "  0.740558297763317,\n",
       "  0.7495894948837205,\n",
       "  0.7356321893889328,\n",
       "  0.7413793173916822,\n",
       "  0.7405583024612201,\n",
       "  0.73891626320449,\n",
       "  0.73891626320449,\n",
       "  0.7224958988246072,\n",
       "  0.7323481155733756,\n",
       "  0.7356321870399813,\n",
       "  0.7323481179223272,\n",
       "  0.7216748838941452,\n",
       "  0.740558297763317,\n",
       "  0.7290640417578185,\n",
       "  0.7348111697605678,\n",
       "  0.7224959011735588,\n",
       "  0.7200328469863666,\n",
       "  0.727422009547943,\n",
       "  0.7266009922685295,\n",
       "  0.7224959011735588,\n",
       "  0.7241379380813373,\n",
       "  0.7348111721095193,\n",
       "  0.738095243576125,\n",
       "  0.7479474603248935,\n",
       "  0.7528735686992777],\n",
       " 'val_VAL_acc_1': [54.1871921182266,\n",
       "  68.47290640394088,\n",
       "  65.51724137931035,\n",
       "  70.93596059113301,\n",
       "  71.92118226600985,\n",
       "  72.41379310344827,\n",
       "  73.39901477832512,\n",
       "  73.39901477832512,\n",
       "  73.89162561576354,\n",
       "  75.86206896551724,\n",
       "  78.32512315270937,\n",
       "  75.36945812807882,\n",
       "  78.81773399014779,\n",
       "  75.86206896551724,\n",
       "  75.36945812807882,\n",
       "  78.32512315270937,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  78.32512315270937,\n",
       "  76.84729064039409,\n",
       "  78.81773399014779,\n",
       "  78.81773399014779,\n",
       "  78.81773399014779,\n",
       "  78.81773399014779,\n",
       "  77.33990147783251,\n",
       "  78.81773399014779,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  78.81773399014779,\n",
       "  79.80295566502463,\n",
       "  78.81773399014779,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  81.2807881773399,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  81.2807881773399,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  78.81773399014779,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  78.81773399014779,\n",
       "  78.81773399014779,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  81.77339901477832,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  79.80295566502463,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  78.81773399014779,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  78.32512315270937,\n",
       "  78.81773399014779,\n",
       "  78.81773399014779,\n",
       "  77.33990147783251,\n",
       "  77.33990147783251,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  78.32512315270937,\n",
       "  77.83251231527093,\n",
       "  77.33990147783251,\n",
       "  79.3103448275862,\n",
       "  78.81773399014779,\n",
       "  78.81773399014779,\n",
       "  77.83251231527093,\n",
       "  78.32512315270937,\n",
       "  77.83251231527093,\n",
       "  77.83251231527093,\n",
       "  76.35467980295566,\n",
       "  76.35467980295566,\n",
       "  76.35467980295566,\n",
       "  76.35467980295566,\n",
       "  76.84729064039409,\n",
       "  77.83251231527093,\n",
       "  79.80295566502463,\n",
       "  77.83251231527093,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  77.33990147783251,\n",
       "  79.3103448275862,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  78.32512315270937,\n",
       "  78.32512315270937,\n",
       "  77.83251231527093,\n",
       "  77.33990147783251,\n",
       "  76.35467980295566,\n",
       "  76.35467980295566,\n",
       "  76.84729064039409,\n",
       "  78.32512315270937,\n",
       "  78.81773399014779,\n",
       "  77.83251231527093,\n",
       "  78.81773399014779,\n",
       "  79.80295566502463,\n",
       "  78.32512315270937,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  81.2807881773399,\n",
       "  82.26600985221675,\n",
       "  81.2807881773399,\n",
       "  81.77339901477832,\n",
       "  81.77339901477832,\n",
       "  81.2807881773399,\n",
       "  80.29556650246306,\n",
       "  78.32512315270937,\n",
       "  79.80295566502463,\n",
       "  78.32512315270937,\n",
       "  78.81773399014779,\n",
       "  78.32512315270937,\n",
       "  78.81773399014779,\n",
       "  77.33990147783251,\n",
       "  78.32512315270937,\n",
       "  78.32512315270937,\n",
       "  78.81773399014779,\n",
       "  78.32512315270937,\n",
       "  78.32512315270937,\n",
       "  77.33990147783251,\n",
       "  77.83251231527093,\n",
       "  78.32512315270937,\n",
       "  77.83251231527093,\n",
       "  76.84729064039409,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  81.2807881773399,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  80.78817733990148,\n",
       "  79.3103448275862,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  79.80295566502463,\n",
       "  81.2807881773399,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  79.3103448275862,\n",
       "  78.81773399014779,\n",
       "  79.80295566502463,\n",
       "  80.78817733990148,\n",
       "  80.29556650246306,\n",
       "  80.78817733990148,\n",
       "  78.81773399014779,\n",
       "  78.32512315270937,\n",
       "  80.29556650246306,\n",
       "  79.80295566502463,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  80.29556650246306,\n",
       "  78.81773399014779,\n",
       "  77.33990147783251,\n",
       "  78.81773399014779,\n",
       "  79.3103448275862,\n",
       "  77.33990147783251,\n",
       "  78.32512315270937,\n",
       "  77.83251231527093,\n",
       "  77.83251231527093],\n",
       " 'test_loss': 5.7464833580009325,\n",
       " 'test_ATT_loss': 0.8268375810287582,\n",
       " 'test_VAL_loss': 1.6398819256573915,\n",
       " 'test_ATT_acc': 95.82504970178927,\n",
       " 'test_VAL_acc': 100.0,\n",
       " 'test_VAL_jac': 0.7630208432674408,\n",
       " 'test_VAL_acc_1': 79.16666666666667,\n",
       " 'model_filename': 'model_storage/GAT/model.pth'}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.save_dir+'GAT_knn_feature/best_config.p', 'wb') as fp:\n",
    "    pickle.dump(train_state,fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.save_dir+'GAT_knn_feature/best_config.p', 'rb') as fp:\n",
    "    train_state = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.DataFrame(train_state['val_VAL_acc'], columns=['val_VAL'])\n",
    "states['train_VAL'] = pd.DataFrame(train_state['train_VAL_acc'])\n",
    "states['val_ATT'] = pd.DataFrame(train_state['val_ATT_acc'])\n",
    "states['train_ATT'] = pd.DataFrame(train_state['train_ATT_acc'])\n",
    "states['val_VAL_1'] = pd.DataFrame(train_state['val_VAL_acc_1'])\n",
    "states['train_VAL_1'] = pd.DataFrame(train_state['train_VAL_acc_1'])\n",
    "states['train_VAL_jac'] = pd.DataFrame(train_state['train_VAL_jac'])\n",
    "states['val_VAL_jac'] = pd.DataFrame(train_state['val_VAL_jac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_VAL</th>\n",
       "      <th>train_VAL</th>\n",
       "      <th>val_ATT</th>\n",
       "      <th>train_ATT</th>\n",
       "      <th>val_VAL_1</th>\n",
       "      <th>train_VAL_1</th>\n",
       "      <th>train_VAL_jac</th>\n",
       "      <th>val_VAL_jac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.699507</td>\n",
       "      <td>88.919668</td>\n",
       "      <td>50.203252</td>\n",
       "      <td>58.171745</td>\n",
       "      <td>54.187192</td>\n",
       "      <td>50.415512</td>\n",
       "      <td>0.117729</td>\n",
       "      <td>0.137931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.118227</td>\n",
       "      <td>91.412742</td>\n",
       "      <td>64.634146</td>\n",
       "      <td>69.252078</td>\n",
       "      <td>68.472906</td>\n",
       "      <td>59.279778</td>\n",
       "      <td>0.446445</td>\n",
       "      <td>0.519704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.610837</td>\n",
       "      <td>92.243767</td>\n",
       "      <td>75.609756</td>\n",
       "      <td>77.285319</td>\n",
       "      <td>65.517241</td>\n",
       "      <td>61.772853</td>\n",
       "      <td>0.516621</td>\n",
       "      <td>0.605911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95.073892</td>\n",
       "      <td>94.459834</td>\n",
       "      <td>79.268293</td>\n",
       "      <td>81.717452</td>\n",
       "      <td>70.935961</td>\n",
       "      <td>65.650970</td>\n",
       "      <td>0.555402</td>\n",
       "      <td>0.647783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96.551724</td>\n",
       "      <td>96.675900</td>\n",
       "      <td>81.707317</td>\n",
       "      <td>86.703601</td>\n",
       "      <td>71.921182</td>\n",
       "      <td>68.698061</td>\n",
       "      <td>0.592798</td>\n",
       "      <td>0.667488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>99.014778</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.544715</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>79.310345</td>\n",
       "      <td>89.750693</td>\n",
       "      <td>0.897969</td>\n",
       "      <td>0.724138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>99.507389</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.138211</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>77.339901</td>\n",
       "      <td>91.689751</td>\n",
       "      <td>0.907202</td>\n",
       "      <td>0.734811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>99.507389</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>94.715447</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>78.325123</td>\n",
       "      <td>91.135734</td>\n",
       "      <td>0.886427</td>\n",
       "      <td>0.738095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>99.014778</td>\n",
       "      <td>99.722992</td>\n",
       "      <td>94.715447</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>77.832512</td>\n",
       "      <td>91.135734</td>\n",
       "      <td>0.885965</td>\n",
       "      <td>0.747947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>99.507389</td>\n",
       "      <td>99.722992</td>\n",
       "      <td>95.731707</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>77.832512</td>\n",
       "      <td>90.027701</td>\n",
       "      <td>0.870729</td>\n",
       "      <td>0.752874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_VAL   train_VAL    val_ATT   train_ATT  val_VAL_1  train_VAL_1  \\\n",
       "0    86.699507   88.919668  50.203252   58.171745  54.187192    50.415512   \n",
       "1    92.118227   91.412742  64.634146   69.252078  68.472906    59.279778   \n",
       "2    92.610837   92.243767  75.609756   77.285319  65.517241    61.772853   \n",
       "3    95.073892   94.459834  79.268293   81.717452  70.935961    65.650970   \n",
       "4    96.551724   96.675900  81.707317   86.703601  71.921182    68.698061   \n",
       "..         ...         ...        ...         ...        ...          ...   \n",
       "212  99.014778  100.000000  96.544715  100.000000  79.310345    89.750693   \n",
       "213  99.507389  100.000000  96.138211  100.000000  77.339901    91.689751   \n",
       "214  99.507389  100.000000  94.715447  100.000000  78.325123    91.135734   \n",
       "215  99.014778   99.722992  94.715447  100.000000  77.832512    91.135734   \n",
       "216  99.507389   99.722992  95.731707  100.000000  77.832512    90.027701   \n",
       "\n",
       "     train_VAL_jac  val_VAL_jac  \n",
       "0         0.117729     0.137931  \n",
       "1         0.446445     0.519704  \n",
       "2         0.516621     0.605911  \n",
       "3         0.555402     0.647783  \n",
       "4         0.592798     0.667488  \n",
       "..             ...          ...  \n",
       "212       0.897969     0.724138  \n",
       "213       0.907202     0.734811  \n",
       "214       0.886427     0.738095  \n",
       "215       0.885965     0.747947  \n",
       "216       0.870729     0.752874  \n",
       "\n",
       "[217 rows x 8 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAxRElEQVR4nO3deXiU1f338ffJZLIvhCwQCJAAYV8CAQQRFMENF1DR4o615ddqLfapFdeKVq2ttVq3KloRt1BAKIqIIIsgmwRk30IgQBLIStZJJrOc54+ZxASSAJmQyUy+r+vKNTP33MuZm+EzZ86c+xyltUYIIYR38XF3AYQQQjQ/CXchhPBCEu5CCOGFJNyFEMILSbgLIYQX8nV3AQCioqJ0fHy8u4shhBAeZdu2bfla6+j6nmsV4R4fH09qaqq7iyGEEB5FKXWsoeekWUYIIbyQhLsQQnghCXchhPBCEu5CCOGFJNyFEMILnTPclVIfKqVylVJ7ai1rr5RaqZRKc95G1HruCaXUYaXUQaXUNRer4EIIIRp2PjX3j4Brz1j2OLBKa50IrHI+RinVD5gK9Hdu845SytBspRVCCHFeztnPXWu9TikVf8biScAVzvtzgbXATOfyeVprM3BUKXUYGAFsaqbytk6mQkhbAQNvh8J0yN4BA6dA+irH8z0nOG5z9sG+JaDtjsedk6G383MzPw32fAF2W4sXXwjhRjF9YcAtzb7bpl7E1EFrfRJAa31SKRXjXN4Z2FxrvUznsrMopaYD0wG6du3axGK0AlrD/34Lh5ZDeT6kfugI+I3/glO7HesM/xWEdIT1/wBrJaAA5zj6Q++FiARY9w+wlDufE0K0GQNuaVXh3pD6kqne2UC01rOB2QDDhg3znBlDsndAZA8wBkPGeshKdQR7UBSseMqxztD7YMfnMOwBx+OtHzhu48fAlA8hJAZsFvhuFmx6y/Fcl0vgto8grFMLvyAhhDdqarjnKKVinbX2WCDXuTwT6FJrvTgg25UCtho2C6z8M2x+B9r3cAR82grHc52HwS2z4b3Lof8kuOkNmPgK+Po7nr/yabBbITgalPPzz2CEa16EsY+CtcoR+Epq7UKI5tHUcP8SuA942Xm7pNbyz5VS/wQ6AYnAj64W0q1MhVCeB1/NgOObYPAdkL7a8XfNSxB/GUT3cQT5H3ZDQDvHdtXBDhDUvuH9B0Y0/JwQQjTROcNdKZWC48fTKKVUJvAsjlCfr5R6ADgO3Aagtd6rlJoP7AOswENaa8/8hdBmhdV/gQ2vOx4bg+CWD2DQbY7ANxVCVM+620hQCyFaCdUaJsgeNmyYbnWjQi7+DexMgSF3Q9wIRw09soe7SyWEEDWUUtu01sPqe65VDPnb6hz61hHsYx6F8c+4uzRCCHHBJNxrO7zK0RRTcASi+8LlM91dIiGEaBIJ92qmQlg0HfyCoMc4R7D7+rm7VEII0SQS7tVWPA2VRXDv/6DjQHeXRgghXCKjQgKkr4Edn8HoGRLsrdzh3FJ2nCiitNLi8r6sNjs5JZXNUCqHPy3YyROLdjXb/tqSzNMmHvxsG6NfXs3p8qpzrp+WU0ryX1ayPi2vBUrnmSTczaWw9BGI7AljH3N3adzuwKkS9mYXu7sY9dpxoogJ/1zH5Lc38Mf5O13e37/XpnPJS6v44/ydFJnOHSiNsdjsfL37JCv25tAaeqBdCKvNzqLtmXy25Rj7T5a4pQwPfJTK6gO5ZBVVkLL1eKPraq157qt9FJRX8c6a9BYqoedp2+Gefxg+uAqKjsONb4AxwN0luijmp55g54miRtex2Ow8uXg31/1rPXfM3oypylrvepUWG69/d+i8ale1WW123l5zmO3HT1/QdrWt3HcKg49ibK9ofsworAnRfdklzE89AcCag7msPZjb4D72nyzh9e8OYaqysmBbJh3DAvhyZxb3f7SVSsv5X5KhtebTzcc4nFsKwO6sYkxVNgrKq8gpMTPvx+Pszry4H5J7sor5bwNBeCinlDkbjmK3n/uDZtWBXP7f/J08tXgPE99Yz8yFu8grNZ9XGSw2O59uPsYTi3bz0rL9ZJ42XdBrADhZXMHBnFIevbo3l/WM4uONx1iyI4snFu3mhaX7aspit2veX3eE38/bwQ+H8+kXG8amIwUs3JbJm6vS6j12scnCaysPkZFf3uDxv9yZzcbD+djsmtnr0jmUU9poeU8Umnh1xUFOFlc0ul7t95o7tN029/1LHX3Zff3g7kUQP9rdJboo0nJKeWzhLkL9fVnw21H06Rh21jpaa55YtJuF2zK5flAsX+86yRfbs7hnZLez1l17MI/Xv0sju6iCv08ZXOe5YpOFx77YyWPX9qFHdEid577Zc4pXvj0IwKSkTsy8tg+d2gUCYKqy8uSi3dyaHMeYxOgGX8vqA3kkd4tg4oCOrDuUR0aBidjwAP7v01ROFFZgUIqn/rcbhWL1o5cTGx5YZ/t/rjzEW6vTsGvHt4DjhSb+PmUQYQG+/Paz7fxqbioPX9mTL7Zn0i7Ijwev6EG7oPp/VP9ufy5P/28PvTuE8vXvL2PzkYKa59an5fHE4t0M6hzO/x4ajbrAYSV2nijiPz8c5flJ/Rs8PsBLy/az6UgBl/eK4cudWRwvNPH7KxOJCPbjoc+2k5ZbRn6ZmT9d06fR421KLyDA6MO3j4zlk03H+GhjBl/vPkmP6GCGdI1g1k39692usLyKqbM3cSinjMhgP0orrczdmEHvjqH07RjGizcPwNdQf/3xdHkVb685TIDRQM8Yx3tlZPdIEqKCeWBuKjPm7SA80IipysrmowX8d/oo3lx9mHe/TycqxI8JfWN4+dZBXPa31Ty6wPEt7q01h+ndMZTuUcH89ZZBBPoZmL0+nbfXpPPO2sO8OHkgtw/vUqcc+7JLeGTeT/j6+DC2VzTf7c/h/fVHWfzgpcRFBJ1V7nk/HufPX+6lymrn272nuGdUPOsP5fHyrYNoH+z4t8ovM/PqikP8d+tx7BqqrHaGdI1g6a5sXpg8gNAAY6P/Hs2lbYZ7cRbMvwdik+D2j6Fdl3Nu4mkqqhy10DkbM/Dz9SHI38D9c7ay+MHRdAyv+w3lo40ZLNyWyYzxiTwyIZEThSbeXZvOvB+PEx5o5MmJfRnQORygJsQWbMvkqn4dOW2q4q3Vh7l/dDwVFhvf7s0hNjzwrED4cMNRukUGcdPgTsxed4Rv957i/8b24DeX9+Ddten8b0c2K/bl8PZdQ+nVIZTOzuCvtNi47d1NXJLQnv0nS3j8uj4M6eq4Evin46dZVlzJicIKwgON/HHBTvx9fdBoHv78J0oqLXSJCOK3V/Rga8Zp3liVxi1DO2O2OJpQ/Aw+XNO/I+GBRv5680Ce+2ofv5i9mQCjD1VWR1PFf/9vVJ0PqpJKC5mFFbzw9T7aBRk5mFPKZ1uOs/lIIV3aB5J5uoJ/f5+O1rAzs5jtx4voGB5AaaWF+MhgAow/T29QUWVjwj+/51RJJcndIpgzbTjB/r78bfkBNqYXcKq4ko8fGFFnmxV7HR+Sr9w2mE1HCtDa8e/34Q9HqbLZWbQ9i1HdI0nLLSO5WwRvr0mnc7sg7rzEMfKqqcqKn8GnTuhuPlLAsG7t6RYZzNM39OPOS7ryztp0Dp4q5aONGUxJjqv596/tlW8Pkp5Xznv3JHN1vw6cLK7krTWHSc8t47+pJxgYF87dI7ux/fhpHvpsO3PuH86uE8U8s2QPZqu9Zj+XJLQnLMCXvrFh9IsN41eXJdCrYyhThsax5mAuv/44lQGzvkVruOuSrrwweUDNB+ZzN/Unq6iSGwfFMmdjBpmnK1iyM5vyKhuv/yKJz7ccZ0xiFGarnee+2ssVvaOJCQtg1pd72XykAIOPIjzQSESQH9/tz+GWIZ35bn8Ov3hvM09M7EN8ZDCfbj7GxvQCLu8VzWdbjjG6ZxRTkuN4dMFOnvmfYw6j6NCDPHtjfz7aeJQ3Vx2mwmJj2qUJ5JRU8sH6o6COUmW1U1hexYfThmNs4EOvObXNK1T3fAELfwnTv4dOSS133Bb0wEdb2ZpRiNlqZ3JSZ+69tBu3v7uJrpHBLPjNKAKNBjYczqdnTAjXvLaOod0i+Oj+4SilWLIjixnzdtA9KpjiCgvFFRbm3D+cMYnRXPv6OgL9DJworCC/zPF1OdjPgF1DSIAveaVmokL82PzEeHwNPvx0/DRbMwp5adkBZt3Yj2mjE8g8beLlbw6wdNdJYsMDKCyvYnTPKA6eKiWrqAKl4N27k7mmf0eW7T7Jg59tr3ld3z4ylp4xIQx+bgWjekSy4XA+YxKjuHlIZ37z6XZmjE9EA2+sSqN3h1CyiyooNTu+Fl/TvwPv3JVMfpmZK/+xljGJ0bx7T3LNvk8VV7Jyfw5X9e1AQbmZe//zI0H+BhY/OJqoEMdYQTe++QO7sxzNLZ88MIL3vj/Cj0cLAZg6ogs/HM7nSF45kcF+VNnsBBoN5DqbFXp3COWrhy/Dz9fxHzs1o5Ap727i2v4dWbk/hzGJUcwYn8jN72xkTGIU69PySYwJ4ekb+nF5r2jKzVaufHUtOSVm2gUZKTJZiA0P4GSx40fhOdOGk/LjcVbsc+xrzrTh/OrjVNan5fPu3cn0iA5myrubmNA3puZbV5GpiiF/Wckfr+rF765MrPMeKq6wMOqvq7huQCyv3l73W9rOE0VMfmcD0y6N59kb636Qa62ZOnszh3JKWfPoFfxp4S5W7sshqUs70nPL6B4dzOW9YxgR355ffrSVKpudCX078MF99V5oyfeH8th6tJCoED/uHtmtwW8D1eZsOMpzX+2jQ5g/OSVm/jt9JDFhAVz92vdcPzCWey+N55Z3NhLsZ6C8ysYLkwcwvm8M6w/lMyU5jl1ZxTy2cCeHcsoAMBoU/TqFs/NEEYPjwpk3fRSBfgZ+SMunzGxl85ECPt6UQVxEEMcLTVzZJ4YnJ/alZ0wIOSWVXPmPtXQIC+COEV15cdl+/jKpP/eMim/0NZwvuUL1TJnbwDcAOtT/dbM1251ZTEZBOTcObnhoYLtds+VooWPUeA2/vCyB3h1DeefuZKbN+ZHZ644QFeLHn5fsxc/XB7td8+cb+9XUhm4a3Im4iCAGxYVjMtv4xexN/PbT7bx/7zAOnCrl0at7cWtyHDuOFxEWaKRLRBATXvuevFIztyXHsWBbJpuOFGC22Jn+SSp2DRFBRqYMc3xDiosI4q07h3LfpYU8/9U+ysxWXrx5AEaDD1uPFvL6d2k8/9U+Lu8VzZIdWUSH+tOnYyh5pWZ6dQhBKcXgLuGs3JeDn68PT1/fjy7tg1j+yBh6xYRi15qxiVEM6RpBkamKrRmFGA0+XJYYhcFH0SEsgC8fvoyIM5o8OoYH1DRFdQwP4D/ThvOL9zbx7JK9vH3XUA6eKmV3VjH3jerGzUPjSOrSjn6xYfz1mwN8sT2T8X07UGSycCSvnHF9YogND2D2uiM8eEUPx7eDbw4wd2MGvx7bHXC0mQPMuqk/Y3tF8+Ti3axPyyfQaODNO4aw7dhpnl+6j/s+/JExiVEopcgpMXNVvw6s3JfDgM5hTBrcmReX7Wdc72jG9YlhXJ8Y9mYXExcRhK/Bh7fuHMrU2Zv49cephAb4UlppZdH2LP7fVb3pGB7AlqOFaA2XdI88630UHmhkSnIc8348QWiAL1f168ClPSJ5/bs0/r02nchgfx6Z0Ous7ZRSzLqpPze8+QMPzE1l+/HTJMaEsONEEb4+in/+Iqnm29CNgzvxxfZMRnZveHC9y3tFc3mvhpvrznT/6ATaB/vx12UHGNq1HSMS2qOUYvrY7ry9Jp1le04RE+rPij+MJT2vjKFdI1BK1TTZJHVpx7Lfj+GHw/lUVNno3ymcrpFB7DxRREJ0MIF+jm9SlyVGATCqeyTLdp/Ez9eHub8cUaesHcIC+Pr3Y4gI8iM8yMgnm4/xw+H8mnB/a3UacRFBTB5S77QXrtFau/0vOTlZt6gPrtb6g6ta9pjNZOp7m3SPJ77W2UWmBtc5mlemu81cqlO2HNNllZY6z/1yzo966PMr9OV/X63Hv7pW3/TWD/qt1WmNHjO7yKQvefE7nfjkMt1t5lK99WjBWeu8vy5d3/n+Jm0yW/WAPy/Xl/51le799DJ945vr9ZG8Ml1cUVXvvm02uy431y3jhsN5utvMpfoP837SiU8t07O+3KPtdruutFhr1nll+QHdbeZS/cryA42W3VWvrTyou81cqjel5+u/L9+vEx5fqnNLKs9az2R2lO297w/rbjOX6q93ZWubzV6zXGutp324Rff/83KdX+rY/tH5O3TyX1Zou92utdZ6U3q+vvWdDfr1lYdqtqm0WPXs79P1iBdX6gHPLtcvfb1Pm8xWfcs7G/TC1BM6p6RCX/XPtTo14+x/k2rlZot+dcVBPe6VNXrJjiyd8PhS/ffl+7XWWs9cuFP3fnpZnXNbW0Z+mR750ne699PLdM8nv9YPf75dd5u5VD/8+XZ9sqii0XOXsuWY7jZzqe75pOP9+sBHW/V73x+us86hUyV6wqtrdUZ+WaP7aooqq63O67LZ7HpB6gk9/tW1+pvd2c16LJPZqq02+znX+9OCHXrwc99qm82uyyotus/T3+gnFu1q8nGBVN1Arra9ZhmbBf4a55hI49qXWuaYF8hqs2PTGn/futPPllRaGPr8Sqx2zYNX9OCxa/ugtWbprpP8e206iR1CmHltH7YfP83vPv+JpQ9fdlZb6YbD+dz1wRYA3rhjCDc18g2gtn3ZJdz+3iasdju7nr2mpmmhPnM3ZrB8zynaB/sx66b+RIf6N7huQ15atp/Z644AsPjBS2va2asdzi3j3e/TeX5Sf4L8Lt4X0Op2cQCr3U6vDqF88sAlDa6fedrk+OZRT7nSckq56rV1/GFCL2ZMSOS6f60nOtSfj3854qKVvz7TP05lU3oBU4bFMWdDBneM6Mpfb2n8+o5ik4Vb393I4dwybkuO4+9TBp3XD8WfbD6GzWZn2uiE5iq+R/tiWyZ/XLCTb2aM4VBOKTPm7eC/00fW+83pfEizTG05ex1T3cUln3tdN9Ba85tPt7P/ZAlf/PZSCsrNHM0vJzY8gNwSM1a7pkv7QD7/8TgPjevJG6vSeG/dEXpEB7N8zyk2pRdw/aBYjAZFrw6hZ+3/0h6R9OkYSnGFhesGdDzvcvXrFMYnD4wgu6iy0WAHuO/SeO67NP5CX3odT07sy/UDY9mTXUxSl3ZnPd8zJoR/3Db47A2bWaCfgTfuGMKjC3aSlV/BY+foeRIXEdRguRI7hHJF72g+3XKMB8YkkJZTyrje59/c0FyenNiX38/7iTkbMhjZvT3PNdAbprbwICOfPDCCb/ec4q6R3c67B1B9Pa7askuczU+bjxTwQ1o+seEBDI9vZL4HF7S9mvvWD+DrP8KMXRDR+t543+3L4Vcfp6IUtA/yo6BWf/JukUEUllfx/r3DmDp7M1EhfuSXVXH3yK48d9MANqUXcPd/tuDn60NiTAhf/35MvcfIPG3CYtMkRAW31MvyeFVWO6kZhYzsHomPT9NnzFp3KI97P/yRqcO7MG/rCd6+cyjXD4ptxpKeH7tds+lIAUld2hHs3/bqeO405u+rCfbzJT2vjPtHJ/DkxL5N3pfU3Gvb+z9o19Xx5wZ7sor5dPMxLu0ZxY2DYskpMfPm6jSOFzouwNh/soSeMSE8ObEPTy/ew8NX9uTGwZ148ev9fH8oj+sHxTKyeySf//oSXv7mAKN6RPHcTQMw+ChG93TUyg+cKmVAp7O7rlWrr/+uaJyfrw+X9oxyeT9jEqMY2rUd87Y6Lroa0Pns6w5ago+PYnQzvB5x4SYndea9748Q5OfLbclxF+04bavmfmoPvDsarnreMY5MCymusPB/n6SSllNGoakKg1JY7ZrwQGNNf/R+ncJQCvwMPnX6clcrM1t57su93D2yG4PraaaoNj/1BI8t3MXzk/pzbzN1txLNy2y18fHGYxwtKOeFSQNc+iYg2japudssjjlPt38MvoEw5J6LcpjdmcXklFTSNTKopr27ymrnN59sY9ux00xJjqNzu0DuGRnPyv057DhxmiA/X+4Z2Y0u7RuvTYf4+/LKebQxT07qTH6ZmUmDL0LXKtEs/H0NNd0hhbhY2ka4b/0Alj/uuD/sl41PWN0ExSYLf1ywk+/25wCOIP7xqfEEGg08/sUuNh0p4LVfDObmIT9/BZuSHMeUi/CVzM/Xhwev6HnuFYUQXs37w91uhy3vQdxwuOF1iEo85yYX6qONGaw6kMNj1/YmJjSARxfsZNX+XI7ml7PopywevbpXnWAXQoiLzfvD/fBKOH3UMRdqxwEu7+5kcQX/+PYQm9Lz8TcaePfuZFYfzCWpSzsevKInNrvmlW8PMHvdEfafLGFSUiceGic1aSFEy/L+IX9T50BoLPS9yeVd5ZZUcu3r6/lqVzbD4tuTW1LJi8v2syuziHG9YwAw+ChuGNSJ3VnFBBoNPHNDvwseFVAIIVzl3eGuNRzfCL2uAcP5D7O59mAuhfWMV/635QepqLLx1e8u4407hnDbsC6sO5SH1nBln5ia9W52jhMxY0JizYBTQgjRkry7WaYgHSqLofP5X42allPKtDlb6RsbxjM39GXR9izMVjs2u51lu0/xm8t70LujoyfMtEvjmbspg6gQf/rF/txfeUDncNb9aRxd2gc2dBghhLiovDvcs5x95zvXP5RofVYfcMzicyinlDvf30JYgG9N7fvyXtH87sqf28/jo4KZPrY70SH+Z/VV7hopFwoJIdzHy8N9G/iFQHTvc65aWF5FiL8vaw7m0qdjKDPGJ3Iop4xfXhbf6MwpT1zX9EuHhRDiYvHucM9MhU5DwMfQ6Gp2u+bGN38gPNDIoZxSpo/tznUDY7mu8YHyhBCi1fLecLdUwqndMOqhBldZezCXMrOVLhFBZBVVkFXkmPB2XK0fR4UQwhN5b7gXpoPdArGDGlzl+a/2cbK4kl8M74JS8NTEvvx4tJAhjYzdIoQQnsB7w93knI0+uP7xsjPyyzmSXw7A3E0ZDOnSjl+N6c6vxsiYH0IIz+e9/dxNjkmLCax/HJk1Bx29YnpEB5/VT10IITydS+GulJqhlNqjlNqrlHrEuay9UmqlUirNeRtxjt1cHBWnHbeB9R9+9YFcukcH8+cb++Nn8OGa/uc/K5EQQrR2TQ53pdQA4NfACGAwcINSKhF4HFiltU4EVjkft7wKZ829nhEgy8xWthwp5MreMVzeK5pds64msZ4p6YQQwlO5UnPvC2zWWpu01lbge+BmYBIw17nOXGCySyVsKlOhY+x249lXiS7ankmVzc5E5/RmAcbGu0oKIYSncSXc9wBjlVKRSqkgYCLQBeigtT4J4LyttzFbKTVdKZWqlErNy8tzoRgNqCg6q9Z+8FQpWUUVzNmQweAu7Rja1T0tRkIIcbE1ubeM1nq/UupvwEqgDNgJWC9g+9nAbHBMs9fUcjSoorBOe7vVZmfKvzdistiw2TX/mprU7IcUQojWwqUfVLXW/9FaD9VajwUKgTQgRykVC+C8zXW9mE1gqhvuR/LLKTVb6dUhlOHxEUwc2PIzzgshREtxqZ+7UipGa52rlOoK3AKMAhKA+4CXnbdLXC5lU1QUQszP477szS4G4F9Tk2rmNxVCCG/l6kVMXyilIgEL8JDW+rRS6mVgvlLqAeA4cJurhWySitN1+rjvySrB39eH7lHBbimOEEK0JJfCXWs9pp5lBcB4V/brMq2d4f5zs8yerGL6xobha/De67aEEKKadyaduQTs1preMna7Zl92Cf07hZ1jQyGE8A7eGe5nDD1w4rSJUrOVAZ3D3VgoIYRoOd4Z7tVDDwS150ShiWeW7AVgoIS7EKKN8M5RIauHHgiMYOYXu9hxooinr+8rNXchRJvhnTV3k6PmrgMj2JtdwuQhnWUoXyFEm+Kd4e6suRfYQyiusNAzOsTNBRJCiJblpeHuqLkfLnW0OvWMkXAXQrQt3hnupkLwD+dwfiUAPSTchRBtjHeGe0UhBEVwOLeMID8DncID3F0iIYRoUV4a7o6hB9LzyugRHYJSyt0lEkKIFuWd4e4cETI9t0za24UQbZJ3hntFIVb/CLKLK+kRLQOFCSHaHu8Md9NpipVjWN+EKKm5CyHaHu8Ld5sVzMWUOMO9Y7i/mwskhBAtz/vCvbIIgNPa0RwTHSI9ZYQQbY/3hbtzRMh8myPcY8Kk5i6EaHu8L9ydQw+csgQRGuBLgNHg5gIJIUTL88Jwdww9kGUOJCZUau1CiLbJ+8Ld2SxzosKfmFBpbxdCtE3eF+7OZpmjJn+ipeYuhGijvC/cTYVoZeBomY80ywgh2izvC/eK0+jACCotWnrKCCHaLC8M90Ksfu0ApM1dCNFmeV+4mwqpNDrmSpVmGSFEW+V94V5RRLkhDEB+UBVCtFleGO6FlDrHlZFmGSFEW+V94W4qpFCH4OfrQ1igr7tLI4QQbuFd4V5lAmsFJ6uCiIsIlBmYhBBtlneFu6kAgIyKQBIiZZIOIUTb5WXhng9AWpk/8VES7kKItsulcFdK/UEptVcptUcplaKUClBKtVdKrVRKpTlvI5qrsOfkrLmfsoaQIOEuhGjDmhzuSqnOwO+BYVrrAYABmAo8DqzSWicCq5yPW0a5I9xPEyrhLoRo01xtlvEFApVSvkAQkA1MAuY6n58LTHbxGOfPWXMv0KHSLCOEaNOaHO5a6yzgH8Bx4CRQrLVeAXTQWp90rnMSiGmOgp4XUz52DFT5hhAbJn3chRBtlyvNMhE4aukJQCcgWCl19wVsP10plaqUSs3Ly2tqMeoyFVBqCKNbZCg+PtINUgjRdrnSLDMBOKq1ztNaW4BFwKVAjlIqFsB5m1vfxlrr2VrrYVrrYdHR0S4Uo5byfAp1KPFRQc2zPyGE8FCuhPtxYKRSKkg5rhYaD+wHvgTuc65zH7DEtSKeP20qIMcaQrz0cRdCtHFNvj5fa71FKbUQ2A5YgZ+A2UAIMF8p9QCOD4DbmqOg58Nelk+BjqBjuLS3CyHaNpcGX9FaPws8e8ZiM45afIvTpnwKdTcZMEwI0eZ5zxWqdhuGyiIKCZWhfoUQbZ73hHvFaRSaQh0mk3QIIdo87wl35wVMhTpU5k4VQrR53hPu5Y5BwyqM7Qjyk3HchRBtm/eEu7PmTlCUe8shhBCtgPeEe0UhAH6hkW4uiBBCuJ/3hHtVOQAhYe3cWw4hhGgFvCjcTQCESbgLIYRrFzG1JlWVpWjtS1S4DD0ghBBeE+4VZSVo/KWPuxBC4EXhbjaVYsVfhh4QQgi8KNwtlWVUan+5gEkIIfCqH1TLKSeAdoFGd5dECCHczmvC3cdiogJ//I0GdxdFCCHczmvC3WCrwKT98ff1mpckhBBN5jVJaLCaKCdAwl0IIfCicPe1VWBWAThm/BNCiLbNa8LdaKugyke6QQohBHhTuNsrqfIJdHcxhBCiVfCOcLdZMeoqCXchhHDyjnC3OEaEtBok3IUQArwl3J0jQkq4CyGEg5eEu6PmbvMNcnNBhBCidfCOcHc2y9gl3IUQAvCWcHc2y9h8pVlGCCHAa8LdWXM3ykQdQggB3hLuzmYZjNIsI4QQ4C3h7myWQWruQggBeE24lwGg/KXmLoQQ4C3hbnHW3P2k5i6EEOAl4a6dP6j6SLgLIQTgQrgrpXorpXbU+itRSj2ilGqvlFqplEpz3kY0Z4HrYzeXU6H9CPDzmilhhRDCJU0Od631Qa11ktY6CUgGTMBi4HFgldY6EVjlfHxR2c1lzok6ZIo9IYSA5muWGQ+ka62PAZOAuc7lc4HJzXSMBtnM5VRof/yNXtHKJIQQLmuuNJwKpDjvd9BanwRw3sbUt4FSarpSKlUplZqXl+fSwXWVCRMyf6oQQlRzOQ2VUn7ATcCCC9lOaz1baz1Maz0sOjrapTJocxkV+BNglGYZIYSA5qm5Xwds11rnOB/nKKViAZy3uc1wjMZZTJRrmRxbCCGqNUca3sHPTTIAXwL3Oe/fByxphmM0zmKiAj/5QVUIIZxcCnelVBBwFbCo1uKXgauUUmnO51525RjnVQ5rJZX4yQ+qQgjh5FLHcK21CYg8Y1kBjt4zLcdqxoxRau5CCOHkFVVdH5sZszZKm7sQQjh5RRoqmxkzfgRIs4wQQgBeEu4GZ7hLs4wQQjh4frhrjcHubHOXmrsQQgDeEO5WM4CzzV1q7kIIAV4R7pUAzt4ynv9yhBCiOXh+Gjpr7pX4SbgLIYST56ehs+Zu8/FDKeXmwgghROvgNeFuN/i7uSBCCNF6eE24awl3IYSo4QXh7mhzt/tKuAshRDUvCPfqmnugmwsihBCth+eHu8UR7kjNXQghanh+uFurwz3AveUQQohWxAvC3dHmrqTmLoQQNbwg3J01d6PU3IUQoprXhLuSZhkhhKjhNeEube5CCPEzrwl3Jc0yQghRwwvC3YwdhcHXz90lEUKIVsMLwr0SM34YZSx3IYSo4QXh7pgc22jw/JcihBDNxfMT0VKBGSN+Mpa7EELU8PhE1FYzlRjxM8hY7kIIUc3zw91SKc0yQghxBo9PRG2tpBI/jNIsI4QQNTw+EbWl0tHmLjV3IYSo4fGJaLdUOJplpOYuhBA1PD8RrWbM+MkPqkIIUYsXhHuldIUUQogzeH4iWs2OH1SlzV0IIWq4lIhKqXZKqYVKqQNKqf1KqVFKqfZKqZVKqTTnbURzFbbeMlilK6QQQpzJ1UT8F7Bca90HGAzsBx4HVmmtE4FVzscXjZJmGSGEOEuTE1EpFQaMBf4DoLWu0loXAZOAuc7V5gKTXSviOcphM0tXSCGEOIMridgdyAPmKKV+Ukp9oJQKBjporU8COG9j6ttYKTVdKZWqlErNy8trWgm0xscZ7tIsI4QQP3MlEX2BocC/tdZDgHIuoAlGaz1baz1Maz0sOjq6aSWwWVBoKrWfNMsIIUQtriRiJpCptd7ifLwQR9jnKKViAZy3ua4VsRHOWZgcNXfp5y6EENWaHO5a61PACaVUb+ei8cA+4EvgPuey+4AlLpWwMTXh7idt7kIIUYuvi9s/DHymlPIDjgD34/jAmK+UegA4Dtzm4jEaVqfmLuEuhBDVXAp3rfUOYFg9T413Zb/nzWoGwKylK6QQQtTm2YlYq1lGau5CCPEzz07E6pq79HMXQog6PDsROyfz/thNrLcPlGYZIYSoxdUfVN1LKSq0ERsG6QopRCtlsVjIzMyksrLS3UXxWAEBAcTFxWE0Gs97G88Od8Bis6MUGHwk3IVojTIzMwkNDSU+Ph6l5P/phdJaU1BQQGZmJgkJCee9nce3ZVTZ7PgZfORNI0QrVVlZSWRkpPwfbSKlFJGRkRf8zcfjw91i1fJjqhCtnAS7a5py/jw+FatsNpk/VQghzuDxqSg1dyGEOJvHp6LFZsfoK1/5hBDNIyQk5JzrvPbaawQEBFBcXExBQQFJSUkkJSXRsWNHOnfuTFJSEgaDgX79+pGUlET79u1JSEggKSmJCRMmtMCr8ILeMmabXa5OFcJDPPfVXvZllzTrPvt1CuPZG/s36z7PJSUlheHDh7N48WKmTZvGjh07AJg1axYhISE8+uijddafNm0aN9xwA1OmTGmxMnp8KlqsdmmWEUI0aObMmbzzzjs1j2fNmsVzzz3H+PHjGTp0KAMHDmTJkvMfvDY9PZ2ysjJeeOEFUlJSLkaRm4XH19wtNrtcnSqEh2jpGjbA1KlTeeSRR3jwwQcBmD9/PsuXL+cPf/gDYWFh5OfnM3LkSG666abz6pWSkpLCHXfcwZgxYzh48CC5ubnExNQ74ZxbeXwqVkmzjBCiEUOGDCE3N5fs7Gx27txJREQEsbGxPPnkkwwaNIgJEyaQlZVFTk7Oee1v3rx5TJ06FR8fH2655RYWLFhwkV9B03h+zd2qZegBIUSjpkyZwsKFCzl16hRTp07ls88+Iy8vj23btmE0GomPjz+vi4R27dpFWloaV111FQBVVVV0796dhx566GK/hAvm8VXeKpsdP1+Du4shhGjFpk6dyrx581i4cCFTpkyhuLiYmJgYjEYja9as4dixY+e1n5SUFGbNmkVGRgYZGRlkZ2eTlZV13tu3JM8Pd6sdP6m5CyEa0b9/f0pLS+ncuTOxsbHcddddpKamMmzYMD777DP69OlzXvuZN28eN998c51lN998M/PmzbsYxXaJ0lq7uwwMGzZMp6amNmnbq/75PT1jQvj33cnNXCohRHPYv38/ffv2dXcxPF5951EptU1rXd9seJ5fc5feMkIIcTbP/0HVpqW3jBCiWe3evZt77rmnzjJ/f3+2bNniphJdOI8Pd7NVukIKIZrXwIEDa6469VQen4oWmx1/aZYRQog6PD4VLTa79HMXQogzeHy4V0mzjBBCnMWjU9Fu11jt8oOqEEKcyaNT0WK3A0hXSCFEg4qKiuqMCnm+Jk6cSFFRUZOOabVaiYqK4oknngDgxRdfrBnz3WAw1NxXSpGUlES/fv0IDAysWb5w4cImHbc2j+4tU2V1hrvU3IXwDN88Dqd2N+8+Ow6E615u8OnqcK8eFbKazWbDYGh46JJly5Y1uUgrVqygd+/ezJ8/n5deeomnnnqKp556CnBMBnJmT5yMjAxuuOGGZu2h49GpaLE5rq6VH1SFEA15/PHHSU9PJykpieHDhzNu3DjuvPNOBg4cCMDkyZNJTk6mf//+zJ49u2a7+Ph48vPzycjIoG/fvvz617+mf//+XH311VRUVDR6zJSUFGbMmEHXrl3ZvHnzRX19DfHomrvFVt0sIwOHCeERGqlhXywvv/wye/bsYceOHaxdu5brr7+ePXv2kJCQAMCHH35I+/btqaioYPjw4dx6661ERkbW2UdaWhopKSm8//773H777XzxxRfcfffd9R6voqKCVatW8d5771FUVERKSgqjRo266K/zTB5dc69ulpGauxDifI0YMaIm2AHeeOMNBg8ezMiRIzlx4gRpaWlnbVM9/ylAcnIyGRkZDe5/6dKljBs3jqCgIG699VYWL16MzWZr7pdxTh5dc6+yyQ+qQogLExwcXHN/7dq1fPfdd2zatImgoCCuuOKKesd19/f3r7lvMBgabZZJSUlhw4YNxMfHA1BQUMCaNWtabGLsai6Fu1IqAygFbIBVaz1MKdUe+C8QD2QAt2utT7tWzPrVNMvID6pCiAaEhoZSWlpa73PFxcVEREQQFBTEgQMHXG4fLykp4YcffuDEiRM1Hwhz5swhJSWlxcO9OVJxnNY6qdawk48Dq7TWicAq5+OLwmKt/kFVwl0IUb/IyEhGjx7NgAED+NOf/lTnuWuvvRar1cqgQYN45plnGDlypEvHWrRoEVdeeWWdmv6kSZP48ssvMZvNLu37Qrk0nruz5j5Ma51fa9lB4Aqt9UmlVCywVmvdu7H9NHU896P55fzj24P89ooeDOgcfsHbCyEuPhnPvXlc6Hjurra5a2CFUkoD72mtZwMdtNYnAZwBX++04Eqp6cB0gK5duzbp4AlRwbx919AmbSuEEN7M1XAfrbXOdgb4SqXUgfPd0PlBMBscNXcXyyGEEC3qoYceYsOGDXWWzZgxg/vvv99NJarLpXDXWmc7b3OVUouBEUCOUiq2VrNMbjOUUwjhwbTWKOVdXZbffvvtFjtWU5rPm/xLpFIqWCkVWn0fuBrYA3wJ3Odc7T5gSVOPIYTwfAEBARQUFDQpoIQj2AsKCggICLig7VypuXcAFjs/jX2Bz7XWy5VSW4H5SqkHgOPAbS4cQwjh4eLi4sjMzCQvL8/dRfFYAQEBxMXFXdA2TQ53rfURYHA9ywuA8U3drxDCuxiNxjpXhIqWIR3EhRDCC0m4CyGEF5JwF0IIL+TSFarNVgil8oBjLuwiCsg/51ptj5yX+sl5aZicm/q11vPSTWsdXd8TrSLcXaWUSm3oEty2TM5L/eS8NEzOTf088bxIs4wQQnghCXchhPBC3hLus8+9Spsk56V+cl4aJuemfh53XryizV0IIURd3lJzF0IIUYuEuxBCeCGPDnel1LVKqYNKqcNKqYs2nZ8nUEplKKV2K6V2KKVSncvaK6VWKqXSnLcR7i5nS1BKfaiUylVK7am1rMFzoZR6wvkeOqiUusY9pb74Gjgvs5RSWc73zQ6l1MRaz7WJ8wKglOqilFqjlNqvlNqrlJrhXO657xuttUf+AQYgHegO+AE7gX7uLpcbz0cGEHXGsr8DjzvvPw78zd3lbKFzMRYYCuw517kA+jnfO/5AgvM9ZXD3a2jB8zILeLSeddvMeXG+3lhgqPN+KHDIeQ489n3jyTX3EcBhrfURrXUVMA+Y5OYytTaTgLnO+3OBye4rSsvRWq8DCs9Y3NC5mATM01qbtdZHgcM43ltep4Hz0pA2c17AMSWo1nq7834psB/ojAe/bzw53DsDJ2o9znQua6uq57Pd5pyfFs6Yzxaodz7bNqKhcyHvI/idUmqXs9mmutmhzZ4XpVQ8MATYgge/bzw53Oubs6st9+scrbUeClwHPKSUGuvuAnmItv4++jfQA0gCTgKvOpe3yfOilAoBvgAe0VqXNLZqPcta1fnx5HDPBLrUehwHZLupLG6na81nC9SZzxZA5rNt8Fy06feR1jpHa23TWtuB9/m5aaHNnRellBFHsH+mtV7kXOyx7xtPDvetQKJSKkEp5QdMxTF/a5sj89mel4bOxZfAVKWUv1IqAUgEfnRD+dyiOricbsbxvoE2dl6UY77Q/wD7tdb/rPWUx75vXJlD1a201lal1O+Ab3H0nPlQa73XzcVyF5nPthalVApwBRCllMoEngVepp5zobXeq5SaD+wDrMBDWmubWwp+kTVwXq5QSiXhaFLIAP4P2tZ5cRoN3APsVkrtcC57Eg9+38jwA0II4YU8uVlGCCFEAyTchRDCC0m4CyGEF5JwF0IILyThLoQQXkjCXQghvJCEuxBCeKH/D85hkA1eDdM3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "states[['val_ATT','train_ATT']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "states[['val_VAL','train_VAL']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "states[['val_VAL_1','train_VAL_1']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABHSElEQVR4nO3deVxU5f7A8c/DgCCIioCKooKKu+CCuGtmmm3ulbZv2mbb/XVv3pZr3fb13jbzWplZpplmalmWqbll4q64oqAiCgjKvs3M8/vjAQQEHRXEge/79eLFzDlnznnmMHzPM9/zLEprjRBCCOfnUtUFEEIIUTEkoAshRDUhAV0IIaoJCehCCFFNSEAXQohqwrWqDuzn56eDgoKq6vBCCOGUNm/efFJr7V/WuioL6EFBQWzatKmqDi+EEE5JKXW4vHWSchFCiGpCAroQQlQTEtCFEKKakIAuhBDVhAR0IYSoJiSgCyFENSEBXQghqgmHArpSaphSap9SKlopNbmM9T5KqYVKqR1KqY1KqU4VX1QhhLhAOWmQuAeqaphwax5Efg55WZflcOcN6EopC/AxcB3QARivlOpQarNngW1a61DgLuD9ii6oEOIKkhoHyQfBbqvqkpzb8ikwtRe8Hwq/Pm/KfTmtex9++hvsmHtZDudIDT0CiNZaH9Ja5wFzgRGltukA/A6gtd4LBCmlGlVoSYUQVS/7FCx4AP7bGT7sBh90gayUqi5V+Y7vgAatwL8dbJgGix+79H0e+QsWTYL87DPL8jJh70/mIlcoJQbWvGMeH1x56cd1gCMBvSlwtNjzuIJlxW0HRgMopSKAFkBg6R0ppSYqpTYppTYlJSVdXImFEFXDZoXv7oGoH6DPY3DDu6bGu/qdqi5Z2bSGpH3QejDc/h10uQ3it54//bLiFTjwG+TnmAtA0n4TqL+43jz+8UnY+hX8+oLZ/uQBeLc9zL2t5AVjxcugLND6Goj547J8m3FkLBdVxrLSZ+QN4H2l1DZgJ7AVsJ71Iq2nA9MBwsPDZe47IS6XnfMhZjVc/w641rrw18dthtVvwaFVMOJj6HqHWX58O2ycDsc2Q14GtLkWfIKg1dVQr1idzmaFI39Ciz7gYqmId1RSTirsXQodR4Gbh1mWfhzy0sGvjXneqBNs+RLS4qFe6TppgVOxsPpt8A2Bfk/ClllgcQfPBnB4HcwYar6lBHSByE+hzTCIWQX5WdBpLER9D5knTY09aiH0ngQBYRC93FxMAsMr/r0X40hAjwOaFXseCMQX30BrnQbcC6CUUkBMwY8QoqplnzJ53JxUsOXDoGfBOwAs5fz7n4yGzV9A//8zgSxxD3w+BGp5weApZ4I5wKDnTKohLxM86sGa9wANwQPg7iVmm+SDJk0TvwWuehaueqbi3pst33xj+O1fkB4Ph9eaCw5A0l7z27+d+d24oK1GQlT5AT3qh4IyHzhTA9/7E3j6gncTc5Hwbwf3/QKf9IHlL0LWSQgZCn2fgF3zYd/PkLgblAv0fAhc3c1+tn1jzmeDlhX3/ktxJKBHAiFKqWDgGDAOuK34Bkqp+kBWQY79AWB1QZAXQlSFvCwTUNw8YO1/TWuPsNtg+zfmp1YdaHkVDP/QBJlCSfvgy5sgIwGO/gV3/gAbPgGLGzy2BeqUGrXVuzH8/SAodea4q16HPz+C9ASo0xAWPggphyAwAta8C6E3X3pQy0oxF52Nn5lA3rCj+Vaw9Wto0dekV5L2mW0LA3qjjuZ3wi5oM7Ts/e7+ARp3NrX4rGRo1guObjDHGPIyNOwAPi3ArTYMfMa8N4DQW8zr6jU37/3UYeg4+syFI7AHbPrclPnRjeAXcmnvvxznzaFrra3AJGAZsAeYp7WOUko9pJR6qGCz9kCUUmovpjXME5VSWiHE+WltatTvtIEvbjAtLTrfDCOnwl2L4ab3IWwcHPgVvr0DrLnmdXYbzLvLvH7oq3BsC3x5I+z4FkJvPTuYF1LFsrK1PE0w1XbYs9ikaOIiYfALcMuX5sKw/MVLe2/LX4T3OsDv/wb/NnDbPHhoLQz/AFr0g5+fgbTjJqDX9gEvP/Naj3pQv7kJ6GVJiTFpkc63mJq1px+M+dTkwQHaXgch15wJxp3GmtSMe12TelEK2t1gvhn4hcCQf5/Z9+3fwR0LzOPtcy7+/Z+HQ+Oha62XAktLLZtW7PGfQOVccoQQF+bwOhO0moabmvaAp6HP4ybgtBwIDDTbNe8NC+6HP96Ewf+CXd+bYHTzTJOLbhAM308Ea44JcI5q2B7825v8s4vFpHe63GG+LYTfB39+DOknTO0+fhsc2QCdxpgLRm6GyWM36ljyQlFo1euw9j/mAtXvqTO17kLDPzCpkJ/+ZnLZ/u1K7qdRJ5P33/ylSQs1CD6zbvci87vjSKgbaG78utU25ywt/uxatcUVbv3KfFsozNv3exLqBkCPCebiVqi2j7k52mowbP8WBj0PLhXfr7PKJrgQoto6scvUdG+bB36tL//xt8wC93omh108qJTWeay5cbf5S5Mv/+MNk7poX9Aqud0NMGElJO2BRqW7npxH5zGmtYhygRFTzwS8bnfD+g9MPrlRJ/jubnND8dfnwK8tpB6F3DQIHWe+SRS+Lu24qZnvmGty+MM/Kjvg+7aCq583bc4Lj1dco06wbykseRw6jDTfGgpFLYSm3U0tHsCltvk95nOw5ZX9Phu2L/ncu7HJpZcnbJy5iMauKbi4ViwJ6EJUtOVTIOUg7PsJ/C5z9jErxdQ0u95x7mBeKPw+2PujyZsnR8O4b0rWHP3bmJ8L1fsxk39u3Blq1z+z3K+1SYusftsE8sahcN1bJv2TEAUBoSZFsv5DyE2HcbNNDf7bO8zzfn8zN3XLCuZFx54Evq3NvYP2w0uua38jHFxhblQe+NXk/Gt5mnTL8W0w9JWz91f8HsOlaneDSdHsWSIBXdQAeVmQcaJSWwJUqti1pokaQOy6c9fWKsOq101OPPw+x7ZvOcg0Mzy2GbrfYwJORXDzgOD+Za/r/Sh8PwH6Pw39/2Zaz7ToXXIb7wBY9izMGg6H10P9FnDvUvBve/5jK2Xy3W2vO3tdQBhM+B0O/WH2vfdHc/Pz6EazvkPpPpMVzK023P+ryb1XAgno4sry63MmBTD8g5LN4y5W0v6CPG5j+GuaSScUpkHST4CLG3j5XvpxCq39L9RpDK0GmeZuNmv5zQMvhjXP3HAsTEUUF7cZNn4KERPPzi2Xx8UFrnnJtO4Y9mbFlfNc2l0P/4w7dy271yOmlc2eJSZtcs0Uk4euKC36mqaIPzwM9oIuM817n0m3VKbSaZoKJAFdXDnsdvMPrFxg0aPg4mpyjhdLa5g9BlKPmRpfWhzErIE7F8KGqfD7y+BR17T+SIiCkGtN4F9wvwmanW82LTbO5atR4NXQtH3OOW2+zvd5zKQats+BEzugabeLfw+l3883N5sbiYOeg4gJZ4KizQpLnjDlv/r5C9tvx5Hm53I6VzAvXD9mBmQmmZuMFc3ialqpbP7C5MgDwyv2glFFJKCLK8exzeYfeMRUEwyXPGlyrOXdkMtKgb/+Bz0fNPnVbbNN07t6gRAyxDw+fcT06svLNP/Au+abtsM7vjWdQRKi4OsxZn/7l5mejtHLTZBOjj53QE87bgI4mItQ026gbeZmo2dBU7nD6y49oNuskJ1i9nVolckP//x3yM80vSB3zjd53oSdcMtX5iJVHVhcKyeYFxr6smkpU5nHuMwkoIsrx76lps1vu+tNE6//9Tc19Qkryq7RrfuvaWN9eJ0J3KcPm8Cq7VC3qanFAoz4yNSYc9LMGB07vjVtjUdPNz3/dn1vevut/Y9poRLU37Qr/vU50znGu5xx5mLXmN+dxpjOOju/M83kGnUy5fUJNmkDLnFAqN9eMN8oLLXMvif+Ad8/YNpha13wnm3Q5jpof9OlHasmcXWvVsEcZIILUVzKIdNFO/r3s9cVdj6pLFqbnHOLPuarr3cjuGqy6S5+eN3Z2+emw6aZ5oZe7BrISIQHVsALyTDqf5B2zIx0517P9O4DU3O95l/Q9gaTo1cK6jaBPpNgwN/NcXNTTZvrZhHmNXGR5Zc5ZrXprDL6U7j2dbDnQ9j4Mxcf/3YlR98rT+xa02a6LKdiTV68RV9zobnpA1NzHf6hCe7tb4SnD8Doz0zq6HypDFGtSQ1dmNTEX/8zNT5rthlBr/Vgs+70EfjlnyYdcf3bJkURtxHu/dm0Tqgomz6Hk/tMx4xCYeNNW+b1H0JQvzPLs0/BH2+b4HvnQkiMMq1iArub9R1HwS+TzQUq5NqSg0H1eMD8lFbLy4wzsmexaR1ht5oacdxGEzTLErvGNMFzsUDvR0wLkXrFhj3ybQWHVpp7A+V1Ijm2GWbeYIL13UtKBuSsFPjlWbP/MZ+XrE26e8ODq89sH3pz2fsXNYoE9Jou+7Rp4xu7xgQ/nxZm9LzTR00u+ptxZ3ru/fjkmdet+8DUSDOTYNgbZQf39BPmQhB6i2muVZ6jG81ASK2uNkG8kFtt02Jj1etmiFK/EPP706tN55O215sgXhjIC7m6mzRI5GdnN4c7l54TzQ+YINo41Ix9vfFTU2MPCDuz7emj5rwU70Hp06Lk/nxbmV6WacegfjPOYrebburKYs5/YQ5/+1xzD+HIBrDlml6FZaUGpDYuSpGAXlPFbzUDGcWsNp0qhn9kmgmeijEBPep7E8ASo0wLjs63mJ6EjTubHnV/vFFyX3f/WLIDSeRnpnZpyzV55JFTyy7Hb/8yeXDvgLJ7/4XfZzqhbPrCdPpY/LjJGd+//NxDkXa/1+TGQ6696FNEswiTuz66waRPHl5/pra/90fzu+Wg8l/foJX5nXKw7IC+7yeT0rnxv+ZbyKJHTTvrw+tMr8ke95ubso07X/x7EDWKBPSaauk/zJgWfiFw+zxTOwaTumjaHTbPNO2pPf1M6xDXWma8DzA119i1piNKYATMudXUYgf+3eTCT+w0Nc/gAaYDxcb/mVYYjTqZi4hfG9O55FSsCeZdbofr3jRphNLqNDQ3+rbNNt8Cjqw3F5hmPc79/hp3gmcucQTnVoNNQG97gwm+UQtNCxatTff6pt2hYbvyX+9bENCTD5qRDUvbPtec4253QZMusPJ1M1Tt4CnQ98lKGetDVG8S0GuipH0mNzzkZej7+NnrBz4DCyaYHPSAf5zdicW3FTwdfSbghFwLf30CmYkmYFvcCkaq+9zcNMxMNDVQMD3+jv4Fm2aYIVx9Q+CG98ruKFMo/D4TTFe/ZS4uXW6vmPNwPiHXwDOHTVftaX1h5WsmzZMQZca7vuk8U+d6NwFXD3MeS8tNNy1uwu81tf4mXc2FVYhLIAG9Oss8acansLib1iJ5WaYnXPwWk7cNvbXs17W5Fp7cYQbq7zC87G2K1x77PQVfDDOpmrY3mPz2gKfPjIFx80yTLjkVa3rjZZ402+5ZYlprnCuYg7lh2O5GcyEZ/OLlzR0XppGufc10Ivp+gim/m5fJ05+Li4tJu5TV0mXfzyYd1XFUhRdZ1FwS0J1dbrrpDl66+3riHpg10oyLUpa215ffvhpMIOsyvvz1xbXobVqO1G1iBk8qK+DWCzwzJZl3IzM+9uAXHNu/UmaQpqrUahBc86IZeKtWnfJTRKX5toTEvWcvj/rBtJUPjKjokooaTAL6lcRuN0OV+oY4Nu+jzQpfXGeatz36l7nB6N3E5MNnjzWpj/Hfmt+NQ0076+PbzQ3PMAeDtaNueLdi93cl6vuEuefQtLvpYu+IBq1g3y9nj+kSF2l6s0qeXFQghwK6UmoY8D5gAT7TWr9Ran094GugecE+39Faf1HBZXVeyQfNJAJD/n3uQPDz303rENfapoVF57HmhllpuelmhLjo5eYGJMDn15oWKQArXzWtKu5adPaohWU18xOOKZyR5kI06Wqadx5aaQI4mE5QmYnmJrEQFei8AV0pZQE+BoZgJoyOVEot1lrvLrbZo8BurfVNSil/YJ9SanbBHKM1W+ZJM1bIqRio08iMH1HIZjVdz11cTauJyM9MzdmjHhxcCYsfA4/6Z/LYWsP8+8x419pmlgX1N03+ds4zrUGCB5ou9MM/Kn8iXHH5tL3e/H02TD0T0BMKLryOjogohIMcqaFHANFa60MASqm5wAigeEDXgLdSSgF1gBTAWsFldS52u6kpR35qus036WqC9lX/NAPqJ+6F7+4xKZZCba4zTfJcLGY288+HmqAeu9YMhq/tJl3S/R7TNvnELjOyn3tdk5/u85i5ERkxoaretSjNtZZpT77iFfM3b9hOArqoNI4E9KbA0WLP44Cepbb5CFgMxAPewK1aa3vpHSmlJgITAZo3vwzjDlelPYvMWCLtbjTTe+Vlmgl3f33OBOats01N/NrXzfYt+piOPIU3FC1uMOYzmDPeNAXcON20yfYNgevfPXuM7WumXN73JxzX/T7Txnznd+ZGcEKUaX9eOHmxEBXEkYBeVhsxXer5tcA24GqgFfCbUmqN1jqtxIu0ng5MBwgPDy+9D+e14RMzGe81L5rntnwz1rZ/e7ilYKJcrc0wrptmmEkVut0JAyefu6WJbyuYtBHys+HrsXB4rWk+V5ETJojK5+VrJjU4vs08T9gltXNRKRyJDHFA8X7LgZiaeHH3Am9orTUQrZSKAdoBGyuklFUpP8e0HgkIM+2ro383g1QVzhautRnXJOMEhN9vbkbu+NZ09x73zZmu4krBA8shJ9WMUXIhA1u51YbbvjVdwlsPqfj3KCpfQJgZq8VmhaS9lTKfpBCOtJmKBEKUUsFKqVrAOEx6pbgjwGAApVQjoC1QRvc4J7Rvqem6vugRM4LfoVXw1UgzuQGYf870eJNG2TLLBPgN00wLhrbXl9yXxc18zb6YUQrd65gOP9LMzTkFdDFjrh9cYWaQlxYuohKct4autbYqpSYByzDNFmdoraOUUg8VrJ8GvAzMVErtxKRontFalzPAs5PZvcjMXnPLl2Y4VYAvh5teg/cuPTN2eONQE9Bb9DEzx9z0voyGJ84oHKnx95fM4GJlje0ixCVyKBmrtV4KLC21bFqxx/HA0Iot2hUgLwsO/GqaErboA4DVZmdpx3e5ccfjuHw1ElzcyKrbiq/d72LiiafRX49GedQz81EKUahxJxPIE3aZtJmjHZOEuADy/b2UnHwbaw8UfLmIXg75WdBhBABZeVYe+nozj2+oy4TcJ8hPPADHNrEgtS0fHm7O7Xn/JNG7g+n+XgGTP+RZ7Xy04gDJGZU8W5CofLW8zCiT4PiQCkJcIAnopXy94TB3fP4X66NPwq752D39eGy9J9P+OMi46RtYsTeRF27sgA65lmuyXmGZ6yBm5Q9m4SN90cFXMTznJRbVuZnH52wlKb3sQJyYlsOQ9/7g8TlbOZ6aXW5ZFm6N451f9zPrz8OV9G7FZdU03DRVLX1vRYgKIgG9lJV7E/Ajla9XboN9P7PLZwhLdiXyxs97OZCQwfQ7w7m/XzDT7+xOv4gIHsyYQN9evWndsA4TB7QkIS2XJ+ZuY/H2eEZ/so79Cekl9m+12XlszlaOpGSxLOoE46ZvwDQOKslu10xfbe4r/7Kr7AG2cvJtFf7+RSUa+rKZ9/RcszcJcQkkoAMxJzMZNXUd66NP0vXIF2z0eIRRR14DWx5vJXSjX2s/lj7enx8f78c1HUy7cVeLC6+M7MSCh3vz7PXtARjYxp9Bbf0Z2z2QeQ/2JjvPxvCP1jIv8ky/rE/XxPBXTAqvj+7MKyM7cTg5i6j4tLPK9EvUCQ4mZdIjyId9CelEJ2aUWH8gIZ3wV5Yzc90lTuLgJFKz8pn2x0FeW7qnzAugU/BsAH6tq7oUohqrsT1U8m123CzmevbqT7vZeuQ0L3+1lIUu34NyZYhlM3vtzVib0YRZA1rSoUnds/ahlKJ7iwYlnn9x75nhUJc+0Z+/fbudfyzYwcGTGdzdO4iPVhzgmvaNGN0tkOSMXJSC3/ckUq+2G9NXH+KvmGRCGnqzLOoErfy9eO+WLvR/ayUf/H6A3q18GdW1Ke6uLvz7x91k5Fp585d95Nns/LTjOP1D/DmRlsORlCy+uKcHXu7V48+bkWvluvdXE5+aA5gLZ9/W0suyIuVabSzZfpxhnRpTp5p8bq5UOfk2PNws59/wItS4GvrvexIY+fE6wl76ld92J7BkezzL9yQSEeTDZPun2HHB9sDvZDfrz/aWE7glvBn9Qy4ueDT09mDmvT24vWdz/vfHIQa+vZI8m53nbjA1et867nRtVp/F248xaup6vo08iq+XO+sOnuTaTo35/uG+NGvgSURwAxZvj+ef3+/krhkbeWvZPtYcOMmE/sEoBa8t3UtKVh4fr4rmpx3H2RiTwrQ/yphUAVPTfePnvew5fva3gqp0rlr3J6uiiU/N4Yt7euBXpxafr60e30riTmXx/A87+X5LXJWmz1Kz87l7xkae/m47ry3dc/4XXEYpmXnc8r8/eWlJFDviTpOZe+lDRFXlN7wTqTmM/HhdpX2Ga8yl2Gqz88KiKOZsPEKQryctfL2YMGsTAK0b1mF26HbcTuzgh4AnGNk0DLf7f+RWoJw5fRxWmJq5pkMjvv7zML1a+hLsd6YFzOD2jXh72T5qu1lY8lg/2jY+e9KET+8MJykjh53HUnlm/k42xqQwsI0//xjWjohgX+JOZXFX7yBOZuRSu5aF5xfuYvrqQ9Sr7UaHJnXp08pckKIT07l7RiTHTmez9cgpvn2wd5ll3hSbwoq9idwS3owgPy+01sSn5tC0/pncb2JaDr513LG4XHpb+5V7E5n8/Q4+v7sHnZrWK7HuaEoWn66JYXTXpgxq15Dbe7bg/d8PcDApg1b+dS752FXp28ijfL3hCF9vOMK8TUeZdV9ParmaOlZiWg7fbY7j/n7BlVab01qjlOKlJVFsPnyKiKAGzN14hDt7taB9wNnfSKvCqz/tYfPhU2w5fIov1sVS283C1w/0xMvdwsq9STw4oCUuF/AZXLg1jhcX7+aXJ/sTUO/y3ss4nJzJuOkbSM+xEtKwcj67qqquVuHh4XrTpk2X7XhLtsfz2JytTOgfzD+GtSPfZuc/v+0n2K8Oo5tl4vH5QHTLq1C3fXtZOwQdTMrghg/W8MboUEZ2Pf9wt4lpOaBM7b888aezGf7RWk5mmNGL7+jVnBdu7MCdn23kQGI613ZszNzIoyx6tC9hzeqXeG1KZh5D/7Oakxm5uCiY/UAvUjLzmDRnC789NYDWDb05mpLF4Hf/IKxZPf47rmuJQF8erTVbj54m7lQ2fVv54lvHvWjd7Z9tYF10Mi18PVk8qR/1arsVrXt8zlZ+3X2ClU9fRUC92iSl59L3zRXc3D2QV0d1Pu9xr2Sjpq5Da7gtojn/WLCDUV2b8vbYUCwuivtmRrJyXxL/HtGRu3oHVfixl+9OYMriKF4Z2YkJszZxV+8gHh/cmqveWUWTerX58r4I/L3d2XM8jfmb42jh68mYboGXLY2XkJbD91uO8eYve3l0UCvu7h3E5sOnePnH3Xi5u5KZayU+NYf3bgnD39udNQdO0raRNyO7Nj2rknEwKYPbP/2L23o2Z8a6GE5n5fPE4BCeGtLmsryXQvd8sZHNsaYiVVYK11FKqc1a6/Cy1tWYGvrPu47jV8edyde1x+KicLO48NwNHUxX/S9vAjcP1IiPLnvvzlb+ddj14rW4WhzLfjWse575N4Em9Wuz9pmrycy18r/Vh5i++hDropOJOZnJa6M6M7xLE37aeZz/rT7I1NvPTHZhs2smL9hBanYes+6L4PG5W5m36Sg5+Ta0hvUHk2nd0JvlexLIs9mJik9jzNT1/PBoXxrXK79cWmsmL9jJt5vMzeGm9WvzyR3d6NSkHsdOZ7P+YDJDOjRi5d5EBr2zikcHteb+fsHsiDvN4u3xTBrUuqg25e/tzsguTViwJY6nh7bFx8uBmZ2uQGk5+Ww/eppHB7Xmlh7NSEzP4Z1f95OanU+npvVYuS8Jz1oWPlsTw+09W1zyN6Fdx1LZEZfKrT2aoYA3ftnLsdPZ3PdlJC5KcV+/IOp71uI/t3bhka+3MPqTdSx8pC9TFkexMSYFgNX7k5h+Z/gF1YjPZ9+JdHYdS2VM98CiZTn5Nm76cC2J6bl0a16fx64OwcPNwnWdA7C4KCZ+tZlaFhda+nvx7x93k55jxa41WkNmnvWsC+Dna2M4kZbDe7/tx82iaB9Ql28jj5Kanc/Crcdo1qA2vYJ9advYm3q13RjSoRGqnDjw5i97iy4cF2LlvkRW7Uvi+RvaX1IwP58akUPPzrOxcm8Swzo1Mv8YuemwawEc/hOWvwixa2DwFDM8bRVwNJhfCA83C7513Hn2+va8P64Lx05l066xN7f2aEYdd1fu6RPE0p0niv5Zc/JtPPjVJn7dncAzw9oxoI0/Qzs0YvnuBP7YnwRAZOwpAFbsTaSVvxfzH+pDek4+938Zec4c8Icrovl201Em9A9m1n0R5NnsDP9oHR2nLCtKe025qQPfPtiLto28efnH3exPSOe1pXvw9arFgwNLzrp0X79gcvLtzFwfW+HnrVCu1UautfLy2n8dSsGuKbq5O+nqEP49oiOr9iXywe8H6Nq8Pm+NDeVIShbX/nc17V/4hbCXfmXB5riifXzw+wGW7jxe7jF2x6fxj/nbybPaeW3pHp5duJPx0zfw7m/7iE7M4J4+QSjgptAAAn08ARjUtiGzJ/TkRGoOE2ZtYmNMCs/f0J5/3diB5XsSmb6m4oZoys6z8cCsSP7vu+1sOJRctPy33Qkkpucy7Y7ufP9I3xIpp6EdG/P00DZ8ML4Lb48N5XRWPmGB9dgxZSjdW/gwbdVB8qxnRu5Ozcpn4ZZjjO0eyNND2/DqqM48eU0IJ9JymLk+lojgBtSr7casPw/z9/k7mPjVZjYcMv8TpzLzmLkuht0FrdCSM3KZ9sdBXloSRWaule1HT5/3M6K1Zua6GB6fs5VgP69K+bZVXI2oof+xP5HsfBvXdQooWPAWrP/gzAZtrjOTRlRTI7o0JTSwPnXcXYtqeg9f1Yrvtxzj2YU7Wfp4f6aujGb5nsQSX/Gv6xTAvE0mgDSq605kTArpOflsOJTMvX2D6dCkLu/eEsZDX29hWdQJ3CwuPP/DLgBeG9WJoR0a88pPe5ixLoZRXZvy7PXtUUrx0+P9+H1PItuPnmbRtniubtuQQB9PAn08mXp7N/q8sYJJ32xhf0IG/x7REW8PtxLvp13jugxq68/7vx9g9YEk3r05jJYVnE9/6KvNZObZ+HZir3Jra5diXfRJPNxc6Nq8ftGyu3oHcVNoEzJyrTSu54GLUvQMPkxOvo1xEc3YfvQ0zyzYQeN6HtSr7cZ7v+3Hw82FDgF1CfI7u2fyrD9jmbcpjl4tfdkYk0JEUAP2nkhjY2wKLf29eOHGDtzWsznNCoJ5oW7NfXhoYCs+XBGNt4cr4yKa41XLwubDp3h72T66NqtPz5ZmUvLsPBtR8am0D6hblI75eGU0Qb5e3BAacM5z8OGKAxxNyaa+pxsv/7ibxZP6YXFRfLc5jqb1azOkQ9lDS0+6OqTo8U+P9yPYzwvPWq48PjiEu2dsZP7mOG7raeZb+GztIbLzbdzbN4iOTcz9GavNTq+WDYgI9uWpa0JQSpGVZ+V4ag43fbiWJTvisWvNI7O3kJqdj4uCJwa3IdCnNlrDqax87pqxkc2HT3F/v2BeuLFDue9xyY7jvLhkN/1D/Hh5RKeieySVpUbk0P82bxsr9yYS+dw1uNrz4L32ZqLfHg+Af9szQ+HWMCv2JnDfzE2Mj2jOj9vj6Rfixyd3nEnB5FpthL+yHK3hyWtCeOWnPUy+rh1v/LyXuRN70aulL3a7ptfrv9OtuQ8n0nJISjdNMb1quXJP3yD++f1O7ukTxAs3digzbZCVZ8VFqRK1sCmLdvHln4cJ9vPi16cGFDUvLf26+Zvj+O/yA2iteXNMKNe0b3RB6YC/DiXjWcuVzoElb8SezMgl4tXl2DXMe7A3EcENytnD+a3Ym8Anqw7y0W3daFSQLjuYlMGoj9cRHtSAGff0cHhfaTn5jP1kPcdTc+jYpC4741JxUYqOTevyzQO9Srx3u13T8/XfSUrPpYFXLVIy8/juod6EBdZn8+FTBPrUplkDz3KPlZNv467PNzKsU2Pu62f+P9Jz8hn+0TpSMvPw8XQjM89GanY+eVY7Qb6evHdrFxLTcnjo6y10a16f7x/pW7S/46nZPDJ7Cw/0a8kNoQHmXsgbK7gxNICr2jXk8Tlbuat3C8Z0C2Tk1HU8Nqg1fxva9oLOtdaasdP+5FBSBj8/MYAZ62KYvvoQN4YG8NFt3Rzax2NztrL2QBJe7q7UcnXhrTGhfLEulp93Hadjk3okpOXQppE3a6NPUsfdFbvW/Dl5MPU83dhwKJn9Cel0alqPbs19yLfZuea9P6jtZmHp4/0rLFVV43PoG2NS6NXS16Q2ohZDdgr0fhRaDarqolWpq9s14s5eLfhqgxlaYNLVJTu9uLtaeGJwCHati1IDb/y8l6b1a9O9hQ8ALi6KYZ0aM2fjEfJtmudvaI+PZy3+77vtvLg4ih5BPky5qUO5tVzPWmd/BB/o35Klu07wwo3tywzmha+7q3cQA0L8uW9mJBO/2kz7gLrMmdCT+p4l8+pHkrPYeSyV6zs3LipHYnoO982MJNjfix8f6w+cac72a1QCdg0ebi5MX33wogN6anY+/5i/k5MZuTw2ZyvfPNDT5K1nRlLL1YWXhl/YJBd1PdyYcU8PRn68ng2HUnigXzCtGtbhn9/vZE7kEW7v2aJo213xqSSl59K0fm2OnTa14K7N6uNqcaF3K9/zHsvDzcK8h0q2gvL2cGPaHd15beke6ni4UtfDDW8PV1o3rMN/ftvP6KnrqVXw99p9PA2bXRddxL/ecJitR07z6DdbSM7syKnMfPJsdiZd3ZpgPy92HD3NZ2tjmPXnYbzdXbmlR7OzynQ+SineHNOZ6z9Yy+B3V5GZZ+OOXs158SbHz/NNoQEs2R7Pqax8vnmgJ+FBDWjpX4c/9iex81gq4yOa8fDA1iyLOkHPlg0Y/tE6vtoQS8+Wvtz+2V/Y7OYzdHP3QFyU4nByFp/dVbH3Hc6l2gf0hLQc4k5l80BEQ/j5Gdj1PTRoaSZTFjx/Y3uiEzNoUr920VfS4h7ob/LXNrumobc7dWu7MfPeHiUC7bBOjZn152FcXRSjujbF28ONt5ftIyE9h3/d2PGCUxbNGngS+dw1Dm0b5OfFL08OKGinv4Onv9te4sbd52tjePPnveTZ7Pzvzu5c29GMcvjusv1k5tmIik/jVGYePl61+HBFNHM3HqFubTeCfD0Z0aUp7/9+gMjYFHoElR/UrTY7x1NzzqrxvrNsHymZuTw0sBXT/jjIoHdXcSrTfIX/8r6Ic9aQyxPo48mMe8L54PdoJgxoSUNvd37cEc/rS/fSp5UfwX5eWG12ftudgIuCV0Z14t4vIrmqjX+F3Ktp29ibL++LOGv5sE6NmfPXEVbuS6Rbcx+mrjrIoaQMQhp5k2+zM29THAPa+OPu6sKUxVF41XLlqrb+Ramy52/sQNvG3pzMyOPWHs1ocJE3u1s39OaFGzvw3q/7eHVU5wu+eTmwrT8+nm70CGpAn4JKTAOvWkzo35L/LN/P1e0a0dzXkwkDzP/FwDb+vPPrfmq7WWjmU5sv7o1g/uajTF11EBelGN21KYPbX757c9U+5fLzzuM8PHsLG3r+SePtH0LIUDMaYouy22DXRIWfgfMF3pMZudRxdz2rXbTVZqfPGyvo3sKnKGWzcl8icaeyubNXi7J2VSlmrovhxSW76RncgKeGtKF5A08GvLWSvq39OHY6m3ybnYkDWrJ4Wzx/xaTQM7gBf8WkMPX2bvQL8aPP6yvIKOi48vBVrXj4qlbc9OFacvPtDOvUmIS0HN67pQv/9902th9N5c7eLbijVwue+nYby/ck8PDAVjw1pA1uFhcS03Po+8YKbglvxqujOrNo2zEWbYvH1UUxZXhHh5p6OupoShbXv7+GPJuddgF12Rl3GruG8BY+fPdQbz5aEc2Qjo1o1/jytC3fn5DO0P+s5j+3hjGqayA/7TjOo99sYcY94fRp5cf4Tzew9chpvri3B4PaVk6wK2xjfzFOpOZQr7YbtWud+ZznWm0si0rgxs4BJWrbqdn5zFofy+97E3ltVOeiFixxp7LwdnejnqfbWfu/VOdKuVT7gP7yj7v5ecN21tX+G6rNMLj5i0o/Zk10PDUbL3fzNbyqaK2ZuT6WT1cf4mRGHv1D/Fi1P4k//n4Vh5IyuWuGmRGxTaM6XNuxMRMGtKTP6ysY3qUJzRt48sbPe/ni3h6s2X+SBwe2pFFdD6LiUxk1dT1aa/JtmnaNvdl7Ip2QhnU4kJiBVy0LmXk2egT5EBl7ivERzXl9dGf+u3w//11+gJVPX1WiI1lliT+dzdvL9hFzMpNeLX3x9nDl6nYNq6SDkNVmp9OLy7gtogXtArz516JdBNSrzfK/DcTiojidlceGQ8lc27Fxpdxwru4uOaArpYYB72NmLPpMa/1GqfV/B24veOoKtAf8tdYp5e2zsgN6Rq614K78XiZmf8Hw7B9gUqSZeFlUaymZedzwwRqOp+YwPKwJH4zvCsAPW4/RpH5tegT5FAWSB76MZNvR01jtmo5N6jL7gV5n7e9gUgbeHq7MWn+Yj1ZG07ulL7Mf6MnmI6d48+e99AhuwDPDzM3iaX8c5InBIXyz8QidmtQtMbZPTTJq6jqOJGeRnJlH39a+/OeWLg71oRDnd0k3RZVSFuBjYAhmwuhIpdRirfXuwm201m8DbxdsfxPw1LmC+eXwwe8Hioaf7e6718w4JMG8RmjgVYuPb+/GvxbtKnGjt6x86oA2/izfk0jHJnXLvUlZOMTAU0Pa0NSndlFrmh5BDZj/cJ+i7Z4e2oYdcad5//cDANzfr2WZ+6sJOjWpx9YjpwkNrMeMe3rg7lo5wxeIkhy5KRoBRGutDwEopeYCI4Dd5Ww/HphTMcW7OHa7Zsn2eMJb+DCorR8Bf8ZAo/5VWSRxmXVr7lPUeuVcxkc0J6ShNxHBDc7bG9Piohgf0bzc9a4WF766vyeHkzPxcLPQpALz5M7m6vYNWXfwJB/f1k2C+WXkSEBvChwt9jwO6FnWhkopT2AYMKmc9ROBiQDNm5f/j3GpNh85xfHUHCZf144RzXNhdRY0LL/xv6i53Bxsxucoi4uq8E5OzmhQ24aVdsNTlM+RdkxlVVvKS7zfBKwrL92itZ6utQ7XWof7+/s7WsYL9uP2eDzcXLimfSNIKPgi0ejC2vwKIYSzcSSgxwHFW/kHAvHlbDuOKk63pOfks2h7PIPbNTJdkRN3Awr821VlsYQQotI5EtAjgRClVLBSqhYmaC8uvZFSqh4wEFhUsUW8MJ+tMcNjFg3olLALfILAXb4GCyGqt/Pm0LXWVqXUJGAZptniDK11lFLqoYL10wo2HQX8qrXOrLTSnkdyRi6frTnEdZ0aExpY3yxM2C3pFiFEjeBQ13+t9VJgaall00o9nwnMrKiCXYyZ62PJyrfxf0MLBq4/vgNSDkLHUVVZLCGEuCyqzXjoWXlWvtpwmCHtG9G6oTfEb4UvrgfvAOh6+/l3IIQQTq7aDM41L/Iop7PymVgwaA5r3gXXWnD/b1DvwgboEUIIZ1RtaugLthwjNLAe4UENICsF9v0CobdKMBdC1BjVIqAnpeey81gqQ9oXzHCyawHY8yFsfNUWTAghLqNqEdBXF8x5OahdQc+0Hd9Co04QEFqFpRJCiMurWgT0VfuT8KvjToeAupCXCcc2Q9vrqrpYQghxWTl9QLfZNWsOJDGwjb8ZeD5+K2g7BNbMYUuFEDWX0wf0mJOZnM7KPzPAUlzBGOtNu5f/IiGEqIacPqAfTckCINivYH7GuEgzZ6hXxY2gJ4QQzsDpA/qRgoDerIEnaG0CemCPKi6VEEJcfk4f0A8nZ1HbzYJ/HXdIjYOMBGha5uxMQghRrTl9QD+SkkXzBp5mjsjj283Cpt2qtlBCCFEFqkFAzzTpFoDkaPPbL6TqCiSEEFXEqQO61pojKVm08C0W0L0agke9qi2YEEJUAacO6EnpueTk22leVEM/CL6tqrZQQghRRZw6oBe2cGlevIYuAV0IUUM5FNCVUsOUUvuUUtFKqcnlbHOVUmqbUipKKfVHxRazbIeTCwJ6A0/ISYPMRPBtfTkOLYQQV5zzjoeulLIAHwNDMBNGRyqlFmutdxfbpj4wFRimtT6ilGpYSeUt4egpE9Cb1q8NSTvMQgnoQogaypEaegQQrbU+pLXOA+YCI0ptcxvwvdb6CIDWOrFii1m2U5l51PVwxcPNYvLnIAFdCFFjORLQmwJHiz2PK1hWXBvARym1Sim1WSl1V1k7UkpNVEptUkptSkpKurgSF5OWY6VubTfzJDkaUOATfMn7FUIIZ+RIQFdlLNOlnrsC3YEbgGuBF5RSbc56kdbTtdbhWutwf3//Cy5saanZ+dQrCugHoV4zcPO45P0KIYQzcmRO0TigWbHngUB8Gduc1FpnAplKqdVAGLC/QkpZjpIBXVq4CCFqNkdq6JFAiFIqWClVCxgHLC61zSKgv1LKVSnlCfQE9lRsUc+Wlp1PXQ83MyhX8kHJnwsharTz1tC11lal1CRgGWABZmito5RSDxWsn6a13qOU+gXYAdiBz7TWuyqz4FCshp55EnJTpYYuhKjRHEm5oLVeCiwttWxaqedvA29XXNHOLzU7n3qebpAiLVyEEMJpe4rm5NvItdpNDb1wUC6poQshajCnDehpOfkA1PVwNQHdxQ3qNa/iUgkhRNVx3oCeXRDQC2voPkFgcSiDJIQQ1ZLTBvTUgoBuUi6HJH8uhKjxnDagp2VbAajrYTE3RSV/LoSo4Zw2oBfW0H1tJ8GaIzV0IUSN5/QBvX72YbNAAroQooZz2oBeeFPUK6MwoEvKRQhRszltQE/Nzqe2mwXXU4fAzRO8A6q6SEIIUaWcOqAXdSrybQWqrEEhhRCi5nDagJ6WUzygS/5cCCGcNqCnZufj4wGcOgwNJH8uhBBOHNCttHJLBm2TGroQQuDEAT0tO59gddw8kYAuhBDOG9Az86wE2gsmTpImi0II4bwB3WbT1M9PAjcv8GxQ1cURQogq51BAV0oNU0rtU0pFK6Uml7H+KqVUqlJqW8HPvyq+qCVZ7Rov22nw8q3sQwkhhFM473izSikL8DEwBDMZdKRSarHWenepTddorW+shDKWyWbXeFpTwcvvch1SCCGuaI7U0COAaK31Ia11HjAXGFG5xTo/q92Ol/UUeEoNXQghwLGA3hQ4Wux5XMGy0norpbYrpX5WSnUsa0dKqYlKqU1KqU1JSUkXUVxDa41dQ22poQshRBFHAnpZfep1qedbgBZa6zDgQ+CHsnaktZ6utQ7XWof7+/tfUEGLs9nN4T3zpYYuhBCFHAnocUCzYs8DgfjiG2it07TWGQWPlwJuSqlKqzpb7RoPcnGz50hAF0KIAo4E9EggRCkVrJSqBYwDFhffQCnVWCkzOpZSKqJgv8kVXdhCNrumAenmiQR0IYQAHGjlorW2KqUmAcsACzBDax2llHqoYP00YCzwsFLKCmQD47TWpdMyFcZq1/iogoAuOXQhhAAcCOhQlEZZWmrZtGKPPwI+qtiilc9u1/iqNPNEauhCCAE4aU9Rq13jU5RykRq6EEKAkwZ0m13jW5hykW7/QggBOGlAt9rtNFBp2JUFPOpXdXGEEOKK4JQB3W6HBqST51YfXJzyLQghRIVzymhoaujp5Ln7VHVRhBDiiuGUAd1W0GwxXwK6EEIUccqAbrVrfEkjz11uiAohRCGnDOg2u6a+ysDqXr+qiyKEEFcMpw3otcgHV4+qLooQQlwxnDKgW+0aV+zg4lBHVyGEqBGcMqDb7BpXrCiLW1UXRQghrhhOGdCtdrvU0IUQohSnDOh2mx0XpaWGLoQQxThlQLdZc80Di9TQhRCikFMGdLvNCoCLpFyEEKKIUwd0JOUihBBFHAroSqlhSql9SqlopdTkc2zXQyllU0qNrbgins1uzQfARVIuQghR5LwBXSllAT4GrgM6AOOVUh3K2e5NzFR1lUrbTEBXEtCFEKKIIzX0CCBaa31Ia50HzAVGlLHdY8ACILECy1emMwFdUi5CCFHIkYDeFDha7HlcwbIiSqmmwChgGueglJqolNqklNqUlJR0oWUtYiu8KSoBXQghijgS0FUZy3Sp5/8FntFa2861I631dK11uNY63N/f38EilrGfwhq6qwR0IYQo5EgSOg5oVux5IBBfaptwYK5SCsAPuF4pZdVa/1ARhSxNWwtr6JJDF0KIQo5ExEggRCkVDBwDxgG3Fd9Aax1c+FgpNRP4sbKCOYC2SysXIYQo7bwRUWttVUpNwrResQAztNZRSqmHCtafM29eGXRRDr3W5T60EEJcsRyq4mqtlwJLSy0rM5Brre+59GKdpzwFOXQXV6mhCyFEIafsKVpUQ5ebokIIUcQ5A3pRDl0CuhBCFHLKgE5BDd0iAV0IIYo4ZUDXdkm5CCFEaU4Z0CnsWCTD5wohRBHnDOj2gg6pEtCFEKKIkwb0PPNbcuhCCFHEKQO6lhq6EEKcxSkDemErFwnoQghxhnMGdLsEdCGEKM0pA7qSgC6EEGdxyoBeVEOXm6JCCFHEKQO60oU1dEvVFkQIIa4gThnQz+TQpYYuhBCFnDKgu0gOXQghzuKUAV1auQghxNkcCuhKqWFKqX1KqWil1OQy1o9QSu1QSm1TSm1SSvWr+KIWO17hXNSSQxdCiCLnreIqpSzAx8AQzITRkUqpxVrr3cU2+x1YrLXWSqlQYB7QrjIKDKDs+Vix4GompRZCCIFjNfQIIFprfUhrnQfMBUYU30BrnaG11gVPvQBNJXLRNmxI7VwIIYpzJKA3BY4Wex5XsKwEpdQopdRe4CfgvrJ2pJSaWJCS2ZSUlHQx5TX7sduwKQnoQghRnCMBvay8xlk1cK31Qq11O2Ak8HJZO9JaT9dah2utw/39/S+ooCUKpK3YpYYuhBAlOBLQ44BmxZ4HAvHlbay1Xg20Ukr5XWLZyuWirdiUtHARQojiHAnokUCIUipYKVULGAcsLr6BUqq1UuYOpVKqG1ALSK7owhYyAV1q6EIIUdx5q7laa6tSahKwDLAAM7TWUUqphwrWTwPGAHcppfKBbODWYjdJK5yLtknKRQghSnEob6G1XgosLbVsWrHHbwJvVmzRyqe0DbukXIQQogSn7Cnqoq3YJeUihBAlOGlAt8lNUSGEKMUpA7pFW9FSQxdCiBKcMqC7aJukXIQQohQnDuiSchFCiOKcMqBbsKJl6FwhhCjBOQO6tkkOXQghSnHKgO6CpFyEEKI0pwzortomKRchhCjFKQO6Cza01NCFEKIEp4yKrlixy/RzQlSY/Px84uLiyMnJqeqiiAIeHh4EBgbi5ubm8GucMqBbtF16igpRgeLi4vD29iYoKAglUztWOa01ycnJxMXFERwc7PDrnC7lYrdrLEgOXYiKlJOTg6+vrwTzK4RSCl9f3wv+xuR0Ad2mNa7KBhLQhahQEsyvLBfz93C+gG7XuEoNXQghzuJ0Ad1q11iwS0AXQohSHAroSqlhSql9SqlopdTkMtbfrpTaUfCzXikVVvFFNWx2jRtWSbkIUYPVqVOn3HXBwcHs27evxLInn3ySt956C4CtW7eilGLZsmUO77O4adOmMWvWrAss8eVx3qiolLIAHwNDMBNGRyqlFmutdxfbLAYYqLU+pZS6DpgO9KyMAtvsmlrYJaALUUleWhLF7vi0Ct1nhyZ1mXJTxwrdZ3nGjRvH3LlzmTJlCgB2u5358+ezbt06AObMmUO/fv2YM2cO11577QXv/6GHHqrQ8lYkR2roEUC01vqQ1joPmAuMKL6B1nq91vpUwdMNQGDFFvMMq92OG3JTVIjq5JlnnmHq1KlFz1988UVeeuklBg8eTLdu3ejcuTOLFi1yaF/jx49n7ty5Rc9Xr15NUFAQLVq0QGvN/PnzmTlzJr/++utFtbt/8cUXeeeddwD49NNP6dGjB2FhYYwZM4asrCwAEhISGDVqFGFhYYSFhbF+/foLPs7FcCQqNgWOFnsex7lr3/cDP5e1Qik1EZgI0Lx5cweLWJKtoNmiBHQhKsflqkkXN27cOJ588kkeeeQRAObNm8cvv/zCU089Rd26dTl58iS9evVi+PDh5239ERoaiouLC9u3bycsLIy5c+cyfvx4ANatW0dwcDCtWrXiqquuYunSpYwePfqiyz169GgmTJgAwPPPP8/nn3/OY489xuOPP87AgQNZuHAhNpuNjIyMiz7GhXCkhl7W2dNlbqjUIExAf6as9Vrr6VrrcK11uL+/v+OlLMZqteOqJOUiRHXStWtXEhMTiY+PZ/v27fj4+BAQEMCzzz5LaGgo11xzDceOHSMhIcGh/RXW0q1WK4sWLeLmm28GTLpl3LhxgLmIzJkz55LKvWvXLvr370/nzp2ZPXs2UVFRAKxYsYKHH34YAIvFQr169S7pOI5yJCrGAc2KPQ8E4ktvpJQKBT4DrtNaJ1dM8c5mt+WbBxYJ6EJUJ2PHjmX+/PmcOHGCcePGMXv2bJKSkti8eTNubm4EBQU5nCIZP348Q4cOZeDAgYSGhtKwYUNsNhsLFixg8eLFvPrqq0W9MdPT0/H29r6oMt9zzz388MMPhIWFMXPmTFatWnVR+6kojtTQI4EQpVSwUqoWMA5YXHwDpVRz4HvgTq31/oov5hm2woDu4vj4BkKIK1/hzcz58+czduxYUlNTadiwIW5ubqxcuZLDhw87vK9WrVrh6+vL5MmTi9Ity5cvJywsjKNHjxIbG8vhw4cZM2YMP/zww0WXOT09nYCAAPLz85k9e3bR8sGDB/PJJ58AYLPZSEur2JvM5TlvQNdaW4FJwDJgDzBPax2llHpIKVV4u/dfgC8wVSm1TSm1qbIKbLeagK6khi5EtdKxY0fS09Np2rQpAQEB3H777WzatInw8HBmz55Nu3btLmh/48ePZ+/evYwaNQow6ZbCx4XGjBnDN998A0BWVhaBgYFFP++99165+y7M47/88sv07NmTIUOGlCjf+++/z8qVK+ncuTPdu3cvSsVUNqV1menwShceHq43bbrwuL/30GHazQplT9iztB9VZqpeCHGB9uzZQ/v27au6GE7hscceo1u3btx7772Vfqyy/i5Kqc1a6/Cytne6nqLabjUPLJJyEUJcXi+88AJ//fUXw4cPr+qilMnp8hZFKRdp5SJEjbZz507uvPPOEsvc3d3566+/KmT/r776Kt99912JZTfffDMbN26skP1XBqeLinZbHgBKauhC1GidO3dm27Ztlbb/5557jueee67S9l8ZnC7lYrOalIvcFBVCiJKcLqBraeUihBBlcrqAbi+4KeoiKRchhCjB+QK6tTCHLjV0IYQozukCurZJDV2I6ub06dMlRlt01PXXX8/p06cv6DUzZ84s6j1a6OTJk/j7+5ObmwvAiBEj6N27d4ltio+yeD59+vS5oDJVFKer5tpt0g5diEr182Q4sbNi99m4M1z3RrmrCwN64WiLhWw2GxaLpdzXLV269IKLMnr0aJ5++mmysrLw9PQEYP78+QwfPhx3d3dOnz7Nli1bqFOnDjExMQQHB1/wMS7XcLmlOWEN3dwUtUhAF6LamDx5MgcPHqRLly706NGDQYMGcdttt9G5c2cARo4cSffu3enYsSPTp08vel1QUBAnT54kNjaW9u3bM2HCBDp27MjQoUPJzs4u81h169ZlwIABLFmypGhZ8SF2FyxYwE033VQ0tszFKJz9KCMjo9wx3WfNmkVoaChhYWFntae/aFrrKvnp3r27vhjRG37UekpdnRy14qJeL4Q42+7du6v0+DExMbpjx45aa61XrlypPT099aFDh4rWJycna621zsrK0h07dtQnT57UWmvdokULnZSUpGNiYrTFYtFbt27VWmt9880366+++qrc482bN0+PHDlSa631sWPHdEBAgLZarVprrQcPHqxXr16t9+3bpzt37lz0milTpui3337boffj5eWltdY6Pz9fp6amaq21TkpK0q1atdJ2u13v2rVLt2nTRiclJZV4f6WV9XcBNuly4qrT1dBb+XoA0KCOZxWXRAhRWSIiIkqkOj744APCwsLo1asXR48e5cCBA2e9Jjg4mC5dugDQvXt3YmNjy93/jTfeyNq1a0lLS2PevHmMHTsWi8VCQkIC0dHR9OvXjzZt2uDq6squXbsu+n1orcsc033FihWMHTsWPz8/ABo0aHDRxyjO6QI6hWO5SNd/IaotLy+voserVq1i+fLl/Pnnn2zfvp2uXbuWOS66u7t70WOLxYK1oBNiWWrXrs2wYcNYuHBhiXTLt99+y6lTpwgODiYoKIjY2NiLTrsAJcZ037ZtG40aNSInJwet9XlnXroYzhvQpdmiENWGt7c36enpZa5LTU3Fx8cHT09P9u7dy4YNGyrkmOPHj+e9994jISGBXr16AWaI3V9++YXY2FhiY2PZvHnzJQX08sZ0Hzx4MPPmzSM52cwFlJKSculvCGcM6EUTXEhAF6K68PX1pW/fvnTq1Im///3vJdYNGzYMq9VKaGgoL7zwQlHwvVRDhw4lPj6eW2+9FaUUsbGxHDlypMT+g4ODqVu3btGAX6+88kqJMdPLU1j7Lm9M944dO/Lcc88xcOBAwsLC+Nvf/lYh78npxkPn6Eb48yO49nWo17TiCyZEDSTjoVec5ORkunXrdkEzLJWnUsZDV0oNU0rtU0pFK6Uml7G+nVLqT6VUrlLq6YsquaOaRcAtsySYCyGuOPHx8fTu3Zunn67cMFie8+YtlFIW4GNgCGbC6Eil1GKt9e5im6UAjwMjK6OQQghxMR599FHWrVtXYtkTTzxRIbMNJScnM3jw4LOW//nnn/j6+l7y/i+GI4noCCBaa30IQCk1FxgBFAV0rXUikKiUuqFSSimEqHSV1fKiKn388ceVtm9fX99KHY/9YtLhjqRcmgJHiz2PK1h2wZRSE5VSm5RSm5KSki5mF0KISuDh4UFycvJFBRFR8bTWJCcn4+HhcUGvc6SGXtYl+6L+6lrr6cB0MDdFL2YfQoiKFxgYSFxcHFLRunJ4eHicsyVNWRwJ6HFAs2LPA4H4CzqKEOKK5ubmdlGDUIkriyMpl0ggRCkVrJSqBYwDFldusYQQQlyo89bQtdZWpdQkYBlgAWZoraOUUg8VrJ+mlGoMbALqAnal1JNAB611WuUVXQghRHEOdbfUWi8FlpZaNq3Y4xOYVIwQQogqUmU9RZVSScDFdqXyA05WYHGqEzk3ZZPzUj45N2W7Us9LC621f1krqiygXwql1Kbyur7WdHJuyibnpXxybsrmjOfF+QbnEkIIUSYJ6EIIUU04a0Cffv5Naiw5N2WT81I+OTdlc7rz4pQ5dCGEEGdz1hq6EEKIUiSgCyFENeF0Af18k23UJEqpWKXUTqXUNqXUpoJlDZRSvymlDhT89qnqcl4OSqkZSqlEpdSuYsvKPRdKqX8WfIb2KaWurZpSV75yzsuLSqljBZ+bbUqp64utqynnpZlSaqVSao9SKkop9UTBcuf+zGitneYHM/TAQaAlUAvYjhlioMrLVkXnIxbwK7XsLWBywePJwJtVXc7LdC4GAN2AXec7F0CHgs+OOxBc8JmyVPV7uIzn5UXg6TK2rUnnJQDoVvDYG9hf8P6d+jPjbDX0osk2tNZ5QOFkG+KMEcCXBY+/pIbMIqW1Xo2ZOau48s7FCGCu1jpXax0DRGM+W9VOOeelPDXpvBzXWm8peJwO7MHM8+DUnxlnC+gVNtlGNaGBX5VSm5VSEwuWNdJaHwfzoQUaVlnpql5550I+RzBJKbWjICVTmFaokedFKRUEdAX+wsk/M84W0Ctsso1qoq/WuhtwHfCoUmpAVRfISdT0z9EnQCugC3AceLdgeY07L0qpOsAC4El97tFhneLcOFtAl8k2itFaxxf8TgQWYr4CJiilAgAKfidWXQmrXHnnokZ/jrTWCVprm9baDnzKmdRBjTovSik3TDCfrbX+vmCxU39mnC2gy2QbBZRSXkop78LHwFBgF+Z83F2w2d3Aoqop4RWhvHOxGBinlHJXSgUDIcDGKihflSgMWAVGYT43UIPOizKzYX8O7NFav1dslVN/ZhwaD/1KocuZbKOKi1VVGgELC2ZpdwW+0Vr/opSKBOYppe4HjgA3V2EZLxul1BzgKsBPKRUHTAHeoIxzoc0ELfOA3YAVeFRrbauSgleycs7LVUqpLpiUQSzwINSs8wL0Be4EdiqlthUsexYn/8xI138hhKgmnC3lIoQQohwS0IUQopqQgC6EENWEBHQhhKgmJKALIUQ1IQFdCCGqCQnoQghRTfw/1CghtdSjrCoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "states[['val_VAL_jac','train_VAL_jac']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Prediction on VEN-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = osp.join(os.getcwd(), '../../data/VEN')\n",
    "#transform = T.Compose([T.NormalizeFeatures(), T.ToSparseTensor()])\n",
    "dataset_XL = VEN_XL_Homo('dataset/Venice_XL_homo')\n",
    "data_XL = dataset_XL[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=80963, x=[80963, 1753], y=[80963, 20], att_lab=[80963], val_lab=[80963], node_type=[80963], train_mask=[80963], val_mask=[80963], test_mask=[80963], edge_index=[2, 290091503], edge_attr=[290091503], n_id=[80963])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_XL.n_id = torch.arange(data_XL.num_nodes)\n",
    "data_XL = data_XL.to(device)\n",
    "data_XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT_L(dataset_XL.num_features, 256, dataset_XL.num_classes,\n",
    "            heads=2, dropout=.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(args.save_dir+'GAT_best_model/model.pth',map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "train_loader = NeighborLoader(\n",
    "    data_XL,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data_XL.train_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "val_loader = NeighborLoader(\n",
    "    data_XL,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data_XL.val_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args.seed)\n",
    "test_loader = NeighborLoader(\n",
    "    data_XL,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=data_XL.test_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_loader = NeighborLoader(\n",
    "    data_XL,\n",
    "    # Sample 30 neighbors for each node and edge type for 2 iterations\n",
    "    num_neighbors=[3*args.sample_nodes] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type paper\n",
    "    batch_size=args.batch_size,\n",
    "    input_nodes=~(data_XL.train_mask + data_XL.val_mask + data_XL.test_mask),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_Homo(model, loader):\n",
    "    model.eval()\n",
    "    seed_everything(args.seed)\n",
    "    all_preds = []\n",
    "    \n",
    "    for batch in tqdm(loader):\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch.batch_size\n",
    "        edge_index = to_undirected(batch.edge_index)\n",
    "        out = model(batch.x, edge_index)[:batch_size]\n",
    "        out_att = out[:,:9].softmax(axis=1)\n",
    "        out_val = out[:,9:].softmax(axis=1)\n",
    "        IDs = batch.n_id[:batch_size].unsqueeze(dim=-1).int()\n",
    "        \n",
    "        now = torch.hstack([IDs, out_att, out_val])\n",
    "        all_preds.append(now)\n",
    "    \n",
    "    final = torch.vstack(all_preds)\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 362/362 [26:09<00:00,  4.34s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict_Homo(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 624/624 [43:27<00:00,  4.18s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_val = predict_Homo(model, val_loader)\n",
    "#pred_test = predict_Homo(model, test_loader)\n",
    "#pred_unlab = predict_Homo(model, unlabel_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 624/624 [43:37<00:00,  4.19s/it]\n"
     ]
    }
   ],
   "source": [
    "#pred_val = predict_Homo(model, val_loader)\n",
    "pred_test = predict_Homo(model, test_loader)\n",
    "#pred_unlab = predict_Homo(model, unlabel_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 921/921 [1:02:41<00:00,  4.08s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_unlab = predict_Homo(model, unlabel_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False], device='cuda:0')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~(data.train_mask + data.val_mask + data.test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0000e+00, 5.2877e-03, 8.9800e-01, 5.4645e-03, 2.0237e-03, 2.2924e-03,\n",
       "        5.9337e-02, 9.0199e-03, 1.7338e-03, 1.6838e-02, 2.8692e-02, 9.9442e-02,\n",
       "        1.8664e-01, 4.2723e-01, 1.1571e-01, 1.0448e-01, 7.7790e-03, 4.0255e-03,\n",
       "        7.0834e-03, 8.1606e-03, 1.0753e-02])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.vstack([pred_train, pred_val, pred_test, pred_unlab]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.vstack([pred_train, pred_val, pred_test]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.vstack([pred, pred_unlab]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 2.0000, 2.0000,  ..., 2.0000, 2.0000, 2.0000],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:,1:].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds).sort_values(0).set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.606621</td>\n",
       "      <td>0.042645</td>\n",
       "      <td>0.012985</td>\n",
       "      <td>0.019549</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>0.256311</td>\n",
       "      <td>0.015131</td>\n",
       "      <td>0.014469</td>\n",
       "      <td>0.027402</td>\n",
       "      <td>0.239379</td>\n",
       "      <td>0.088697</td>\n",
       "      <td>0.135640</td>\n",
       "      <td>0.395381</td>\n",
       "      <td>0.033715</td>\n",
       "      <td>0.082694</td>\n",
       "      <td>0.005551</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>0.008834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.252455</td>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.721182</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.212146</td>\n",
       "      <td>0.081509</td>\n",
       "      <td>0.119666</td>\n",
       "      <td>0.451611</td>\n",
       "      <td>0.028806</td>\n",
       "      <td>0.086718</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.008845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.009880</td>\n",
       "      <td>0.191691</td>\n",
       "      <td>0.012672</td>\n",
       "      <td>0.010790</td>\n",
       "      <td>0.564881</td>\n",
       "      <td>0.020956</td>\n",
       "      <td>0.090688</td>\n",
       "      <td>0.006743</td>\n",
       "      <td>0.091698</td>\n",
       "      <td>0.099023</td>\n",
       "      <td>0.085721</td>\n",
       "      <td>0.125811</td>\n",
       "      <td>0.129485</td>\n",
       "      <td>0.035432</td>\n",
       "      <td>0.411415</td>\n",
       "      <td>0.045142</td>\n",
       "      <td>0.017808</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>0.018913</td>\n",
       "      <td>0.016565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.022011</td>\n",
       "      <td>0.239435</td>\n",
       "      <td>0.024383</td>\n",
       "      <td>0.018035</td>\n",
       "      <td>0.051965</td>\n",
       "      <td>0.186176</td>\n",
       "      <td>0.360085</td>\n",
       "      <td>0.016943</td>\n",
       "      <td>0.080967</td>\n",
       "      <td>0.046681</td>\n",
       "      <td>0.116083</td>\n",
       "      <td>0.210690</td>\n",
       "      <td>0.209185</td>\n",
       "      <td>0.074825</td>\n",
       "      <td>0.260412</td>\n",
       "      <td>0.026387</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.014159</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.898003</td>\n",
       "      <td>0.005465</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.059337</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.016838</td>\n",
       "      <td>0.028692</td>\n",
       "      <td>0.099442</td>\n",
       "      <td>0.186639</td>\n",
       "      <td>0.427228</td>\n",
       "      <td>0.115714</td>\n",
       "      <td>0.104485</td>\n",
       "      <td>0.007779</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.007083</td>\n",
       "      <td>0.008161</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80958.0</th>\n",
       "      <td>0.071110</td>\n",
       "      <td>0.222845</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.009293</td>\n",
       "      <td>0.008150</td>\n",
       "      <td>0.540893</td>\n",
       "      <td>0.037078</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.090071</td>\n",
       "      <td>0.112130</td>\n",
       "      <td>0.258910</td>\n",
       "      <td>0.152992</td>\n",
       "      <td>0.334889</td>\n",
       "      <td>0.017436</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>0.004125</td>\n",
       "      <td>0.009595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80959.0</th>\n",
       "      <td>0.931158</td>\n",
       "      <td>0.017134</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.018813</td>\n",
       "      <td>0.009902</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>0.007012</td>\n",
       "      <td>0.005527</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.246164</td>\n",
       "      <td>0.249971</td>\n",
       "      <td>0.178524</td>\n",
       "      <td>0.187882</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.003415</td>\n",
       "      <td>0.004380</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.009457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80960.0</th>\n",
       "      <td>0.832561</td>\n",
       "      <td>0.046017</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.036893</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0.054922</td>\n",
       "      <td>0.010387</td>\n",
       "      <td>0.006642</td>\n",
       "      <td>0.003834</td>\n",
       "      <td>0.311779</td>\n",
       "      <td>0.129848</td>\n",
       "      <td>0.213426</td>\n",
       "      <td>0.163755</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.152770</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.009385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80961.0</th>\n",
       "      <td>0.587178</td>\n",
       "      <td>0.218733</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.018808</td>\n",
       "      <td>0.010307</td>\n",
       "      <td>0.033901</td>\n",
       "      <td>0.085850</td>\n",
       "      <td>0.025017</td>\n",
       "      <td>0.007861</td>\n",
       "      <td>0.197058</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.186971</td>\n",
       "      <td>0.202354</td>\n",
       "      <td>0.013396</td>\n",
       "      <td>0.135650</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>0.004019</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.010585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80962.0</th>\n",
       "      <td>0.680041</td>\n",
       "      <td>0.110760</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.075414</td>\n",
       "      <td>0.011273</td>\n",
       "      <td>0.074967</td>\n",
       "      <td>0.014522</td>\n",
       "      <td>0.012880</td>\n",
       "      <td>0.013968</td>\n",
       "      <td>0.180312</td>\n",
       "      <td>0.332448</td>\n",
       "      <td>0.151884</td>\n",
       "      <td>0.185603</td>\n",
       "      <td>0.010979</td>\n",
       "      <td>0.111718</td>\n",
       "      <td>0.003960</td>\n",
       "      <td>0.004599</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>0.010697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80963 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               1         2         3         4         5         6         7   \\\n",
       "0                                                                               \n",
       "0.0      0.606621  0.042645  0.012985  0.019549  0.004886  0.256311  0.015131   \n",
       "1.0      0.252455  0.009630  0.000875  0.002867  0.001045  0.721182  0.001338   \n",
       "2.0      0.009880  0.191691  0.012672  0.010790  0.564881  0.020956  0.090688   \n",
       "3.0      0.022011  0.239435  0.024383  0.018035  0.051965  0.186176  0.360085   \n",
       "4.0      0.005288  0.898003  0.005465  0.002024  0.002292  0.059337  0.009020   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "80958.0  0.071110  0.222845  0.012092  0.009293  0.008150  0.540893  0.037078   \n",
       "80959.0  0.931158  0.017134  0.001273  0.018813  0.009902  0.007399  0.007012   \n",
       "80960.0  0.832561  0.046017  0.001951  0.036893  0.006793  0.054922  0.010387   \n",
       "80961.0  0.587178  0.218733  0.012346  0.018808  0.010307  0.033901  0.085850   \n",
       "80962.0  0.680041  0.110760  0.006174  0.075414  0.011273  0.074967  0.014522   \n",
       "\n",
       "               8         9         10        11        12        13        14  \\\n",
       "0                                                                               \n",
       "0.0      0.014469  0.027402  0.239379  0.088697  0.135640  0.395381  0.033715   \n",
       "1.0      0.001459  0.009149  0.212146  0.081509  0.119666  0.451611  0.028806   \n",
       "2.0      0.006743  0.091698  0.099023  0.085721  0.125811  0.129485  0.035432   \n",
       "3.0      0.016943  0.080967  0.046681  0.116083  0.210690  0.209185  0.074825   \n",
       "4.0      0.001734  0.016838  0.028692  0.099442  0.186639  0.427228  0.115714   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "80958.0  0.008469  0.090071  0.112130  0.258910  0.152992  0.334889  0.017436   \n",
       "80959.0  0.005527  0.001782  0.246164  0.249971  0.178524  0.187882  0.010783   \n",
       "80960.0  0.006642  0.003834  0.311779  0.129848  0.213426  0.163755  0.006383   \n",
       "80961.0  0.025017  0.007861  0.197058  0.239599  0.186971  0.202354  0.013396   \n",
       "80962.0  0.012880  0.013968  0.180312  0.332448  0.151884  0.185603  0.010979   \n",
       "\n",
       "               15        16        17        18        19        20  \n",
       "0                                                                    \n",
       "0.0      0.082694  0.005551  0.003396  0.003088  0.003626  0.008834  \n",
       "1.0      0.086718  0.003646  0.002293  0.002107  0.002652  0.008845  \n",
       "2.0      0.411415  0.045142  0.017808  0.014686  0.018913  0.016565  \n",
       "3.0      0.260412  0.026387  0.010870  0.015876  0.014159  0.014831  \n",
       "4.0      0.104485  0.007779  0.004025  0.007083  0.008161  0.010753  \n",
       "...           ...       ...       ...       ...       ...       ...  \n",
       "80958.0  0.098389  0.003754  0.004059  0.003721  0.004125  0.009595  \n",
       "80959.0  0.102800  0.003415  0.004380  0.003295  0.003330  0.009457  \n",
       "80960.0  0.152770  0.003210  0.003906  0.002351  0.003185  0.009385  \n",
       "80961.0  0.135650  0.003489  0.004019  0.003198  0.003681  0.010585  \n",
       "80962.0  0.111718  0.003960  0.004599  0.003972  0.003828  0.010697  \n",
       "\n",
       "[80963 rows x 20 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.to_csv(args.save_dir + 'preds_XL_trans.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.606621</td>\n",
       "      <td>0.042645</td>\n",
       "      <td>0.012985</td>\n",
       "      <td>0.019549</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>0.256311</td>\n",
       "      <td>0.015131</td>\n",
       "      <td>0.014469</td>\n",
       "      <td>0.027402</td>\n",
       "      <td>0.239379</td>\n",
       "      <td>0.088697</td>\n",
       "      <td>0.135640</td>\n",
       "      <td>0.395381</td>\n",
       "      <td>0.033715</td>\n",
       "      <td>0.082694</td>\n",
       "      <td>0.005551</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>0.008834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.252455</td>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.721182</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.212146</td>\n",
       "      <td>0.081509</td>\n",
       "      <td>0.119666</td>\n",
       "      <td>0.451611</td>\n",
       "      <td>0.028806</td>\n",
       "      <td>0.086718</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.008845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.009880</td>\n",
       "      <td>0.191691</td>\n",
       "      <td>0.012672</td>\n",
       "      <td>0.010790</td>\n",
       "      <td>0.564881</td>\n",
       "      <td>0.020956</td>\n",
       "      <td>0.090688</td>\n",
       "      <td>0.006743</td>\n",
       "      <td>0.091698</td>\n",
       "      <td>0.099023</td>\n",
       "      <td>0.085721</td>\n",
       "      <td>0.125811</td>\n",
       "      <td>0.129485</td>\n",
       "      <td>0.035432</td>\n",
       "      <td>0.411415</td>\n",
       "      <td>0.045142</td>\n",
       "      <td>0.017808</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>0.018913</td>\n",
       "      <td>0.016565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.022011</td>\n",
       "      <td>0.239435</td>\n",
       "      <td>0.024383</td>\n",
       "      <td>0.018035</td>\n",
       "      <td>0.051965</td>\n",
       "      <td>0.186176</td>\n",
       "      <td>0.360085</td>\n",
       "      <td>0.016943</td>\n",
       "      <td>0.080967</td>\n",
       "      <td>0.046681</td>\n",
       "      <td>0.116083</td>\n",
       "      <td>0.210690</td>\n",
       "      <td>0.209185</td>\n",
       "      <td>0.074825</td>\n",
       "      <td>0.260412</td>\n",
       "      <td>0.026387</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.014159</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.898003</td>\n",
       "      <td>0.005465</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.059337</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.016838</td>\n",
       "      <td>0.028692</td>\n",
       "      <td>0.099442</td>\n",
       "      <td>0.186639</td>\n",
       "      <td>0.427228</td>\n",
       "      <td>0.115714</td>\n",
       "      <td>0.104485</td>\n",
       "      <td>0.007779</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.007083</td>\n",
       "      <td>0.008161</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80958.0</th>\n",
       "      <td>0.071110</td>\n",
       "      <td>0.222845</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.009293</td>\n",
       "      <td>0.008150</td>\n",
       "      <td>0.540893</td>\n",
       "      <td>0.037078</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.090071</td>\n",
       "      <td>0.112130</td>\n",
       "      <td>0.258910</td>\n",
       "      <td>0.152992</td>\n",
       "      <td>0.334889</td>\n",
       "      <td>0.017436</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>0.004125</td>\n",
       "      <td>0.009595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80959.0</th>\n",
       "      <td>0.931158</td>\n",
       "      <td>0.017134</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.018813</td>\n",
       "      <td>0.009902</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>0.007012</td>\n",
       "      <td>0.005527</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.246164</td>\n",
       "      <td>0.249971</td>\n",
       "      <td>0.178524</td>\n",
       "      <td>0.187882</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.003415</td>\n",
       "      <td>0.004380</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.009457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80960.0</th>\n",
       "      <td>0.832561</td>\n",
       "      <td>0.046017</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.036893</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0.054922</td>\n",
       "      <td>0.010387</td>\n",
       "      <td>0.006642</td>\n",
       "      <td>0.003834</td>\n",
       "      <td>0.311779</td>\n",
       "      <td>0.129848</td>\n",
       "      <td>0.213426</td>\n",
       "      <td>0.163755</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.152770</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.009385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80961.0</th>\n",
       "      <td>0.587178</td>\n",
       "      <td>0.218733</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.018808</td>\n",
       "      <td>0.010307</td>\n",
       "      <td>0.033901</td>\n",
       "      <td>0.085850</td>\n",
       "      <td>0.025017</td>\n",
       "      <td>0.007861</td>\n",
       "      <td>0.197058</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.186971</td>\n",
       "      <td>0.202354</td>\n",
       "      <td>0.013396</td>\n",
       "      <td>0.135650</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>0.004019</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.010585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80962.0</th>\n",
       "      <td>0.680041</td>\n",
       "      <td>0.110760</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.075414</td>\n",
       "      <td>0.011273</td>\n",
       "      <td>0.074967</td>\n",
       "      <td>0.014522</td>\n",
       "      <td>0.012880</td>\n",
       "      <td>0.013968</td>\n",
       "      <td>0.180312</td>\n",
       "      <td>0.332448</td>\n",
       "      <td>0.151884</td>\n",
       "      <td>0.185603</td>\n",
       "      <td>0.010979</td>\n",
       "      <td>0.111718</td>\n",
       "      <td>0.003960</td>\n",
       "      <td>0.004599</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>0.010697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80963 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                1         2         3         4         5         6         7  \\\n",
       "0                                                                               \n",
       "0.0      0.606621  0.042645  0.012985  0.019549  0.004886  0.256311  0.015131   \n",
       "1.0      0.252455  0.009630  0.000875  0.002867  0.001045  0.721182  0.001338   \n",
       "2.0      0.009880  0.191691  0.012672  0.010790  0.564881  0.020956  0.090688   \n",
       "3.0      0.022011  0.239435  0.024383  0.018035  0.051965  0.186176  0.360085   \n",
       "4.0      0.005288  0.898003  0.005465  0.002024  0.002292  0.059337  0.009020   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "80958.0  0.071110  0.222845  0.012092  0.009293  0.008150  0.540893  0.037078   \n",
       "80959.0  0.931158  0.017134  0.001273  0.018813  0.009902  0.007399  0.007012   \n",
       "80960.0  0.832561  0.046017  0.001951  0.036893  0.006793  0.054922  0.010387   \n",
       "80961.0  0.587178  0.218733  0.012346  0.018808  0.010307  0.033901  0.085850   \n",
       "80962.0  0.680041  0.110760  0.006174  0.075414  0.011273  0.074967  0.014522   \n",
       "\n",
       "                8         9        10        11        12        13        14  \\\n",
       "0                                                                               \n",
       "0.0      0.014469  0.027402  0.239379  0.088697  0.135640  0.395381  0.033715   \n",
       "1.0      0.001459  0.009149  0.212146  0.081509  0.119666  0.451611  0.028806   \n",
       "2.0      0.006743  0.091698  0.099023  0.085721  0.125811  0.129485  0.035432   \n",
       "3.0      0.016943  0.080967  0.046681  0.116083  0.210690  0.209185  0.074825   \n",
       "4.0      0.001734  0.016838  0.028692  0.099442  0.186639  0.427228  0.115714   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "80958.0  0.008469  0.090071  0.112130  0.258910  0.152992  0.334889  0.017436   \n",
       "80959.0  0.005527  0.001782  0.246164  0.249971  0.178524  0.187882  0.010783   \n",
       "80960.0  0.006642  0.003834  0.311779  0.129848  0.213426  0.163755  0.006383   \n",
       "80961.0  0.025017  0.007861  0.197058  0.239599  0.186971  0.202354  0.013396   \n",
       "80962.0  0.012880  0.013968  0.180312  0.332448  0.151884  0.185603  0.010979   \n",
       "\n",
       "               15        16        17        18        19        20  \n",
       "0                                                                    \n",
       "0.0      0.082694  0.005551  0.003396  0.003088  0.003626  0.008834  \n",
       "1.0      0.086718  0.003646  0.002293  0.002107  0.002652  0.008845  \n",
       "2.0      0.411415  0.045142  0.017808  0.014686  0.018913  0.016565  \n",
       "3.0      0.260412  0.026387  0.010870  0.015876  0.014159  0.014831  \n",
       "4.0      0.104485  0.007779  0.004025  0.007083  0.008161  0.010753  \n",
       "...           ...       ...       ...       ...       ...       ...  \n",
       "80958.0  0.098389  0.003754  0.004059  0.003721  0.004125  0.009595  \n",
       "80959.0  0.102800  0.003415  0.004380  0.003295  0.003330  0.009457  \n",
       "80960.0  0.152770  0.003210  0.003906  0.002351  0.003185  0.009385  \n",
       "80961.0  0.135650  0.003489  0.004019  0.003198  0.003681  0.010585  \n",
       "80962.0  0.111718  0.003960  0.004599  0.003972  0.003828  0.010697  \n",
       "\n",
       "[80963 rows x 20 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pd.read_csv(args.save_dir + 'preds_XL_trans.csv', sep='\\t', index_col='0')\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.tensor(np.array(preds)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(('val_ATT_loss', 'val_VAL_loss', 'val_ATT_acc', 'val_VAL_acc', 'val_VAL_acc_k', 'val_VAL_jac_k'), columns=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ATT_loss = F.cross_entropy(data_XL.y[data_XL.train_mask][:,:9], \n",
    "                pred[data_XL.train_mask][:,:9]).cpu().detach().item()\n",
    "train_VAL_loss = F.cross_entropy(data_XL.y[data_XL.train_mask][:,9:], \n",
    "                pred[data_XL.train_mask][:,9:]).cpu().detach().item()\n",
    "\n",
    "train_ATT_acc = compute_1_accuracy(data_XL.y[data_XL.train_mask][:,:9], \n",
    "                pred[data_XL.train_mask][:,:9])\n",
    "train_VAL_acc = compute_1_accuracy(data_XL.y[data_XL.train_mask][:,9:], \n",
    "                pred[data_XL.train_mask][:,9:])\n",
    "train_VAL_acc_k = compute_k_accuracy(data_XL.y[data_XL.train_mask][:,9:].cpu(),  \n",
    "                pred[data_XL.train_mask][:,9:].cpu(),3)\n",
    "train_VAL_jac_k = compute_jaccard_index(data_XL.y[data_XL.train_mask][:,9:].cpu(),  \n",
    "                pred[data_XL.train_mask][:,9:].cpu(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['train'] = pd.DataFrame((train_ATT_loss, train_VAL_loss, train_ATT_acc, train_VAL_acc, train_VAL_acc_k, train_VAL_jac_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ATT_loss = F.cross_entropy(data_XL.y[(data_XL.att_lab) * data_XL.val_mask][:,:9], \n",
    "                pred[(data_XL.att_lab) * data_XL.val_mask][:,:9]).cpu().detach().item()\n",
    "val_VAL_loss = F.cross_entropy(data_XL.y[(data_XL.val_lab) * data_XL.val_mask][:,9:], \n",
    "                pred[(data_XL.val_lab) * data_XL.val_mask][:,9:]).cpu().detach().item()\n",
    "\n",
    "val_ATT_acc = compute_1_accuracy(data_XL.y[(data_XL.att_lab) * data_XL.val_mask][:,:9], \n",
    "                pred[(data_XL.att_lab) * data_XL.val_mask][:,:9])\n",
    "val_VAL_acc = compute_1_accuracy(data_XL.y[(data_XL.val_lab) * data_XL.val_mask][:,9:], \n",
    "                pred[(data_XL.val_lab) * data_XL.val_mask][:,9:])\n",
    "val_VAL_acc_k = compute_k_accuracy(data_XL.y[(data_XL.val_lab) * data_XL.val_mask][:,9:].cpu(),  \n",
    "                pred[(data_XL.val_lab) * data_XL.val_mask][:,9:].cpu(),3)\n",
    "val_VAL_jac_k = compute_jaccard_index(data_XL.y[(data_XL.val_lab) * data_XL.val_mask][:,9:].cpu(),  \n",
    "                pred[(data_XL.val_lab) * data_XL.val_mask][:,9:].cpu(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['val'] = pd.DataFrame((val_ATT_loss, val_VAL_loss, val_ATT_acc, val_VAL_acc, val_VAL_acc_k, val_VAL_jac_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ATT_loss = F.cross_entropy(data_XL.y[(data_XL.att_lab) * data_XL.test_mask][:,:9], \n",
    "                pred[(data_XL.att_lab) * data_XL.test_mask][:,:9]).cpu().detach().item()\n",
    "test_VAL_loss = F.cross_entropy(data_XL.y[(data_XL.val_lab) * data_XL.test_mask][:,9:], \n",
    "                pred[(data_XL.val_lab) * data_XL.test_mask][:,9:]).cpu().detach().item()\n",
    "\n",
    "test_ATT_acc = compute_1_accuracy(data_XL.y[(data_XL.att_lab) * data_XL.test_mask][:,:9], \n",
    "                pred[(data_XL.att_lab) * data_XL.test_mask][:,:9])\n",
    "test_VAL_acc = compute_1_accuracy(data_XL.y[(data_XL.val_lab) * data_XL.test_mask][:,9:], \n",
    "                pred[(data_XL.val_lab) * data_XL.test_mask][:,9:])\n",
    "test_VAL_acc_k = compute_k_accuracy(data_XL.y[(data_XL.val_lab) * data_XL.test_mask][:,9:].cpu(),  \n",
    "                pred[(data_XL.val_lab) * data_XL.test_mask][:,9:].cpu(),3)\n",
    "test_VAL_jac_k = compute_jaccard_index(data_XL.y[(data_XL.val_lab) * data_XL.test_mask][:,9:].cpu(),  \n",
    "                pred[(data_XL.val_lab) * data_XL.test_mask][:,9:].cpu(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['test'] = pd.DataFrame((test_ATT_loss, test_VAL_loss, test_ATT_acc, test_VAL_acc, test_VAL_acc_k, test_VAL_jac_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>val_ATT_loss</td>\n",
       "      <td>1.753365</td>\n",
       "      <td>1.753401</td>\n",
       "      <td>1.754944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val_VAL_loss</td>\n",
       "      <td>2.246282</td>\n",
       "      <td>2.243725</td>\n",
       "      <td>2.243127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>val_ATT_acc</td>\n",
       "      <td>93.318351</td>\n",
       "      <td>96.277877</td>\n",
       "      <td>96.008093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>val_VAL_acc</td>\n",
       "      <td>80.041490</td>\n",
       "      <td>80.879865</td>\n",
       "      <td>80.900141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>val_VAL_acc_k</td>\n",
       "      <td>98.470049</td>\n",
       "      <td>98.716864</td>\n",
       "      <td>98.607595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>val_VAL_jac_k</td>\n",
       "      <td>0.740859</td>\n",
       "      <td>0.735030</td>\n",
       "      <td>0.734388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name      train        val       test\n",
       "0   val_ATT_loss   1.753365   1.753401   1.754944\n",
       "1   val_VAL_loss   2.246282   2.243725   2.243127\n",
       "2    val_ATT_acc  93.318351  96.277877  96.008093\n",
       "3    val_VAL_acc  80.041490  80.879865  80.900141\n",
       "4  val_VAL_acc_k  98.470049  98.716864  98.607595\n",
       "5  val_VAL_jac_k   0.740859   0.735030   0.734388"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(args.save_dir+'eval_metrics_XL_trans.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_confusion_matrix(y, y_pred, k=3):\n",
    "    dim = y.shape[-1]\n",
    "    y = y.topk(k=k, axis=1)[1]\n",
    "    y_pred = y_pred.topk(k=k, axis=1)[1]\n",
    "    conf = np.zeros((dim, dim))\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            conf = np.add(conf, confusion_matrix(y[:,i], y_pred[:,j], labels = range(dim)))\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ATT_conf = confusion_matrix(data_XL.y[(data_XL.att_lab) * data_XL.test_mask][:,:9].argmax(axis=1).cpu(), \n",
    "                pred[(data_XL.att_lab) * data_XL.test_mask][:,:9].argmax(axis=1).cpu())\n",
    "test_VAL_conf = confusion_matrix(data_XL.y[(data_XL.val_lab) * data_XL.test_mask][:,9:].argmax(axis=1).cpu(), \n",
    "                pred[(data_XL.val_lab) * data_XL.test_mask][:,9:].argmax(axis=1).cpu(), labels=range(11))\n",
    "test_VAL_conf_k = (top_k_confusion_matrix(data_XL.y[(data_XL.val_lab) * data_XL.test_mask][:,9:].cpu(),  \n",
    "                pred[(data_XL.val_lab) * data_XL.test_mask][:,9:].cpu(),3)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1330,   15,    0,    4,    2,   33,    5,    0,    0],\n",
       "       [   0, 3187,    0,    0,    1,   12,   11,    0,    0],\n",
       "       [   0,    0,  230,    0,    0,    0,   33,    2,    0],\n",
       "       [  56,    9,    0,  543,    1,    1,   27,    0,    0],\n",
       "       [   5,    8,    0,    0, 2686,    0,    5,    0,    0],\n",
       "       [   5,   14,    0,    0,    4, 1559,    1,    0,    0],\n",
       "       [   2,   27,    0,    0,    9,    0, 2555,    0,    0],\n",
       "       [   4,    6,   36,    9,    0,    0,  144,  114,    0],\n",
       "       [   0,    6,    0,    0,   13,    3,    0,    0,  134]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ATT_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 608,    1,   23,    6,    0,   42,    0,    0,    0,    0,    0],\n",
       "       [  69,  418,  130,   75,    0,  157,    0,    0,    0,    0,    0],\n",
       "       [  49,   18, 1143,   58,    0,  339,    0,    0,    0,    0,    0],\n",
       "       [  37,    5,   38,  369,    0,   87,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [  36,    5,  149,   24,    0, 3200,    1,    0,    0,    0,    0],\n",
       "       [   1,    0,    0,    0,    0,    3,    6,    0,    0,    2,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    1,    0,    0,    8,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_VAL_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1432,  488, 1245,  809,    0,  975,    4,    0,    0,    0,    0],\n",
       "       [ 972, 1178, 2348, 1844,    9, 1857,    0,    0,    0,    0,    0],\n",
       "       [1035,  844, 5875, 4985,   10, 5328,    6,    0,    0,    1,    0],\n",
       "       [ 909,  843, 4856, 4774,   12, 4410,    0,    0,    0,    0,    0],\n",
       "       [   0,    3,   18,   18,    5,   10,    0,    0,    0,    0,    0],\n",
       "       [ 883,  424, 5357, 4623,    0, 5377,    5,    1,    0,    1,    0],\n",
       "       [   8,    0,   16,    7,    0,   17,   22,    2,    7,   17,    0],\n",
       "       [   1,    0,    0,    2,    0,    4,    6,    1,    2,    5,    0],\n",
       "       [   2,    0,    3,    1,    0,    2,   14,    1,    5,   14,    0],\n",
       "       [   2,    0,    4,    1,    0,    5,   18,    1,    7,   19,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_VAL_conf_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False,  ...,  True,  True,  True])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_XL.val_mask[data_XL.val_mask + data_XL.train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ATT_conf = confusion_matrix(data_XL.y[(data_XL.att_lab) * data_XL.val_mask][:,:9].argmax(axis=1).cpu(), \n",
    "                pred[(data_XL.att_lab) * data_XL.val_mask][:,:9].argmax(axis=1).cpu())\n",
    "val_VAL_conf = confusion_matrix(data_XL.y[(data_XL.val_lab) * data_XL.val_mask][:,9:].argmax(axis=1).cpu(), \n",
    "                pred[(data_XL.val_lab) * data_XL.val_mask][:,9:].argmax(axis=1).cpu(), labels=range(11))\n",
    "val_VAL_conf_k = (top_k_confusion_matrix(data_XL.y[(data_XL.val_lab) * data_XL.val_mask][:,9:].cpu(),  \n",
    "                pred[(data_XL.val_lab) * data_XL.val_mask][:,9:].cpu(),3)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1314,    3,    0,    2,    3,   27,    3,    0,    0],\n",
       "       [   1, 3269,    0,    0,    0,   14,   15,    0,    0],\n",
       "       [   0,    0,  259,    0,    0,    0,   20,    1,    0],\n",
       "       [  73,   11,    0,  570,    0,    2,   23,    1,    0],\n",
       "       [   1,    6,    0,    0, 2677,    0,    3,    0,    0],\n",
       "       [   4,   15,    0,    1,    3, 1464,    1,    0,    0],\n",
       "       [   1,   23,    0,    0,   12,    0, 2594,    0,    0],\n",
       "       [   7,    4,   24,    8,    0,    2,  141,  100,    0],\n",
       "       [   0,    6,    0,    0,   15,    3,    0,    0,  143]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ATT_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 594,    5,   15,   15,    0,   38,    0,    0,    0,    0,    0],\n",
       "       [  88,  416,  110,   87,    0,  128,    0,    0,    0,    0,    0],\n",
       "       [  50,   13, 1164,   68,    0,  348,    0,    0,    0,    1,    0],\n",
       "       [  51,    7,   42,  416,    0,   82,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    3,    1,    0,    0,    0,    0,    0,    0],\n",
       "       [  29,    7,  143,   18,    0, 3135,    1,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    5,    0,    0,    0,    0],\n",
       "       [   1,    0,    0,    0,    0,    0,    1,    0,    0,    1,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    3,    0,    0,    5,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_VAL_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1462,  481, 1230,  835,    1, 1006,    1,    0,    0,    0,    0],\n",
       "       [1028, 1169, 2426, 1963,    7, 1939,    3,    0,    0,    0,    0],\n",
       "       [1052,  850, 5833, 4922,    8, 5290,   14,    0,    0,    1,    0],\n",
       "       [ 915,  855, 4755, 4701,   13, 4325,    3,    0,    0,    0,    0],\n",
       "       [   0,    5,   20,   21,    9,   10,    0,    0,    0,    1,    0],\n",
       "       [ 871,  407, 5268, 4579,    1, 5304,   15,    0,    0,    1,    0],\n",
       "       [  11,    1,   24,   14,    2,   31,   21,    0,    3,   13,    0],\n",
       "       [   1,    0,    1,    1,    0,    2,    2,    0,    0,    2,    0],\n",
       "       [   3,    0,    5,    1,    1,    7,   14,    0,    3,   14,    0],\n",
       "       [   3,    0,    7,    3,    0,    8,   14,    0,    3,   13,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_VAL_conf_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ATT_conf = confusion_matrix(data_XL.y[data_XL.train_mask][:,:9].argmax(axis=1).cpu(), \n",
    "                                  pred[data_XL.train_mask][:,:9].argmax(axis=1).cpu())\n",
    "train_VAL_conf = confusion_matrix(data_XL.y[data_XL.train_mask][:,9:].argmax(axis=1).cpu(), \n",
    "                                 pred[data_XL.train_mask][:,9:].argmax(axis=1).cpu(), labels=range(11))\n",
    "train_VAL_conf_k = (top_k_confusion_matrix(data_XL.y[data_XL.train_mask][:,9:].cpu(),  \n",
    "                                pred[data_XL.train_mask][:,9:].cpu(),3)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1460,   12,    0,    2,    0,   22,    5,    0,    0],\n",
       "       [   1, 2619,    0,    0,    0,    8,    8,    0,    0],\n",
       "       [   2,    2,  108,    0,    0,    0,   27,    0,    0],\n",
       "       [  48,   18,    0,  394,    1,    2,   16,    1,    0],\n",
       "       [   4,    7,    0,    0, 2032,    0,    8,    0,    0],\n",
       "       [   4,   26,    0,    0,    4, 1473,    0,    0,    0],\n",
       "       [   1,   54,    0,    2,   11,    0, 2388,    1,    0],\n",
       "       [  24,   13,    8,   14,    0,    0,  397,  229,    0],\n",
       "       [   0,   11,    0,    0,    6,    3,    0,    0,   93]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ATT_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 885,    7,   33,   12,    0,   75,    0,    0,    0,    0,    0],\n",
       "       [ 105,  640,  250,  138,    0,  245,    0,    0,    0,    0,    0],\n",
       "       [  64,   21, 2028,   81,    0,  621,    0,    0,    0,    0,    0],\n",
       "       [  45,   11,   77,  788,    0,  122,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    2,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [  60,   10,  270,   49,    0, 4894,    0,    0,    0,    0,    0],\n",
       "       [   2,    0,    0,    0,    0,    3,   15,    0,    0,    1,    0],\n",
       "       [   1,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    2,    0,    0,   10,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_VAL_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2132,  714, 1777, 1222,    0, 1539,    3,    0,    0,    2,    0],\n",
       "       [1432, 1950, 4056, 3382,    9, 3282,    1,    0,    0,    0,    0],\n",
       "       [1593, 1456, 9605, 8151,   12, 8765,   10,    0,    0,    0,    0],\n",
       "       [1451, 1412, 7884, 7780,   20, 7186,    1,    0,    0,    0,    0],\n",
       "       [   0,   13,   50,   53,   18,   28,    0,    0,    0,    0,    0],\n",
       "       [1366,  695, 8553, 7464,    1, 8671,   12,    0,    0,    1,    0],\n",
       "       [  19,    0,   25,   14,    0,   38,   39,    1,    9,   29,    0],\n",
       "       [  10,    0,    3,    1,    0,   11,   15,    0,    2,   12,    0],\n",
       "       [   3,    0,    2,    0,    0,    7,   19,    1,    7,   18,    0],\n",
       "       [   7,    0,    4,    1,    0,   11,   26,    1,    9,   25,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_VAL_conf_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(train_ATT_conf),pd.DataFrame(val_ATT_conf),pd.DataFrame(test_ATT_conf)],axis=1).to_csv(args.save_dir+'confusion_matrix_ATT_XL_trans.csv')\n",
    "pd.concat([pd.DataFrame(train_VAL_conf),pd.DataFrame(val_VAL_conf),pd.DataFrame(test_VAL_conf)],axis=1).to_csv(args.save_dir+'confusion_matrix_VAL_XL_trans.csv')\n",
    "pd.concat([pd.DataFrame(train_VAL_conf_k),pd.DataFrame(val_VAL_conf_k),pd.DataFrame(test_VAL_conf_k)],axis=1).to_csv(args.save_dir+'confusion_matrix_VAL_k_XL_trans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_metrics(confusion_matrix, classes):\n",
    "    '''\n",
    "    Compute the per class precision, recall, and F1 for all the classes\n",
    "    \n",
    "    Args:\n",
    "    confusion_matrix (np.ndarry) with shape of (n_classes,n_classes): a confusion matrix of interest\n",
    "    classes (list of str) with shape (n_classes,): The names of classes\n",
    "    \n",
    "    Returns:\n",
    "    metrics_dict (dictionary): a dictionary that records the per class metrics\n",
    "    '''\n",
    "    num_class = confusion_matrix.shape[0]\n",
    "    metrics_dict = {}\n",
    "    for i in range(num_class):\n",
    "        key = classes[i]\n",
    "        temp_dict = {}\n",
    "        row = confusion_matrix[i,:]\n",
    "        col = confusion_matrix[:,i]\n",
    "        val = confusion_matrix[i,i]\n",
    "        precision = val/(row.sum()+0.000000001)\n",
    "        recall = val/(col.sum()+0.000000001)\n",
    "        F1 = 2*(precision*recall)/(precision+recall+0.000000001)\n",
    "        temp_dict['precision'] = precision\n",
    "        temp_dict['recall'] = recall\n",
    "        temp_dict['F1'] = F1\n",
    "        metrics_dict[key] = temp_dict\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_metrics_k(confusion_matrix, classes, k=3):\n",
    "    '''\n",
    "    Compute the per class precision, recall, and F1 for all the classes\n",
    "    \n",
    "    Args:\n",
    "    confusion_matrix (np.ndarry) with shape of (n_classes,n_classes): a confusion matrix of interest\n",
    "    classes (list of str) with shape (n_classes,): The names of classes\n",
    "    \n",
    "    Returns:\n",
    "    metrics_dict (dictionary): a dictionary that records the per class metrics\n",
    "    '''\n",
    "    num_class = confusion_matrix.shape[0]\n",
    "    metrics_dict = {}\n",
    "    for i in range(num_class):\n",
    "        key = classes[i]\n",
    "        temp_dict = {}\n",
    "        row = confusion_matrix[i,:]\n",
    "        col = confusion_matrix[:,i]\n",
    "        val = confusion_matrix[i,i]\n",
    "        precision = val*k/(row.sum()+0.000000001)\n",
    "        recall = val*k/(col.sum()+0.000000001)\n",
    "        F1 = 2*(precision*recall)/(precision+recall+0.000000001)\n",
    "        temp_dict['precision'] = precision\n",
    "        temp_dict['recall'] = recall\n",
    "        temp_dict['F1'] = F1\n",
    "        metrics_dict[key] = temp_dict\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Criterion i', 'Criterion ii', 'Criterion iii', 'Criterion iv', 'Criterion v', 'Criterion vi', \n",
    "              'Criterion vii', 'Criterion viii', 'Criterion ix', 'Criterion x', 'Others']\n",
    "categories = ['Building Elements',\n",
    " 'Urban Form Elements',\n",
    " 'Gastronomy',\n",
    " 'Interior Scenery',\n",
    " 'Natural Features and Land-scape Scenery',\n",
    " 'Monuments and Buildings',\n",
    " 'Peoples Activity and Association',\n",
    " 'Artifact Products',\n",
    " 'Urban Scenery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {}\n",
    "metrics_dict['test_ATT'] = per_class_metrics(test_ATT_conf, categories)\n",
    "metrics_dict['val_ATT'] = per_class_metrics(val_ATT_conf, categories)\n",
    "metrics_dict['test_VAL'] = per_class_metrics(test_VAL_conf, classes)\n",
    "metrics_dict['val_VAL'] = per_class_metrics(val_VAL_conf, classes)\n",
    "metrics_dict['test_VAL_k'] = per_class_metrics_k(test_VAL_conf_k, classes)\n",
    "metrics_dict['val_VAL_k'] = per_class_metrics_k(val_VAL_conf_k, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame.from_dict({(i,j): metrics_dict[i][j] \n",
    "                           for i in metrics_dict.keys() \n",
    "                           for j in metrics_dict[i].keys()},\n",
    "                       orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">test_ATT</th>\n",
       "      <th>Building Elements</th>\n",
       "      <td>0.957523</td>\n",
       "      <td>0.948645</td>\n",
       "      <td>0.953063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urban Form Elements</th>\n",
       "      <td>0.992526</td>\n",
       "      <td>0.974022</td>\n",
       "      <td>0.983187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gastronomy</th>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.864662</td>\n",
       "      <td>0.866290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interior Scenery</th>\n",
       "      <td>0.852433</td>\n",
       "      <td>0.976619</td>\n",
       "      <td>0.910310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Natural Features and Land-scape Scenery</th>\n",
       "      <td>0.993343</td>\n",
       "      <td>0.988954</td>\n",
       "      <td>0.991144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">val_VAL_k</th>\n",
       "      <th>Criterion vii</th>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Criterion viii</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Criterion ix</th>\n",
       "      <td>0.187500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Criterion x</th>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Others</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   precision    recall  \\\n",
       "test_ATT  Building Elements                         0.957523  0.948645   \n",
       "          Urban Form Elements                       0.992526  0.974022   \n",
       "          Gastronomy                                0.867925  0.864662   \n",
       "          Interior Scenery                          0.852433  0.976619   \n",
       "          Natural Features and Land-scape Scenery   0.993343  0.988954   \n",
       "...                                                      ...       ...   \n",
       "val_VAL_k Criterion vii                             0.525000  0.724138   \n",
       "          Criterion viii                            0.000000  0.000000   \n",
       "          Criterion ix                              0.187500  1.000000   \n",
       "          Criterion x                               0.764706  0.866667   \n",
       "          Others                                    0.000000  0.000000   \n",
       "\n",
       "                                                         F1  \n",
       "test_ATT  Building Elements                        0.953063  \n",
       "          Urban Form Elements                      0.983187  \n",
       "          Gastronomy                               0.866290  \n",
       "          Interior Scenery                         0.910310  \n",
       "          Natural Features and Land-scape Scenery  0.991144  \n",
       "...                                                     ...  \n",
       "val_VAL_k Criterion vii                            0.608696  \n",
       "          Criterion viii                           0.000000  \n",
       "          Criterion ix                             0.315789  \n",
       "          Criterion x                              0.812500  \n",
       "          Others                                   0.000000  \n",
       "\n",
       "[62 rows x 3 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(args.save_dir+'per_class_metrics_XL_trans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
